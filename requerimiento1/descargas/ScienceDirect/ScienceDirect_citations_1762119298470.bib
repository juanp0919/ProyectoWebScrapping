@article{ASHRAF2024100363,
title = {Data Information integrated Neural Network (DINN) algorithm for modelling and interpretation performance analysis for energy systems},
journal = {Energy and AI},
volume = {16},
pages = {100363},
year = {2024},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2024.100363},
url = {https://www.sciencedirect.com/science/article/pii/S2666546824000296},
author = {Waqar Muhammad Ashraf and Vivek Dua},
keywords = {Explainable AI, Model interpretation, Scientific machine learning, Artificial neural network, Loss function},
abstract = {Developing a well-predictive machine learning model that also offers improved interpretability is a key challenge to widen the application of artificial intelligence in various application domains. In this work, we present a Data Information integrated Neural Network (DINN) algorithm that incorporates the correlation information present in the dataset for the model development. The predictive performance of DINN is also compared with a standard artificial neural network (ANN) model. The DINN algorithm is applied on two case studies of energy systems namely energy efficiency cooling (ENC) & energy efficiency heating (ENH) of the buildings, and power generation from a 365 MW capacity industrial gas turbine. For ENC, DINN presents lower mean RMSE for testing datasets (RMSE_test = 1.23 %) in comparison with the ANN model (RMSE_test = 1.41 %). Similarly, DINN models have presented better predictive performance to model the output variables of the two case studies. The input perturbation analysis following the Gaussian distribution for noise generation reveals the order of significance of the variables, as made by DINN, can be better explained by the domain knowledge of the power generation operation of the gas turbine. This research work demonstrates the potential advantage to integrate the information present in the data for the well-predictive model development complemented with improved interpretation performance thereby opening avenues for industry-wide inclusion and other potential applications of machine learning.}
}
@article{VAERNESBRANDEN202423,
title = {Placental human papillomavirus infections and adverse pregnancy outcomes},
journal = {Placenta},
volume = {152},
pages = {23-30},
year = {2024},
issn = {0143-4004},
doi = {https://doi.org/10.1016/j.placenta.2024.05.126},
url = {https://www.sciencedirect.com/science/article/pii/S0143400424002406},
author = {Magdalena R. Værnesbranden and Anne Cathrine Staff and Johanna Wiik and Katrine Sjøborg and Corina S. Rueegg and Meryam Sugulle and Karin C. {Lødrup Carlsen} and Berit Granum and Guttorm Haugen and Gunilla Hedlin and Camilla G. Johannessen and Björn Nordlund and Camilla F. Nystrand and Anbjørg Rangberg and Eva M. Rehbinder and Knut Rudi and Yvonne Sandberg and Håvard O. Skjerven and Cilla Söderhäll and Riyas Vettukattil and Christine M. Jonassen},
keywords = {Placental biopsies, Genital HPV infection, Human papillomavirus, PreventADALL, Placental dysfunction syndromes},
abstract = {Introduction
Knowledge on prevalence and association of human papillomavirus (HPV) in third trimester placentae and adverse pregnancy outcomes is limited. We investigated the prevalence of placental HPV at delivery, explored urine HPV characteristics associated with placental HPV and whether placental HPV increased the risk adverse pregnancy outcomes.
Methods
Pregnant women were enrolled in the Scandinavian PreventADALL mother-child cohort study at midgestation. Human papillomavirus genotyping was performed on placental biopsies collected at delivery (n = 587) and first-void urine at midgestation and delivery (n = 556). Maternal characteristics were collected by questionnaires at gestational week 18 and 34. Adverse pregnancy outcomes were registered from chart data including hypertensive disorders of pregnancy, gestational diabetes mellitus and newborns small for gestational age. Uni- and multivariable regression models were used to investigate associations.
Results
Placental HPV was detected in 18/587 (3 %). Twenty-eight genotypes were identified among the 214/556 (38 %) with midgestational urine HPV. Seventeen of the 18 women with placental HPV were midgestational HPV positive with 89 % genotype concordance. Midgestational high-risk-(HR)-HPV and high viral loads of Any- or HR-HPV were associated with placental HPV. Persisting HPV infection from midgestation to delivery was not associated with placental HPV. Adverse pregnancy outcomes were seen in 2/556 (0.4 %) of women with placental HPV.
Discussion
In this general cohort of pregnant women, the prevalence of placental HPV was 3 %, and midgestational urinary HPV 38 %. High HPV viral load increased the risk for placental HPV infections. We observed no increased risk for adverse pregnancy outcomes in women with placental HPV.}
}
@article{SHEVCHENKO2025638,
title = {A firm in strategic reality: leveraging constraints as opportunities},
journal = {Strategy & Leadership},
volume = {53},
number = {6},
pages = {638-657},
year = {2025},
issn = {1087-8572},
doi = {https://doi.org/10.1108/SL-11-2024-0125},
url = {https://www.sciencedirect.com/science/article/pii/S1087857225000097},
author = {Liudmyla Shevchenko and Oleksiy Osiyevskyy and Yurii Umantsiv and Olga Marchenko},
keywords = {Firm concept, Strategic constraints, VUCA, Strategic evolution},
abstract = {Purpose
This study investigates the role of strategic constraints as a dynamic force in business strategy, emphasizing their potential to act as catalysts for innovation and growth in a volatile, uncertain, complex, and ambiguous (VUCA) environment. It seeks to redefine the perception of constraints, positioning them as integral to organizational evolution and competitive advantage.
Design/methodology/approach
Grounded in a synthesis of strategic management theories and case-based evidence, the paper explores the multifaceted nature of constraints, categorizing them into structural and complex forms. It integrates perspectives from systems theory, strategic management paradigms, and digital transformation literature to build a framework for leveraging constraints.
Findings
Strategic constraints, when proactively managed, can become sources of creativity, innovation, and adaptive capabilities. The study identifies methods for converting constraints into opportunities, including resource optimization, managerial innovation, and leveraging digital transformation. It underscores the importance of aligning organizational culture and competencies with evolving external pressures.
Originality/value
By reframing constraints as opportunities, this paper contributes to strategic management literature with a novel approach to navigating challenges in VUCA environments. It bridges classical strategic theories with contemporary issues such as digital evolution and artificial intelligence, providing actionable insights for practitioners and researchers.}
}
@article{QI2026103178,
title = {T2VEval: Benchmark dataset and objective evaluation method for T2V-generated videos},
journal = {Displays},
volume = {91},
pages = {103178},
year = {2026},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2025.103178},
url = {https://www.sciencedirect.com/science/article/pii/S014193822500215X},
author = {Zelu Qi and Ping Shi and Shuqi Wang and Chaoyang Zhang and Fei Zhao and Zefeng Ying and Da Pan and Xi Yang and Zheqi He and Teng Dai},
keywords = {Text-to-video, Quality assessment, Evaluation benchmark, Objective evaluation},
abstract = {Recent advances in text-to-video (T2V) technology, as demonstrated by models such as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the applicability and popularity of the technology. This progress has created a growing demand for accurate quality assessment metrics to evaluate the perceptual quality of T2V-generated videos and optimize video generation models. However, assessing the quality of text-to-video outputs remain challenging due to the presence of highly complex distortions, such as unnatural actions and phenomena that defy human cognition. To address these challenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset for text-to-video quality evaluation, which contains 148 textual prompts and 1,783 videos generated by 13 T2V models. To ensure a comprehensive evaluation, we scored each video on four dimensions in the subjective experiment, which are overall impression, text–video consistency, realness, and technical quality. Based on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for T2V quality evaluation. T2VEval assesses videos across three branches: text–video consistency, realness, and technical quality. Using an attention-based fusion module, T2VEval effectively integrates features from each branch and predicts scores with the aid of a large language model. Additionally, we implemented a divide-and-conquer training strategy, enabling each branch to learn targeted knowledge while maintaining synergy with the others. Experimental results demonstrate that T2VEval achieves state-of-the-art performance across multiple metrics.}
}
@article{SHAFQAT2024928,
title = {Big data analytics capabilities and leadership: catalysts of firm performance in telecommunications},
journal = {Business Process Management Journal},
volume = {31},
number = {3},
pages = {928-947},
year = {2024},
issn = {1463-7154},
doi = {https://doi.org/10.1108/BPMJ-06-2024-0458},
url = {https://www.sciencedirect.com/science/article/pii/S1463715424000062},
author = {Hira Shafqat and Baojian Zhang and Muhammad Ahmed and Muhammad Rizwan Ullah and Muhammad Zulfiqar},
keywords = {Dynamic capabilities, Talent capabilities, Top management attitude, Big data analytics, Firm performance},
abstract = {Purpose
The proliferation of big data analytics (BDA)-enabled tools and technologies has endowed organizations with the capacity to augment decision-making processes, optimize operational endeavors and foster innovation across diverse business domains. Consequently, BDA has been posited as a catalyst for enhanced customer relationship management, improved risk mitigation strategies and heightened operational efficiencies, all of which converge to augment overall firm performance. Thus, the purpose of this research is to introduce a conceptual framework aimed at explaining the influence of BDA capabilities on the performance of telecommunications firms in Pakistan. Additionally, it examines the potential mediating effect of talent capabilities and moderating effect of top management attitude on firm performance.
Design/methodology/approach
Data from a sample comprising 520 participants were collected via survey questionnaires. The study employed Partial Least Squares-Structural Equation Modeling to empirically evaluate the proposed model.
Findings
Results reveal a positive association between BDA technology and information capabilities with both BDA talent capabilities and firm performance. Furthermore, the analysis suggests that BDA talent capabilities mediate the relationship between BDA dynamic capabilities and firm performance, while top management attitude acts as a moderator, enhancing the relationship between BDA talent capabilities and firm performance.
Originality/value
There is a scarcity of research that has examined the relationship of BDA capabilities, top management attitude and firm performance. This study attempts to examine their interrelationships. First, it enhances the extant literature by elucidating the mediating role of BDA talent capabilities in the relationship between BDA technology and information capabilities and firm performance. Second, the study introduces a novel dimension by incorporating top management attitude as a moderator variable. This augmentation adds layers of complexity to comprehending BDA implementation dynamics, emphasizing leadership’s role in fostering an enabling environment for effective utilization of BDA capabilities.}
}
@article{QUTTAINAH2024,
title = {Cost, Usability, Credibility, Fairness, Accountability, Transparency, and Explainability Framework for Safe and Effective Large Language Models in Medical Education: Narrative Review and Qualitative Study},
journal = {JMIR AI},
volume = {3},
year = {2024},
issn = {2817-1705},
doi = {https://doi.org/10.2196/51834},
url = {https://www.sciencedirect.com/science/article/pii/S281717052400022X},
author = {Majdi Quttainah and Vinaytosh Mishra and Somayya Madakam and Yotam Lurie and Shlomo Mark},
keywords = {large language model, LLM, ChatGPT, CUC-FATE framework, cost, usability, credibility, fairness, accountability, transparency, and explainability, analytical hierarchy process, AHP, total interpretive structural modeling, TISM, medical education, adoption, guideline, development, health care, chat generative pretrained transformer, generative language model tool, user, innovation, data generation, narrative review, health care professional},
abstract = {Background
The world has witnessed increased adoption of large language models (LLMs) in the last year. Although the products developed using LLMs have the potential to solve accessibility and efficiency problems in health care, there is a lack of available guidelines for developing LLMs for health care, especially for medical education.
Objective
The aim of this study was to identify and prioritize the enablers for developing successful LLMs for medical education. We further evaluated the relationships among these identified enablers.
Methods
A narrative review of the extant literature was first performed to identify the key enablers for LLM development. We additionally gathered the opinions of LLM users to determine the relative importance of these enablers using an analytical hierarchy process (AHP), which is a multicriteria decision-making method. Further, total interpretive structural modeling (TISM) was used to analyze the perspectives of product developers and ascertain the relationships and hierarchy among these enablers. Finally, the cross-impact matrix-based multiplication applied to a classification (MICMAC) approach was used to determine the relative driving and dependence powers of these enablers. A nonprobabilistic purposive sampling approach was used for recruitment of focus groups.
Results
The AHP demonstrated that the most important enabler for LLMs was credibility, with a priority weight of 0.37, followed by accountability (0.27642) and fairness (0.10572). In contrast, usability, with a priority weight of 0.04, showed negligible importance. The results of TISM concurred with the findings of the AHP. The only striking difference between expert perspectives and user preference evaluation was that the product developers indicated that cost has the least importance as a potential enabler. The MICMAC analysis suggested that cost has a strong influence on other enablers. The inputs of the focus group were found to be reliable, with a consistency ratio less than 0.1 (0.084).
Conclusions
This study is the first to identify, prioritize, and analyze the relationships of enablers of effective LLMs for medical education. Based on the results of this study, we developed a comprehendible prescriptive framework, named CUC-FATE (Cost, Usability, Credibility, Fairness, Accountability, Transparency, and Explainability), for evaluating the enablers of LLMs in medical education. The study findings are useful for health care professionals, health technology experts, medical technology regulators, and policy makers.}
}
@article{WANG2025157118,
title = {Unlocking the gut-lung axis: Feixin decoction as a novel modulator in hypoxic pulmonary hypertension},
journal = {Phytomedicine},
volume = {146},
pages = {157118},
year = {2025},
issn = {0944-7113},
doi = {https://doi.org/10.1016/j.phymed.2025.157118},
url = {https://www.sciencedirect.com/science/article/pii/S0944711325007573},
author = {Fei-ying Wang and Jian Yi and Ling-ling Zhou and Jun-lan Tan and Xian-ya Cao and Chao Zhang and Jia-jing Wan and Lan Song and Ai-guo Dai},
keywords = {Hypoxic pulmonary hypertension, Gut microbiota, Gut-lung axis, Herbal medicine, Feixin decoction},
abstract = {Background
Feixin decoction (FXD) is an effective traditional Chinese medicine prescription for treating chronic pulmonary heart disease and hypoxic pulmonary hypertension (HPH), However, the pharmacological mechanism of FXD in preventing HPH remains unclear.
Purpose
This study aimed to evaluate the preventive and therapeutic effect of FXD on HPH and confirm the association between HPH, gut microbiota, and FXD.
Methods
Multiple in vivo animal models were used, including HPH rat models, microbiota depletion models, and fecal microbiota transplantation (FMT) models. The HPH phenotype was evaluated through: right heart catheterization for hemodynamic parameters, doppler echocardiography for cardiac function assessment, hematoxylin-eosin staining for histopathological examination, and immunofluorescence labeling for specific protein expression analysis. Concurrently, transmission electron microscopy was utilized to observe the ultrastructure of the intestinal barrier, combined with immunofluorescence to examine the distribution characteristics of tight junction proteins. To elucidate the mechanism by which HPH ameliorates gut microbiota dysbiosis and associated metabolites, the study integrated 16S rRNA sequencing for microbiota composition analysis, dual-platform untargeted metabolomics for differential metabolite screening, and targeted metabolomics for quantitative validation.
Results
FXD exhibited significant therapeutic effects in HPH rats, ameliorating pulmonary vascular remodeling, attenuating right ventricular hypertrophy, reducing systemic inflammation, and restoring intestinal barrier function. Additionally, FXD partially restored intestinal ecological balance by enriching beneficial species (Lactobacillus and Lactobacillus johnsonii) while reducing pathogenic genera (Escherichia-Shigella and Helicobacter rodentium). Concurrently, FXD treatment induced favorable metabolic alterations, characterized by elevated levels of beneficial metabolites including eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA), along with reduced concentrations of pro-inflammatory 5-hydroxytryptamine (5-HT). Gut microbiota depletion and fecal microbiota transplantation (FMT) studies established that FXD's therapeutic effects on HPH are mediated through gut microbiota modulation. Mechanistic investigations revealed that this protection likely involves inhibition of the TLR4/MyD88/NF-κB signaling pathway. In vitro studies further corroborated these findings, showing that FXD-enriched metabolites potently suppressed abnormal proliferation, migration and apoptosis in human pulmonary arterial smooth muscle cells (HPASMCs). Notably, EPA, the most significantly increased metabolite, specifically attenuates hypoxia-induced HPASMCs proliferation by interfering with the TLR4/MyD88/NF-κB signaling axis.
Conclusions
Our study confirms that FXD alleviates HPH by regulating gut microbiota and its associated metabolites and validates the potential of FXD as a gut microbiota modulator and an HPH treatment, thereby providing a new therapeutic strategy to improve treatment efficacy.}
}
@article{ALMALKI2025,
title = {Can GLP-1RAs redefine transplantation standard of care?},
journal = {Trends in Pharmacological Sciences},
year = {2025},
issn = {0165-6147},
doi = {https://doi.org/10.1016/j.tips.2025.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0165614725002032},
author = {Bassem A. Almalki},
keywords = {glucagon-like peptide-1 receptor agonists, transplantation, metabolic disorder},
abstract = {Metabolic derangements, particularly obesity and post-transplant diabetes mellitus, remain major challenges in solid organ transplantation, contributing to graft dysfunction and increased morbidity. Glucagon-like peptide-1 receptor agonists (GLP-1RAs) have emerged as promising agents due to their glucose-lowering, weight-reducing, and cardiorenal protective effects. Accumulating evidence supports their efficacy in improving glycemic control, reducing body weight, and potentially enhancing graft and patient survival across diverse transplant populations. Notably, GLP-1RAs exhibit a favorable safety profile, with minimal risk of drug interactions or rejection. Early data also suggest immunomodulatory and anti-inflammatory benefits. Moreover, newer dual and triple incretin agonists offer enhanced metabolic efficacy, potentially extending these benefits further. While long-term outcomes remain under investigation, GLP-1RAs represent a compelling therapeutic option that may reshape metabolic management paradigms in both pre- and post-transplant care.}
}
@article{GUYE2025124977,
title = {Design and modeling of hollow micropillars evaporator for thermal management in high heat flux applications: Numerical analysis},
journal = {Applied Thermal Engineering},
volume = {262},
pages = {124977},
year = {2025},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2024.124977},
url = {https://www.sciencedirect.com/science/article/pii/S1359431124026450},
author = {Kidus Guye and Vivek Manepalli and Baris Dogruoz and Damena Agonafer},
keywords = {Thermal management, Thin-film evaporation, Microscale evaporation, Data center cooling, High heat flux, Micropillar, Hydrophilic, Hydrophobic, Refrigerant},
abstract = {With the advancement of artificial intelligence (AI) applications, there is a growing demand for sophisticated and powerful chips. The adoption of these processors, along with increased power and bandwidth, highlights the necessity for efficient thermal management solutions. In this context, advanced two-phase thermal management systems have emerged as promising solutions. This study introduces a novel direct-to-chip evaporative cooling (DCEC) device that uses hollow micropillars integrated with a liquid delivery system to enable microscale evaporation of water and dielectric coolants in a fully actively pumped system. The device can operate in different evaporative regimes, characterized by a concave meniscus inside the micropillar’s hole or a droplet confined at the inner or outer edge of the micropillar, depending on the heating power, thermophysical properties of the coolant, and operating pressure. To characterize the heat transfer performance of the DCEC, we conduct a comprehensive numerical analysis, varying different operating and design parameters for water, R-1336mzz(Z), and Opteon™ 2P50. Our numerical results demonstrate device-level heat fluxes of 575 W/cm2, 413 W/cm2, and 365 W/cm2 for water, R-1336mzz(Z), and Opteon™ 2P50, respectively. The corresponding heat transfer coefficients are 2.88 × 105 W/m2 K, 2.07 × 105 W/m2 K, and 1.83 × 105 W/m2 K for water, R-1336mzz(Z), and Opteon™ 2P50, respectively. We also examine the performance of the device across various operating regimes. The results show that the device achieves 1.33× and 3.1× greater heat flux when a droplet is suspended at the outer edge of the hollow micropillar, compared to the stages with a concave or convex meniscus inside the micropillar’s hole. We here conduct a detailed pressure analysis to understand the hydrodynamic characteristics of the device and determine the required inlet pressure for different coolants, operating regimes, and heat loads. These inlet pressure conditions can be integrated into the design of a feedback loop system to mitigate system failures caused by flooding or dryout.}
}
@article{WANG2024107231,
title = {The structure, self-assembly and dynamics of lipid nanodiscs revealed by computational approaches},
journal = {Biophysical Chemistry},
volume = {309},
pages = {107231},
year = {2024},
issn = {0301-4622},
doi = {https://doi.org/10.1016/j.bpc.2024.107231},
url = {https://www.sciencedirect.com/science/article/pii/S0301462224000607},
author = {Beibei Wang and D. Peter Tieleman},
keywords = {Molecular modeling, Molecular dyncamics simulations, Nanodiscs, Membrane mimetics, Membrane proteins},
abstract = {Nanodisc technology is increasingly being used in structural, biochemical and biophysical studies of membrane proteins. The computational approaches have revealed many important features of nanodisc assembly, structures and dynamics. Therefore, we reviewed the application of computational approaches, especially molecular modeling and molecular dyncamics (MD) simulations, to characterize nanodiscs, including the structural models, assembly and disassembly, protocols for modeling, structural properties and dynamics, and protein-lipid interactions in nanodiscs. More amazing computational studies about nanodiscs are looked forward to in the future.}
}
@article{GUSTAVSON2024113,
title = {Brain reserve in midlife is associated with executive function changes across 12 years},
journal = {Neurobiology of Aging},
volume = {141},
pages = {113-120},
year = {2024},
issn = {0197-4580},
doi = {https://doi.org/10.1016/j.neurobiolaging.2024.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0197458024000927},
author = {Daniel E. Gustavson and Jeremy A. Elman and Chandra A. Reynolds and Lisa T. Eyler and Christine Fennema-Notestine and Olivia K. Puckett and Matthew S. Panizzon and Nathan A. Gillespie and Michael C. Neale and Michael J. Lyons and Carol E. Franz and William S. Kremen},
keywords = {Executive control, Brain age, Magnetic resonance imaging (MRI), Longitudinal studies, Neuropsychology},
abstract = {We examined how brain reserve in midlife, measured by brain-predicted age difference scores (Brain-PADs), predicted executive function concurrently and longitudinally into early old age, and whether these associations were moderated by young adult cognitive reserve or APOE genotype. 508 men in the Vietnam Era Twin Study of Aging (VETSA) completed neuroimaging assessments at mean age 56 and six executive function tasks at mean ages 56, 62, and 68 years. Results indicated that greater brain reserve at age 56 was associated with better concurrent executive function (r=.10, p=.040) and less decline in executive function over 12 years (r=.34, p=.001). These associations were not moderated by cognitive reserve or APOE genotype. Twin analysis suggested associations with executive function slopes were driven by genetic influences. Our findings suggest that greater brain reserve allowed for better cognitive maintenance from middle- to old age, driven by a genetic association. The results are consistent with differential preservation of executive function based on brain reserve that is independent of young adult cognitive reserve or APOE genotype.}
}
@article{HASAN2024110544,
title = {DC circuit breaker: A topology with regenerative current breaking capability for DC microgrid applications},
journal = {Electric Power Systems Research},
volume = {234},
pages = {110544},
year = {2024},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.110544},
url = {https://www.sciencedirect.com/science/article/pii/S0378779624004309},
author = {Md Mahmudul Hasan and Ramani Kannan and S.M. Muyeen},
keywords = {DC circuit breaker, Power system protection, Regenerative current breaking, Bidirectional, Power electronics},
abstract = {The modern DC grid is predominantly inductive due to several factors, including line inductance, and the use of current-limiting inductors to control fault current rising rates. These network inductances store a significant amount of energy during regular operation, which is either dissipated or freewheeled to ensure safe current breaking. This research article proposed a highly efficient bidirectional DC circuit breaker topology that not only provides safe current breaking but also effectively recovers the post-current breaking energy from the network’s inductance instead of dissipation. An extensive simulation investigation is performed in PSIM software to test the breaker capability for a 400V/400A DC system and the laboratory-scale 48V/1A experimental setup has been developed and employed for further validation purposes.}
}
@article{WASILEWSKI2025104012,
title = {Harnessing generative AI for personalized E-commerce product descriptions: A framework and practical insights},
journal = {Computer Standards & Interfaces},
volume = {94},
pages = {104012},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2025.104012},
url = {https://www.sciencedirect.com/science/article/pii/S0920548925000418},
author = {Adam Wasilewski},
keywords = {Generative AI, E-commerce, Personalization, Text similarity measures},
abstract = {The role of electronic commerce (e-commerce) in the global economy has been steadily increasing, highlighting the benefits of business digitization for flexibility and resilience in response to environmental changes. Among emerging trends, the integration of artificial intelligence (AI) and machine learning is particularly notable, especially the application of large language models to personalize user interactions throughout the customer journey. A promising future direction is the use of generative AI to create customized e-commerce product descriptions for personalized, multivariant user interfaces. To validate this approach, a framework and metrics are proposed to assess the impact of segment-specific information on generated text. This led to the positioning of AI-generated content within a multivariant user interface architecture and the adaptation of a cosine similarity measure to evaluate text differentiation. The findings confirmed that the specific characteristics of e-commerce customer clusters enable generative AI to produce significantly distinct product descriptions. While differences were not statistically significant in 26.7% of cases, full differentiation was achieved for descriptions of sufficient length.}
}
@article{WANG2025,
title = {The Impact of AI Teaching on Teaching Quality:},
journal = {International Journal of Web-Based Learning and Teaching Technologies},
volume = {20},
number = {1},
year = {2025},
issn = {1548-1093},
doi = {https://doi.org/10.4018/IJWLTT.376489},
url = {https://www.sciencedirect.com/science/article/pii/S1548109325000191},
author = {Jundan Wang},
keywords = {AI Teaching, Teaching Quality, Mediating Effect, AMOS},
abstract = {ABSTRACT
This study analyzed the impact of AI teaching on teaching quality, and revealed the mediating effect of student motivation and teacher expertise in the relationship of AI teaching and teaching quality.Based on the AI-TPACK theory, this study explored the impact of AI teaching on teaching quality and its mediating mechanism using questionnaires and AMOS structural equation modeling. The results show that AI teaching mainly improves teaching quality indirectly through enhancing student motivation and improving teacher expertise, rather than directly. This finding differs from the traditional view that AI teaching directly enhances teaching quality and emphasizes the important roles of students and teachers in AI teaching environments. The study provides some insights for educational policy makers and school administrators, pointing out that when implementing AI teaching strategies, emphasis should be placed on stimulating student learning motivation and teacher expertise growth in order to promote the optimization of teaching quality.}
}
@article{JU2024112542,
title = {Three-stage binarization of color document images based on discrete wavelet transform and generative adversarial networks},
journal = {Knowledge-Based Systems},
volume = {304},
pages = {112542},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112542},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124011766},
author = {Rui-Yang Ju and Yu-Shian Lin and Yanlin Jin and Chih-Chia Chen and Chun-Tse Chien and Jen-Shiun Chiang},
keywords = {Deep learning, Computer vision, Discrete wavelet transform, Generative adversarial networks, Document image processing, Document image enhancement, Document image binarization},
abstract = {The efficient extraction of text information from the background in degraded color document images is an important challenge in the preservation of ancient manuscripts. The imperfect preservation of ancient manuscripts has led to different types of degradation over time, such as page yellowing, staining, and ink bleeding, seriously affecting the results of document image binarization. This work proposes an effective three-stage network method to image enhancement and binarization of degraded documents using generative adversarial networks (GANs). Specifically, in Stage-1, we first split the input images into multiple patches, and then split these patches into four single-channel patch images (gray, red, green, and blue). Then, three single-channel patch images (red, green, and blue) are processed by the discrete wavelet transform (DWT) with normalization. In Stage-2, we use four independent generators to separately train GAN models based on the four channels on the processed patch images to extract color foreground information. Finally, in Stage-3, we train two independent GAN models on the outputs of Stage-2 and the resized original input images (512 × 512) as the local and global predictions to obtain the final outputs. The experimental results show that the Avg-Score metrics of the proposed method are 77.64, 77.95, 79.05, 76.38, 75.34, and 77.00 on the (H)-DIBCO 2011, 2013, 2014, 2016, 2017, and 2018 datasets, which are at the state-of-the-art level. The implementation code for this work is available at https://github.com/abcpp12383/ThreeStageBinarization.}
}
@article{LIU2025,
title = {Knowledge Enhanced Industrial Question-Answering Using Large Language Models},
journal = {Engineering},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2025.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S2095809925004527},
author = {Ronghui Liu and Hao Ren and Haojie Ren and Wu Rui and Wei Cui and Xiaojun Liang and Chunhua Yang and Weihua Gui},
keywords = {Retrieval augmented generation, Knowledge enhancement, Question answering, Large language models, Industrial knowledge automation},
abstract = {Modern industrial systems have grown increasingly extensive, complex, and hierarchical, with operations relying on numerous knowledge-based queries. These queries necessitate considerable human resources while also requiring high levels of accuracy, subjectivity, and consistency, all of which critically influence operational efficiency. To overcome these challenges, this study proposes an industrial retrieval-augmented generation (RAG) method designed to enhance large language models (LLMs) using domain-specific knowledge, thereby improving the precision of question answering. A comprehensive industrial knowledge base was constructed from diverse sources, including journal articles, theses, books, and patents. A Text classification model based on bidirectional encoder representations from transformers (BERTs) was trained to accurately classify incoming queries. Furthermore, the general text embedding–dense passage retrieval (GTE–DPR) model was employed to perform word embedding and vector similarity retrieval, facilitating the alignment of query vectors with relevant entries in the knowledge base to obtain initial responses. These initial results were subsequently refined by LLMs to produce accurate final answers. Experimental evaluations confirm the effectiveness of the proposed approach. In particular, when applied to ChatGLM2-6B, the RAG method increased the ROUGE-L score from 32.52% to 55.04% and improved accuracy from 50.52% to 73.92%. Comparable improvements were also observed with LLaMA2-7B, underscoring the RAG framework’s capability to significantly enhance the accuracy and relevance of industrial question-answering (QA) systems.}
}
@article{ZHANG2025102312,
title = {Algorithm to emotion: a three-stage model of user acceptance for AI-generated news platforms},
journal = {Telematics and Informatics},
volume = {101},
pages = {102312},
year = {2025},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2025.102312},
url = {https://www.sciencedirect.com/science/article/pii/S0736585325000747},
author = {Ke Zhang and Yuchen Xie and Wangjing Han and Dandan Zhang and Yan Quan},
keywords = {AI news recommendation, AIDUA model, User acceptance, Algorithmic transparency, Emotion mediation},
abstract = {With the rapid development of AI, Users’ acceptance of intelligent recommendation technology on news platforms directly affects the effectiveness and sustainability of using intelligent recommendation AI news. This study investigates user acceptance of AI-driven news recommendation platforms by applying the Artificial Intelligent Device Use Acceptance (AIDUA) model. Through a survey of 1,100 users, we examine how six AI-specific factors—social influence, perceived novelty, intelligence, accuracy, transparency, and fairness—shape performance expectancy and effort expectancy, ultimately influencing acceptance decisions. Results reveal that all six factors positively impact performance expectancy, while social influence, accuracy, transparency, and fairness reduce effort expectancy. Notably, perceived accuracy (β = 0.200) exerts the strongest effect, underscoring content quality as a critical driver of trust. Emotion mediates between cognitive evaluations and behavioral outcomes, with positive emotions enhancing acceptance and negative emotions amplifying resistance. The study advances theoretical understanding by extending the AIDUA model to AI journalism, highlighting the dual-path role of cognitive and affective evaluations.}
}
@article{SCHUMANN2025108646,
title = {Identifying distinct types of internet use that predict the likelihood of planning or committing a terrorist attack: Findings from an analysis of individuals convicted on terrorism(-related) charges in England and Wales},
journal = {Computers in Human Behavior},
volume = {168},
pages = {108646},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108646},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225000937},
author = {Sandy Schumann and Jonathan Kenyon and Jens Binder},
keywords = {Internet use, Terrorism, Radicalisation, Mobilisation, Risk assessment},
abstract = {Previous research has documented that the internet plays an increasingly important role in facilitating involvement in terrorism. However, the level of specificity of this literature is low. Advancing current insights, we examined how three concrete examples of active (i.e., generate/disseminate terrorist propaganda; interact with co-ideologues) and two examples of passive (i.e., learn about terrorist ideologies/actors; learn tactical information) internet use are related to distinct distal and proximal dynamics of radicalisation. Additionally, we assessed associations between the different types of internet use and the likelihood of having planned/committed a terrorist attack. We analysed a unique dataset based on closed-source risk assessment reports of individuals convicted of terrorism(-related) offences in England and Wales (N = 377). Results of this secondary data analysis pointed to three internet use repertoires: (1) learning about tactical information and terrorist ideologies/actors; (2) only learning about terrorist ideologies/actors; (3) active internet use and learning about terrorist ideologies/actors. Learning about tactical information and terrorist ideologies/actors was (compared to the other two repertoires) associated with a higher likelihood of having planned/committed an act of terrorism. Additionally, levels of capability were higher if individuals learnt both tactical and ideological information online compared to using the internet actively and browsing content about terrorist ideologies/actors. Individuals characterised by either internet use repertoire did, however, not vary significantly regarding their levels of engagement with extremist ideas and actors and the degree to which they had developed an extremist mindset. The results can inform terrorist/violent extremist risk assessment.}
}
@article{TAN2025,
title = {Visual perception-informed urban design toolkit: Computational urban morphology optimisation to inform real-time perceived safety},
journal = {Journal of Urban Management},
year = {2025},
issn = {2226-5856},
doi = {https://doi.org/10.1016/j.jum.2025.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S2226585625001426},
author = {Xinyu Tan and Qiwei Song and Xun Liu and Waishan Qiu},
keywords = {Perceived safety, Computational urban design, Urban morphology, Greenery, Real-time, Street-view visualisation},
abstract = {The perceived safety of street scenes significantly affects the travel behaviors and social interactions of urban dwellers, particularly females, which ultimately matters to the inclusiveness of cities. Despite efforts to utilize street view imagery (SVI) for auditing perceived safety in urban areas, most studies remain analytical, with limited integration into urban design's form-based ideation workflow. The reason is mechanism dependency: few designers use both analytics (Python) and form-based design (Rhino) tools fluently. The outcome should not be overlooked: when the humanistic pedestrian experience cannot be explicitly integrated into the design process, the planning results may further marginalise disadvantaged groups. To bridge the gap between urban analytics and design, we propose a computational framework that automates the evaluation of pedestrian-oriented perceived safety in real-time, linking form-based urban design features, particularly greenery, to visual safety perception within urban canyons. By integrating datasets such as Place Pulse 2.0 and environmental attributes, the perceived safety of different urban canyons with varying urban forms and streetscape configurations can be seamlessly updated using machine learning in Rhino Grasshopper. Our tests show that in areas with the same urban density and height-width ratio, specific greenery configurations, such as tree density and green coverage, significantly improve perceived safety. Subsequently, urban canyon forms are categorised based on perceived safety outcomes to provide urban design guidelines. Notably, real-time visualisations from Stable Diffusion are further incorporated to improve the Rhino-based framework's usability in real life. This computational framework integrates visual perceptions into form-based urban form ideation (especially greenery characteristics): it alleviates the politics of difference in urban design practice, supporting the facilitation of more inclusive public spaces.}
}
@article{CHEN2024128837,
title = {Recombinant bacteriophage T4 displaying key epitopes of the foot-and-mouth disease virus as a novel nanoparticle vaccine},
journal = {International Journal of Biological Macromolecules},
volume = {258},
pages = {128837},
year = {2024},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2023.128837},
url = {https://www.sciencedirect.com/science/article/pii/S0141813023057367},
author = {Cen Chen and Nan Zhang and Mengling Li and Aili Guo and Yifei Zheng and Farwa Humak and Ping Qian and Pan Tao},
keywords = {Foot-and-mouth disease virus, Bacteriophage T4, Virus-like particle vaccines, B- and T- cell epitopes},
abstract = {Foot-and-mouth disease virus (FMDV) is a highly contagious pathogen that has caused significant economic losses in the livestock industry. Peptide vaccines engineered with the protective epitopes of FMDV have provided a safer alternative for disease prevention than the traditional inactivated vaccines. However, the immunogenicity of the peptide is usually poor and therefore an adjuvant is required. Here, we showed that recombinant T4 phages displaying the B-cell epitope of the FMDV VP1 protein (VP1130–158), without additional adjuvants, induced similar levels of antigen-specific IgG1 but higher levels of IgG2a compared to the peptide vaccine. Incorporation of a CD4+ T cell epitope, either 3A21–35 of FMDV 3A protein or P2830–844 of tetanus toxoid, further enhanced the immunogenicity of VP1-T4 phage nanoparticles. Interestingly, the extrinsic adjuvant cannot enhance the immunogenicity of the nanoparticles, indicating the intrinsic adjuvant activities of T4 phage. Furthermore, the recombinant T4 phage can be produced on a large scale within a short period of time at a relatively low-cost using Escherichia coli, heralding its potential in the development of a safe and effective FMDV vaccine.}
}
@article{CASARRUBEA2024111034,
title = {Sex-dependent behavioral effects of chronic nicotine during adolescence evaluated in young adult rats tested in Hole-Board},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {133},
pages = {111034},
year = {2024},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2024.111034},
url = {https://www.sciencedirect.com/science/article/pii/S0278584624001027},
author = {Maurizio Casarrubea and Stefania Aiello and Giuseppe Crescimanno and Daniel Cassar and Zachary Busuttil and Fabiana Faulisi and Antonio Iacono and Giuseppe {Di Giovanni}},
keywords = {Nicotine, Hole-Board, Rats, Anxiety, Sex differences, T-pattern, TPA},
abstract = {As one of the leading causes of death and serious illnesses, tobacco smoking remains a significant issue in modern societies. Many individuals smoke during adolescence, a trend that has been exacerbated by the prevalence of vaping among young people. In this context, studying the behavioral effects induced by nicotine administration in male and female rats, during the adolescent period, assumes great importance because it can help to better understand the dynamics underlying tobacco use in the two sexes. For this purpose, we employed 4 groups of rats, 2 male and 2 female groups, chronically treated with saline or nicotine 3 mg/kg i.p. for 30 days, spanning from postnatal day 30 to postnatal day 60. Utilizing quantitative analyses and T-pattern detection and analysis, our findings revealed a complex and multifaceted behavioral reorganization in adolescent rats subjected to chronic nicotine administration. Specifically, we observed an increase of anxiety in males and a reduction in females. The distinctive structural changes, induced by chronic nicotine in both sexes, have significant implications, from a translational perspective, for studies on nicotine dependence disorders.}
}
@article{TIBAU2024101331,
title = {ChatGPT for chatting and searching: Repurposing search behavior},
journal = {Library & Information Science Research},
volume = {46},
number = {4},
pages = {101331},
year = {2024},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2024.101331},
url = {https://www.sciencedirect.com/science/article/pii/S0740818824000525},
author = {Marcelo Tibau and Sean Wolfgand Matsui Siqueira and Bernardo Pereira Nunes},
keywords = {ChatGPT, Large language models (LLMs), Searching as learning (SaL), Search tactics, Search strategy adaptation, Conversational information systems},
abstract = {Generative AI tools, exemplified by ChatGPT, are transforming the way users interact with information by enabling dialogue-based querying instead of traditional keyword searches. While this conversational approach can simplify user interactions, it also presents challenges in structuring effective searches, refining prompts, and verifying AI-generated content. This study addresses these complexities by repurposing traditional search tactics for use in conversational AI environments, specifically to support the Searching as Learning (SaL) paradigm. Forty-five adapted tactics are introduced to aid users in defining information needs, refining queries, and evaluating ChatGPT's responses for relevance, utility, and reliability. Using the Efficient Search Tactic Identification (ESTI) method and constant comparison analysis, these tactics were mapped into a stratified model with seven categories. The framework provides a structured approach for users to leverage conversational agents more effectively, promoting critical thinking and iterative learning. This research underscores the importance of developing robust search strategies tailored to conversational AI environments, facilitating deeper learning and reflective information engagement. Additionally, it highlights the need for ongoing research into the design and evaluation of future chat-and-search systems.}
}
@article{PRIY2025103909,
title = {Suppression of flow boiling instabilities in microchannel heat sinks using a passive vapor venting technique},
journal = {Thermal Science and Engineering Progress},
volume = {65},
pages = {103909},
year = {2025},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2025.103909},
url = {https://www.sciencedirect.com/science/article/pii/S2451904925007000},
author = {Akash Priy and Md. Danish Eqbal and Manabendra Pathak and Mohd.Kaleem Khan},
keywords = {Microchannel heat sink, Flow boiling instabilities, Vapor venting, Augmented manifold design, Bubble clogging},
abstract = {Flow boiling in microchannels involving phase-change heat transfer is a promising technique for high heat dissipation from electronic devices. However, two-phase heat transfer in microchannels is adversely affected by flow boiling instabilities. The present work explores a passive vapor venting technique featuring a superhydrophobic stainless steel mesh screen and a fluidic dampener to suppress flow boiling instabilities in microchannels. Experimental investigations have been made to study the flow and heat transfer characteristics in conventional microchannels, i.e., non-venting (NV) and microchannels with passive vapor venting (VV) configuration. Microchannels have been fabricated on a copper block, chosen for its high thermal conductivity, and enclosed within a Teflon housing to enhance thermal insulation. A superhydrophobic coating of Cytonix Fluropel is applied on the stainless steel mesh to make it superhydrophobic. Flow boiling experiments have been performed using deionized water, with mass fluxes ranging from 255-535 kg/m2s and heat fluxes from 10 to 220 W/cm2. An early evacuation of large vapor slugs in the VV configuration eliminates the vapor clogging and backflow, thus promoting frequent rewetting of the hotspots in the microchannels. The high amplitude and low frequency flow fluctuations in NV configurations are converted into low amplitude and high frequency fluctuations in the VV configuration. The temperature and pressure fluctuations are drastically reduced in the VV configuration, resulting in a 95 % higher heat transfer coefficient (HTC) than in the NV configuration. It also produces a 47 % reduction in pressure drop compared to the NV configuration.}
}
@article{LI2024124,
title = {Public data and corporate employment: Evidence from the launch of Chinese public data platform},
journal = {Economic Analysis and Policy},
volume = {84},
pages = {124-144},
year = {2024},
issn = {0313-5926},
doi = {https://doi.org/10.1016/j.eap.2024.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0313592624002121},
author = {Xin Li and Zhaoda Liu and Yongwei Ye},
keywords = {Public data, Digital infrastructure, Employment, Expansion effect},
abstract = {We investigate whether public data influences corporate employment. Our identification scheme treats the launch of Chinese public data platforms as a quasi-natural experiment and then conducts a difference-in-differences estimation. The results show that firms have hired more employees after opening public data platforms in the provinces where their offices are located, especially for companies in the mature stage, service industry companies, and companies located in regions with more advanced digital infrastructure and more developed financial system. Mechanism tests show that public data platform openness plays a role in corporate employment through the production scale expansion effect, the business scope expansion effect, and the digital asset investment effect. In addition, examining the two dimensions of data quality and platform construction quality, we find that the quality of public data openness has a significant effect on corporate employment. Our findings provide evidence for Chinese government to promote the openness of public data.}
}
@article{MA2024,
title = {A Machine Learning and Large Language Model-Integrated Approach to Research Project Evaluation},
journal = {Journal of Database Management},
volume = {35},
number = {1},
year = {2024},
issn = {1063-8016},
doi = {https://doi.org/10.4018/JDM.345400},
url = {https://www.sciencedirect.com/science/article/pii/S1063801624000142},
author = {Jian Ma and Zhimin Zheng and Peihu Zhu and Zhaobin Liu},
keywords = {Large Language Models, Machine Learning Algorithms, Peer Review Assessment, Research Project Evaluation},
abstract = {ABSTRACT
Research project evaluation upon completion is one of the important tasks for research management in government funding agencies and research institutions. Due to the increased number of funded projects, it is hard to find qualified reviewers in the same research disciplines. This paper proposes a machine learning and large language model integrated approach to provide decision support for research project evaluation. Machine learning algorithms are proposed to compute the weights of key performance indicators (KPIs) and scores of KPIs based on the evaluation results of completed projects, large language models are used to summarize research contributions or findings on project reports. Then domain experts are invited to consolidate the weights and scores for the KPIs and assess the novelty and impact of research contribution or findings. Experiments have been conducted in practical settings and the results have shown that the proposed method can greatly improve research management efficiency and provide more consistent evaluation results on funded research projects.}
}
@article{MORTATI2025101327,
title = {Data thickening: A new frontier in design cognition},
journal = {Design Studies},
volume = {100},
pages = {101327},
year = {2025},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2025.101327},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X25000390},
author = {Marzia Mortati and Cabirio Cautela},
keywords = {design cognition, design process(es), big data, thick data},
abstract = {The literature extensively discusses how designers utilize their cognitive abilities in the creative process, primarily leveraging context-specific, observational data. The vast availability of big data introduces new challenges to this, related to the integration of large datasets into the design process. Despite the rich body of research on design cognition, there has been a noticeable lack of studies addressing how different types of data influence design cognitive mechanisms. This article investigates how designers navigate heterogeneous data types (big/thin and small/thick) within the creative process, introducing the concept of “data thickening,” a cognitive mechanism through which designers delve into problems to uncover their essence and bridge the problem-space with the solution-space.}
}
@article{ZHENG20241,
title = {Combining wide seedling strip planting with a higher plant density results in greater yield gains in winter wheat},
journal = {Annals of Agricultural Sciences},
volume = {69},
number = {1},
pages = {1-10},
year = {2024},
issn = {0570-1783},
doi = {https://doi.org/10.1016/j.aoas.2024.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0570178324000010},
author = {Feina Zheng and Jinpeng Chu and Xinhu Guo and Xiu Zhang and Jing Ma and Mingrong He and Xinglong Dai},
keywords = {Winter wheat, Planting pattern, Plant density, Radiation interception, Specific green area nitrogen, Radiation use efficiency},
abstract = {Both increased plant density and wide seedling strip planting (WSP) can improve wheat grain yield. However, whether and how greater gains in grain yield can be achieved by combining WSP with an increased plant density is unclear. In this study, two winter wheat cultivars were subjected to three plant densities (lower, normal, and higher) and two planting patterns (conventional planting [CP] and WSP). The effects of plant density, planting pattern, and their combination on the solar radiation interception and conversion, biomass accumulation, harvest index, and grain yield were investigated. In response to an increase in plant density from lower to higher and a shift from CP to WSP and their combination, grain yield increased by 15.43 %, 10.85 % and 27.62 % for cultivar Taimai198, and by 13.13 %, 8.31 % and 22.41 % for Shannong30, respectively. The larger increases in grain yield were mainly ascribed to enhanced dry matter production, in particular after anthesis with no variation or a slight decline in the harvest index. The higher plant density was the dominant driver of the enhanced radiation interception, whereas WSP was mainly responsible for ameliorating the reduction in radiation use efficiency (RUE) caused by the higher plant density. The combined effects of these two management practices in increasing grain yield were much greater than the independent effects of a shift from CP to WSP or an increase in plant density. Optimizing the planting method may thus be a promising option for further improving grain yield of a densely planted wheat population by increasing the RUE.}
}
@article{LABUZ2026103060,
title = {Deep fakes as a tool of political advertising. Can regulatory framework benefit from the “Ship of Theseus” paradox?},
journal = {Technology in Society},
volume = {84},
pages = {103060},
year = {2026},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.103060},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X25002507},
author = {Mateusz Łabuz and Sławomir Soczyński},
keywords = {Deep fakes, Political advertising, Political manipulation, Ship of Theseus paradox, Epistemology},
abstract = {Deep fakes have been consistently perceived as a threat to social and political processes, particularly elections. A rarely discussed phenomenon, though gaining importance considering the increasing number of real-life use cases, is using deep fakes for political advertising purposes. By manipulating visualizations, artificial intelligence (AI) allows politicians to eliminate or minimize the deficits of their performance and highlight or create features perceived as attractive by the audience. Such manipulations pose a significant epistemic problem, and can severely distort perception of real politicians and their characteristics. These disruptions might correspond to the classic “Ship of Theseus” paradox that addresses the problem of changing identity. The aim of this study is to analyze how this philosophical conundrum could be used for crafting better transparency obligations (disclaimers) for using deep fakes for political advertising purposes. The aim of such disclaimers is to sensitize and protect public opinion against manipulation. However, at the moment the basic labels concentrate on technological dimension of manipulation, overlooking cognitive consequences for human audiences. This paper proposes extensive disclaimers to strengthen their informative and sensitizing potential.}
}
@article{2023100434,
title = {Table of Contents},
journal = {Ophthalmology Science},
volume = {3},
number = {4},
pages = {100434},
year = {2023},
issn = {2666-9145},
doi = {https://doi.org/10.1016/S2666-9145(23)00166-5},
url = {https://www.sciencedirect.com/science/article/pii/S2666914523001665}
}
@article{XU2024102518,
title = {Mapping the potential distribution of Asian elephants: Implications for conservation and human–elephant conflict mitigation in South and Southeast Asia},
journal = {Ecological Informatics},
volume = {80},
pages = {102518},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102518},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000608},
author = {Haixia Xu and Luguang Jiang and Ye Liu},
keywords = {Asian elephant, Maximum entropy, Environmental features, Population characteristics},
abstract = {Asian elephants play a pivotal role in their ecosystem. Understanding the potential distribution area of this species is vital for effective conservation efforts and mitigation of human-elephant conflicts. In this study, we used the maximum entropy to simulate the potential distribution area of Asian elephants across South and Southeast Asia, leveraging Maximum Entropy (MaxEnt) and presence data sourced from the Global Biodiversity Information Facility (GBIF). The analysis revealed that the potential distribution area of Asian elephants spans 530,418 km2 (10.59% of the study area), with significant potential distribution areas observed in Indonesia (136,890 km2) and Malaysia (119,497 km2). Vegetation type emerged as the dominant environmental factor influencing model outcomes, encompassing aspects such as broadleaved evergreen tree coverage, broadleaved deciduous closed tree coverage and EVI. The potential distribution area of Asian elephants overlaps with regions inhabited by 55.25 million people, with 6.07 million people residing in highly suitable habitats. India and Malaysia have high potential for human-elephant conflict (HEC) due to the high number of people living in potential and highly suitable habitats for elephants. Bangladesh and Nepal, on the other hand, have fewer people living in these habitats suitable for elephants, but they face relatively high human population density in these areas.}
}
@article{YAN2024109407,
title = {New objective Liutex vector based on an optimization procedure},
journal = {International Journal of Heat and Fluid Flow},
volume = {107},
pages = {109407},
year = {2024},
issn = {0142-727X},
doi = {https://doi.org/10.1016/j.ijheatfluidflow.2024.109407},
url = {https://www.sciencedirect.com/science/article/pii/S0142727X24001322},
author = {Bowen Yan and Yiqian Wang and Yifei Yu and Chaoqun Liu},
keywords = {Vortex identification, Liutex vector, Objectivity},
abstract = {The objectivity of a vortex identification method, which is critical in capturing vortices in rotating machineries, requires that the visualized vortical structures are independent of the movement of the observer. However, the common methodology to objectivize an Eulerian vortex identification method with relative/net vorticity or relative/net velocity gradient suffers from the issues of requirement to select a spatial volume to do vorticity average and inconsistency with the non-objective counterpart in an inertial reference frame. In this regard, a new objective Liutex vector method to visualize vortices is developed in the current study, which is based on the observed intermittency of Liutex represented vortices in turbulence. The intermittency indicates that a large volume fraction of the considered domain is void of Liutex, and the rotating motion of the observer is very likely to decrease this volume fraction. Therefore, an optimization procedure is proposed to approximate the rotating angular velocity of the observer by maximizing the zero Liutex volume percentage. With the known angular velocity to offset the rotational effects of the observer’s reference frame, objective version of Eulerian vortex identification methods can be introduced. As an example, the new objective Liutex method has been applied to turbulent channel flow at Reτ=180 in arbitrarily rotating coordinate systems, and the results show that the new method is both objective, i.e. independent of observer’s motion including translation and rotation and consistent with the original Liutex method in an inertial reference frame.}
}
@article{TAN2025,
title = {Exploring Gamified Computer-Assisted Instruction in College English Vocabulary Teaching},
journal = {International Journal of Web-Based Learning and Teaching Technologies},
volume = {20},
number = {1},
year = {2025},
issn = {1548-1093},
doi = {https://doi.org/10.4018/IJWLTT.390256},
url = {https://www.sciencedirect.com/science/article/pii/S1548109325000725},
author = {Yi Tan},
keywords = {Gamification, College English, Vocabulary Teaching, Computer-Assisted Instruction, Educational Technology},
abstract = {ABSTRACT
With advancements in technology, traditional college English vocabulary instruction faces challenges such as limited interactivity and student disengagement. Gamified computer-assisted instruction, which combines educational technology and psychology principles, has emerged as a solution. By incorporating game elements like points, task progression, and rewards, this approach creates an interactive and motivating learning environment. Based on constructivist and self-determination theories, this study tests a gamified framework using a self-developed platform. Results show that gamified tasks reduce resistance to learning, improve autonomous learning, and enhance collaboration. Compared to traditional methods, game mechanics increase language input/output and diversify learning approaches. This research offers practical and theoretical support for reforming English vocabulary instruction in higher education.}
}
@article{WU2025190,
title = {Examining the impacts of information and communication technology (ICT) on national development and wellbeing: A global perspective},
journal = {Journal of Economy and Technology},
volume = {3},
pages = {190-201},
year = {2025},
issn = {2949-9488},
doi = {https://doi.org/10.1016/j.ject.2024.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2949948824000593},
author = {Ming-Yi Wu},
keywords = {ICT, Broadband, National Development, GDP per capita, Wellbeing},
abstract = {The impacts of information and communications technology (ICT) on national development and wellbeing is a current research issue. By integrating four World Bank datasets and World Happiness Report’s global wellbeing dataset, this study analyzes the impacts of ICT on national development and wellbeing in 124 economies. There are several significant findings based on multiple regression analysis. First, Internet, mobile, and broadband subscription rates are significant predictors for logged GDP per capita. Second, broadband subscription rate is a significant predictor for perceptions of corruption. Third, Internet and broadband penetration rates are significant predictors for subjective wellbeing, social support and healthy life expectancy. Finally, fixed broadband and fixed telephone subscription rates are significant predictors for freedom to make life choices. Interestingly, fixed telephone subscription rate inversely predicts freedom to make life choices. The findings of this study bring updated insights into ICT impacts on national development and wellbeing around the world.}
}
@article{CHENG2025134134,
title = {Enhanced radar echo extrapolation for precipitation nowcasting quality using the convolutional Kolmogorov–Arnold networks},
journal = {Journal of Hydrology},
volume = {663},
pages = {134134},
year = {2025},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2025.134134},
url = {https://www.sciencedirect.com/science/article/pii/S0022169425014726},
author = {Qiming Cheng and Yihong Su and Yang He and Yang Wu and Fei Liu and Ye Rao and Yunsong Chao and Kaifeng Wang and Zhen Liu and Jun Liu and Yao Chen},
keywords = {Precipitation nowcasting, Deep learning, Kolmogorov-Arnold networks, Physical constraints, Radar echo extrapolation},
abstract = {With the ongoing climate warming, recurrent extreme rainfall events have become a pervasive global challenge. The integration of disaster warnings with precipitation nowcasting can effectively mitigate both human casualties and economic losses. Currently, deep learning techniques are widely employed for radar echo extrapolation as a primary approach to precipitation nowcasting. However, the lack of physical constraints often leads to blurriness in predicted images as the forecast time increases, ultimately resulting in a decline in forecast quality. In this study, we proposed an evolution network that incorporates Kolmogorov-Arnold networks (KANs) to extract physical motion information and enhance the network’s ability to learn such information. The results demonstrated that employing convolutional KANs (ConvKANs) as the fundamental module significantly reduced the number of model parameters while achieving superior performance. ConvKAN proved to be a highly effective foundational module for the evolution network, not only substantially reducing the number of model parameters and effectively improving specific meteorological metrics (CSI, POD) and the clarity metric of predicted images (Tenengrad), but also achieving the best performance in precipitation forecasting. Notably, our CUX2&evOnet (K2) model combining convolutional modules exhibited optimal performance. Compared to the CNN-based model named CUX2&evOnet(C32), it improved CSI, POD, and Tenengrad by 2.02%, 5.17%, and 2.52%, respectively, while requiring only 2.37% of its evolution network parameters. These findings confirm that deep learning models driven by the Kolmogorov-Arnold theorem (KAT) exhibit enhanced proficiency in precipitation nowcasting.}
}
@article{CHEN2024125170,
title = {Responsive luminescent silver-based metal-organic frameworks for highly sensitive and selective detection of hydrogen sulfide in biological system via a self-assembled headspace separation device},
journal = {Talanta},
volume = {267},
pages = {125170},
year = {2024},
issn = {0039-9140},
doi = {https://doi.org/10.1016/j.talanta.2023.125170},
url = {https://www.sciencedirect.com/science/article/pii/S0039914023009219},
author = {Zhongxiu Chen and Ling Li and Zhongshuai Zhao and Ying Zhu and Zhongde Liu},
keywords = {Luminescence silver-based metal-organic frameworks, Hydrogen sulfide, Headspace separation},
abstract = {As a highly toxic gas pollutant and also an endogenous gaseous signaling molecule existing in a variety of physiological processes, the rapid and accurate in-field detection of hydrogen sulfide is of great concern. Nevertheless, two drawbacks as for the optical probes for H2S detection, taking about a long time to reach the optical signal balance or the low selectivity, always exist. Herein, by using a highly photoluminescent and H2S-stimuli responsive silver-based metal-organic frameworks (MOFs): Ag-BDC (BDC = 1, 4-benzene dicarboxylate), we demonstrated that the luminescence intensity of Ag-BDC MOFs was inversely proportional to the concentration of H2S due to the Ag–S coordination and the obstruction of ligand-to-metal charge transfer (LMCT) transition process, and there was a quick response time of below 3.0 min. Combined with a simple customized device to separate H2S from the sample, the selectivity of the method for H2S detection could be greatly improved, and no interference would be caused even if the other sulfur-containing species coexisted. The luminescence probe presented a favorable sensitivity within a linear range of 0.1–1000 μM along with a detection limit of 23.7 nM. When employed to assay the endogenous sulfide level in the human serum and mouse brain tissue, the approach showed recoveries from 96.3% to 102% with relative standard derivation (RSD) less than 2.0%. By the integration of the responsive luminescent silver-based MOFs with a simple self-assembled headspace separation device, obviously the present strategy could be beneficial to the development and design of the in-field fast H2S measurement, possessing particular advantages in biological systems to eliminate the potential interferences.}
}
@article{HU2025104963,
title = {Artificial intelligence enabled tumor diagnosis and treatment: Status, breakthroughs and challenges},
journal = {Critical Reviews in Oncology/Hematology},
volume = {216},
pages = {104963},
year = {2025},
issn = {1040-8428},
doi = {https://doi.org/10.1016/j.critrevonc.2025.104963},
url = {https://www.sciencedirect.com/science/article/pii/S1040842825003518},
author = {Yani Hu and Shuai Liang and Xu Zhou and Zhipeng Zeng and Junli Ding and Dong Hua},
keywords = {Artificial intelligence, Machine learning, Tumor diagnosis, Cancer treatment},
abstract = {In recent years, cancer has emerged as a leading threat to global health, underscoring the critical need for early detection, precise diagnosis, and effective treatment to enhance patient outcomes. Against this backdrop, the rapid adoption of artificial intelligence (AI) in oncology has been driven by three key factors: the proliferation of large-scale datasets, breakthroughs in computational hardware and algorithms, and the development of innovative deep learning (DL) architectures. Specifically, AI applications now span the entire oncology workflow, including image-based screening, pathological diagnosis, intelligent decision support, treatment outcome prediction, and personalized therapy design. Hence, we aim to synthesize the recent explosive growth in AI applications across the oncology continuum and to critically examine the translational challenges that hinder clinical deployment. More importantly, this article provides a timely update on cutting-edge advancements (e.g., multimodal learning, explainable AI and AI-driven drug discovery) and offers a forward-looking perspective on future directions, consolidating actionable insights for researchers and clinicians and paving the way for next-generation AI-driven precision oncology.}
}
@article{MECZNER2024,
title = {Controlling Inputter Variability in Vignette Studies Assessing Web-Based Symptom Checkers: Evaluation of Current Practice and Recommendations for Isolated Accuracy Metrics},
journal = {JMIR Formative Research},
volume = {8},
year = {2024},
issn = {2561-326X},
doi = {https://doi.org/10.2196/49907},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X24003251},
author = {András Meczner and Nathan Cohen and Aleem Qureshi and Maria Reza and Shailen Sutaria and Emily Blount and Zsolt Bagyura and Tamer Malak},
keywords = {symptom checker, accuracy, vignette studies, variability, methods, triage, evaluation, vignette, performance, metrics, mobile phone},
abstract = {Background
The rapid growth of web-based symptom checkers (SCs) is not matched by advances in quality assurance. Currently, there are no widely accepted criteria assessing SCs’ performance. Vignette studies are widely used to evaluate SCs, measuring the accuracy of outcome. Accuracy behaves as a composite metric as it is affected by a number of individual SC- and tester-dependent factors. In contrast to clinical studies, vignette studies have a small number of testers. Hence, measuring accuracy alone in vignette studies may not provide a reliable assessment of performance due to tester variability.
Objective
This study aims to investigate the impact of tester variability on the accuracy of outcome of SCs, using clinical vignettes. It further aims to investigate the feasibility of measuring isolated aspects of performance.
Methods
Healthily’s SC was assessed using 114 vignettes by 3 groups of 3 testers who processed vignettes with different instructions: free interpretation of vignettes (free testers), specified chief complaints (partially free testers), and specified chief complaints with strict instruction for answering additional symptoms (restricted testers). κ statistics were calculated to assess agreement of top outcome condition and recommended triage. Crude and adjusted accuracy was measured against a gold standard. Adjusted accuracy was calculated using only results of consultations identical to the vignette, following a review and selection process. A feasibility study for assessing symptom comprehension of SCs was performed using different variations of 51 chief complaints across 3 SCs.
Results
Intertester agreement of most likely condition and triage was, respectively, 0.49 and 0.51 for the free tester group, 0.66 and 0.66 for the partially free group, and 0.72 and 0.71 for the restricted group. For the restricted group, accuracy ranged from 43.9% to 57% for individual testers, averaging 50.6% (SD 5.35%). Adjusted accuracy was 56.1%. Assessing symptom comprehension was feasible for all 3 SCs. Comprehension scores ranged from 52.9% and 68%.
Conclusions
We demonstrated that by improving standardization of the vignette testing process, there is a significant improvement in the agreement of outcome between testers. However, significant variability remained due to uncontrollable tester-dependent factors, reflected by varying outcome accuracy. Tester-dependent factors, combined with a small number of testers, limit the reliability and generalizability of outcome accuracy when used as a composite measure in vignette studies. Measuring and reporting different aspects of SC performance in isolation provides a more reliable assessment of SC performance. We developed an adjusted accuracy measure using a review and selection process to assess data algorithm quality. In addition, we demonstrated that symptom comprehension with different input methods can be feasibly compared. Future studies reporting accuracy need to apply vignette testing standardization and isolated metrics.}
}
@article{MA2025107688,
title = {Myth of the digital economy: Can it continually contribute to a low-carbon status and sustainable development?},
journal = {Environmental Impact Assessment Review},
volume = {110},
pages = {107688},
year = {2025},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2024.107688},
url = {https://www.sciencedirect.com/science/article/pii/S0195925524002750},
author = {Zihao Ma and Pingdan Zhang},
keywords = {Carbon abatement, Digital economy, Empirical analysis, STIRPAT model, Sustainable development},
abstract = {Facing global warming and looming catastrophic climate change, many countries have launched actions to change themselves and their societies to attain a low-carbon status. Digital economy purportedly offers a potential way to achieve sustainable development. However, this is still debated and a consensus is elusive. Here, we propose a novel nonlinear d curve hypothesis to describe the relationship between the digital economy and carbon emissions, whereby the digital economy can no longer contribute to carbon abatement once it is overdeveloped because of a rebound in energy consumption (i.e., the overdevelopment trap). To test this hypothesis, we used county-level panel data from China and conducted an empirical analysis with expanded STIRPAT models. Through a suite of robustness tests, we find evidence supporting our hypothesis, in that nearly 30 % of our samples (N = 1450 counties) had stepped into the overdevelopment trap, with this problem being most severe in eastern China. Altogether, we believe those countries relying heavily on thermal power—and more likely to suffer from a rebound in fossil energy consumption—should take a more cautious attitude towards implementing their digital economy and consider other ways to meet their carbon abatement goals.}
}
@incollection{GRUSH2026xlvii,
title = {Introduction},
editor = {Bern Grush and John S. Niles and Andrew Miller},
booktitle = {The End of Driving (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {xlvii-lxvi},
year = {2026},
isbn = {978-0-443-22392-1},
doi = {https://doi.org/10.1016/B978-0-443-22392-1.02001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443223921020010},
author = {Bern Grush and John S. Niles and Andrew Miller}
}
@article{ABDELWAHAB2025e356,
title = {Artificial intelligence in nursing education: a bibliometric analysis of trends, challenges, and future directions},
journal = {Teaching and Learning in Nursing},
volume = {20},
number = {2},
pages = {e356-e367},
year = {2025},
issn = {1557-3087},
doi = {https://doi.org/10.1016/j.teln.2024.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S1557308724002506},
author = {Siddig Ibrahim Abdelwahab and Manal Mohamed Elhassan Taha and Abdullah Farasani and Ahmed Ali Jerah and Saleh M Abdullah and Ieman A. Aljahdali and Bassem Oraibi and Hassan Ahmad Alfaifi and Amal Hamdan Alzahrani and Omar Oraibi and Yasir Babiker and Waseem Hassan},
keywords = {AI, Bibliometric Analysis, Nursing, Research Focus, Scopus},
abstract = {Objective
This study aims to perform, for the first time, a comprehensive bibliometric analysis of artificial intelligence (AI), machine learning, and ChatGPT in nursing education.
Methods
The Scopus database was employed to retrieve data and later analyzed on Vosviewer and R Studio,
Results
A total of 101 documents were analyzed, spanning from 1991 to 2024, sourced from 64 different journals. The annual growth rate of the documents was 11.07%, with an average document age of 2.28 years. The average number of citations per document was 8.90, and each document had an average of 4.53 co-authors. Among the authors, Ahn J contributed to 3 documents, the leading institution was the National University of Singapore with 14 publications, the United States had the highest number of publications with 43. Co-words analysis categorized the focus of 101 papers into 20 distinct groups.
Conclusion
The study provides an extensive overview of research trends in AI and machine learning within nursing education.}
}
@article{HUANG2025105310,
title = {Modeling student teachers’ self-regulated learning of complex professional knowledge: A sequential and clustering analysis with think-aloud protocols},
journal = {Computers & Education},
volume = {233},
pages = {105310},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105310},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000788},
author = {Lingyun Huang and Ying Zhan and Shen Ba},
keywords = {Self-regulated learning, Technological Pedagogical Content Knowledge, Sequential clustering analysis, Tthink-aloud protocols},
abstract = {There has been much discussion regarding the positive relationship between self-regulated learning (SRL) and technological pedagogical content knowledge (TPACK) development for student teachers. This study continued this claim and adopted advanced analytical methods to explain how SRL influences TPACK learning. Think-aloud protocols from 39 participants were collected and transcribed when they were learning TPACK by designing technology-infused lessons with nBrowser, a computer-based learning environment. Based on models, nine critical SRL events were retrieved from participants‘ think-aloud protocols and analyzed through sequential clustering analysis. The results show two SRL groups indicating distinct self-regulatory sequential patterns. One group had a shorter sequence length and dominantly enacted elaboration activities (Low-SRL group), while the other had longer sequence lengths and engaged in diverse SRL activities (High-regulation group). Relating to TPACK performance indicated by the quality of lesson plans, the results reveal that the participants in the High-SRL group outperformed their counterparts in the Low-SRL group. The findings are consistent with previous evidence and provide implications for practitioners about the importance of student teachers’ self-regulation trajectories.}
}
@article{ZHAO2025105660,
title = {Automated information mining in hazardous chemical accident reporting: An improved deep learning approach},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {97},
pages = {105660},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2025.105660},
url = {https://www.sciencedirect.com/science/article/pii/S0950423025001184},
author = {Kai Zhao and Lining Wan and Xilei Lu and Jun Zhao and Fei Chen and Miao He and Jinhao Gao and Qibo Wang and Linlin Zhang and Li Zhang},
keywords = {Accident reports, Named entity recognition, Topic mining, Pre-trained, Hazardous chemicals, Safety},
abstract = {Mining and accumulating lessons learned from incident reports, accurately identifying and extracting accident knowledge, can help managers recognize patterns, analyze common attributes, and thus prevent the recurrence of similar incidents. Due to the limitations of unstructured or semi-structured text records, current hazardous chemical accident research heavily relies on expert evaluations, often resulting in inefficiency, lack of intelligence, and subjectivity. To overcome this limitation, we propose an accident analysis framework that combines deep learning-based Named Entity Recognition (NER) and topic mining algorithms. First, we employ a RoBERTa-BiLSTM-Attention-CRF method based on a pre-trained model to automatically recognize and extract information from accident reports according to predefined accident entities. Second, we construct a dictionary specific to the hazardous chemicals domain and apply the Latent Dirichlet Allocation (LDA) model to analyze the causes of accidents within the “CAUSE” entity, thereby deriving the hazardous themes implied in the accidents. Finally, we validate the model on a self-constructed hazardous chemical named entity recognition incident report dataset (HCNER). The research findings indicate that the methodology proposed in this study effectively addresses the challenges associated with dispersed textual information, specialized terminology, and data formatting in the digital processing of hazardous chemical incidents. This advancement propels the development within the domain of hazardous chemical safety management and serves as a viable approach for investigating the causes of such incidents.}
}
@article{ABIDI20252883,
title = {Creative self-efficacy and innovative behaviour amidst the digital deluge: unravelling their nexus with autonomy and workplace civility},
journal = {International Journal of Productivity and Performance Management},
volume = {74},
number = {8},
pages = {2883-2907},
year = {2025},
issn = {1741-0401},
doi = {https://doi.org/10.1108/IJPPM-05-2024-0319},
url = {https://www.sciencedirect.com/science/article/pii/S1741040125000275},
author = {Oualid Abidi and Mirna Safi and Hamsa Sarhan and Rupali (Behl) Bhagat},
keywords = {Creative self-efficacy, Innovative behaviour, Information overload, Communication overload, Work autonomy, Workplace civility},
abstract = {Purpose
The coronavirus disease 2019 outbreak has accelerated the pace of digital transformation, prompting a significant number of businesses to transition swiftly to online operations and remote sales, thus bringing an increase in the information and communication workload of employees. The potential negative psychological effects on individuals facing heightened job demands have been understudied. Thus, this study investigates digital transformation from the perspective of knowledge workers who contend with amplified information flows in their professional roles. Following the job demands-resources (JD-R) model, this study explores the relationship between information and communication overload and innovative behaviours of knowledge workers, mediated by creative self-efficacy.
Design/methodology/approach
This study utilises an empirical research framework to collect data from a sample of 277 knowledge workers in Kuwait. Structural equation modelling was employed to examine and confirm the hypotheses of the research model.
Findings
Our findings demonstrate that communication overload significantly enhances knowledge workers’ creative self-efficacy, thereby fostering innovative behaviour. Work autonomy negatively moderates the relationship between communication overload and innovative behaviour.
Originality/value
This study broadens the conceptualisation of job resources in the JD-R model to encompass workplace civility, thus providing insights for organisational policymakers on the overlooked individual psychological dimensions impacting knowledge workers’ engagement in digital transformation initiatives.}
}
@article{WANG2026104484,
title = {Collaborative multicenter delivery and pickup network design with transportation resource configuration and alliance synergy service},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {205},
pages = {104484},
year = {2026},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2025.104484},
url = {https://www.sciencedirect.com/science/article/pii/S1366554525005253},
author = {Yong Wang and Siyu Luo and Lu Zhen},
keywords = {Multicenter vehicle routing problem, Delivery and pickup demands, Alliance synergy degree, Transportation resource configuration, Multiobjective adaptive large neighborhood search},
abstract = {With the rapid expansion of online shopping platforms, customer demand for parcel delivery and pickup has surged. Logistics alliances, formed through cooperation and resource configuration, offer a more efficient, cost-effective, and sustainable approach for logistics enterprise operations and multicenter logistics network development. However, effectively coordinating synergy services and scheduling transportation resources among alliance members to integrate delivery and pickup activities especially when faced with large-scale and independent demand remains a significant challenge. This study addresses this challenge by proposing and solving a collaborative multicenter delivery and pickup network design problem that incorporates transportation resource configuration and alliance synergy services. First, a measurement function is developed to quantify the alliance synergy degree. Then, a tri-objective optimization model is formulated to maximize alliance synergy while minimizing total operating costs and the number of vehicles. To solve this model, a novel hybrid algorithm is proposed, combining a Gaussian mixture clustering algorithm with an improved multiobjective adaptive large neighborhood search algorithm. This algorithm incorporates multiple removal and insertion operators, as well as an operator weight adaptive adjustment procedure, to improve both global and local search capabilities. Additionally, a transportation resource configuration strategy and an improved α-dominance solution update mechanism are embedded to facilitate vehicle sharing and maintain high-quality Pareto optimal solutions. The effectiveness of the proposed algorithm is demonstrated through comparisons with the CPLEX solver for small-scale problems and with multiobjective genetic algorithm, multiobjective ant colony optimization, and non-dominated sorting genetic algorithm-III for medium-to-large problems. Furthermore, a real-world case study conducted in Chongqing City, China, indicates that the proposed approach can optimize alliance formulation, design open-closed delivery and pickup routes, and enhance synergy and service efficiency. Finally, the effects of different alliance combinations, synergy services, and transportation resource configuration strategies on network optimization are systematically analyzed. This study provides an effective methodology for addressing the multicenter delivery and pickup network design under complex alliance collaboration and resource configuration scenarios, offering valuable insights for improving cost efficiency, resource utilization, and synergy in urban logistics networks.}
}
@article{2025A124,
title = {2025 Abstract Subject Index},
journal = {Journal of the Academy of Nutrition and Dietetics},
volume = {125},
number = {10, Supplement },
pages = {A124-A132},
year = {2025},
note = {2025 Food & Nutrition Conference & Expo},
issn = {2212-2672},
doi = {https://doi.org/10.1016/S2212-2672(25)00609-4},
url = {https://www.sciencedirect.com/science/article/pii/S2212267225006094}
}
@article{RAMMO20241104,
title = {Towards a Change-Specific and Company-Individual Manufacturing Change Management},
journal = {Procedia CIRP},
volume = {130},
pages = {1104-1109},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.213},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124013702},
author = {Jan-Philipp Rammo and Michael F. Zäh},
keywords = {Concept, Change Management, Methods, Digital Tools, Process Flexibility},
abstract = {Manufacturing companies are challenged by various influencing factors in the volatile environment, which requires frequent adjustments to their factories, known as Manufacturing Changes (MCs). The wide variety of changes and the company-individual implementation of Manufacturing Change Management (MCM) processes in practice represent a significant challenge for research, as general approaches are often too rigid and cannot be adequately integrated into the individual structures of companies. Existing MCM processes in the industry often lack flexibility, methodological, and digital support. This contribution proposes a change-specific and company-individual approach to address these challenges. The four-step methodology includes steps for describing MCM processes, creating a systematic change description model, identifying and structuring methods and digital tools, and correlating attributes through a Delphi study. An overall application methodology supports companies in using the methodology and serves as a basis for the final validation.}
}
@article{MOORHOUSE2024103290,
title = {The effects of generative AI on initial language teacher education: The perceptions of teacher educators},
journal = {System},
volume = {122},
pages = {103290},
year = {2024},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2024.103290},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X24000721},
author = {Benjamin Luke Moorhouse and Lucas Kohnke},
keywords = {Generative AI, Initial language teacher education, ChatGPT, Teacher educators},
abstract = {Since the public release of ChatGPT in November 2022, generative AI tools—capable of creating human-like content such as audio, code, images, text, simulations, 3D objects, and videos—have gained significant attention. While the impact of these tools on language teaching and learning has been widely speculated, the perspective of language teacher educators concerning their influence on initial language teacher education (ILTE) remains unexplored. This study investigates how teacher educators, who play a crucial role in adapting ILTE to technological advancements, perceive the effects of generative AI tools on ILTE. Data were collected through in-depth interviews with thirteen English language teacher educators from all four Hong Kong government-funded universities offering ILTE. Findings reveal that participants believe generative AI tools will substantially affect the ILTE curriculum, instruction, and assessment. However, most participants believed they lacked the confidence and competence to address the implications of generative AI tools effectively. This study highlights the need for further research and training to support teacher educators in adapting ILTE to the emerging influence of generative AI.}
}
@article{GHOSH2024185,
title = {Less is more: A minimalist approach to robust GAN-generated face detection},
journal = {Pattern Recognition Letters},
volume = {179},
pages = {185-191},
year = {2024},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2024.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167865524000485},
author = {Tanusree Ghosh and Ruchira Naskar},
keywords = {Fake image detection, Deepfake detection, GAN forensics, Digital image forensics, Synthetic image detection, GAN-face detection},
abstract = {Hyper-realistic images that are not differentiable from authentic images to regular viewers have become extremely easy to generate and highly accessible. Furthermore, the increasing pervasiveness of social media networks in our daily lives has facilitated the easy dissemination of fake news accompanied by such synthetic images. Hyper-realistic artificial face images are often illicitly used as profile pictures on social media sites, further using such profiles to spread fabricated information, resulting in social perils. Most available synthetic image detectors are challenging to implement in practical scenarios due to their high complexity and performance degradation for images from Online Social Networks (OSNs). In this work, we develop a deep learning-based lightweight synthetic image detector called Relative Chrominance Distance Network (RCD-Net). In this paper, we introduce the RCD image feature set for the first time, which gives a pair-wise chrominance component-based distance measure. To show its effectiveness, we explore multiple luminance-chrominance spaces. Compared to the state-of-the-art (SOTA), our model hugely reduces the network parameter requirements, making it incredibly lightweight. We also study the robustness of the proposed solution against common post-processing operations in the context of online social media networks. Experimental results prove that the proposed solution achieves SOTA performance at a much lower complexity than available solutions.}
}
@article{CICEK2025100310,
title = {A comparative review of user acceptance factors for drones and sidewalk robots in autonomous last mile delivery},
journal = {Green Energy and Intelligent Transportation},
volume = {4},
number = {4},
pages = {100310},
year = {2025},
issn = {2773-1537},
doi = {https://doi.org/10.1016/j.geits.2025.100310},
url = {https://www.sciencedirect.com/science/article/pii/S277315372500060X},
author = {Didem Cicek and Burak Kantarci and Sandra Schillo},
keywords = {Last mile innovation, Technology acceptance, Autonomous delivery, Literature review},
abstract = {Autonomous delivery technologies play a pivotal role in meeting the high expectations of customers while addressing the sustainability challenges posed by last-mile delivery traffic, particularly in urban areas. Over the past five years, research on user acceptance of these groundbreaking technologies has surged. This paper represents the first comprehensive review that consolidates and compares user acceptance factors related to deliveries by drones and sidewalk robots, drawing from global questionnaire-based studies. Our research reveals some common factors that consistently influence user acceptance for both drone and sidewalk robot deliveries and also sheds light on technology-specific acceptance factors. However, it's important to recognize that some of these factors may vary depending on the demographics and location of the studies conducted. Our findings intend to provide managerial insights to technology and policy makers, enabling strategic planning for the adoption of these innovative technologies.}
}
@article{MAN2024121869,
title = {Twenty-year responses of aspen stands to forest tent caterpillar defoliation and overstory dieback in Northeastern Ontario, Canada},
journal = {Forest Ecology and Management},
volume = {561},
pages = {121869},
year = {2024},
issn = {0378-1127},
doi = {https://doi.org/10.1016/j.foreco.2024.121869},
url = {https://www.sciencedirect.com/science/article/pii/S0378112724001816},
author = {Rongzhou Man},
keywords = {Defoliation and decline, Trembling aspen, Stand dynamics, Boreal mixedwood succession},
abstract = {Forest tent caterpillar (Malacosoma disstria Hbn.) is a major forest defoliator in North American boreal forests. This pest periodically affects hardwood trees such as trembling aspen (Populus tremuloides Michx.) and balsam poplar (Populus balsamifera L.) across large areas, causing mortality and altering stand attributes and long-term dynamics. In this study, I quantified the responses of aspen stands to forest tent caterpillar defoliation and overstory dieback, including short-term (5 years after dieback) and long-term (20 years later) dynamics. Results indicate that affected stands fully recovered to pre-dieback density but not basal area. Defoliation caused overstory dieback that stimulated understory growth and regeneration but the hardwood regeneration (mostly aspen) did not adequately replace hardwood trees lost to defoliation, resulting in decreased hardwood composition relative to unaffected stands. The high density in affected stands suggested possible further basal area recovery as regeneration and released understory conifers grew into main canopy, following the general trends of boreal mixedwood succession. The results support earlier projections based on residual stand attributes shortly after dieback and reported boreal species growth and mortality rates that indicate forest tent caterpillar defoliation and subsequent overstory dieback would accelerate aspen stand transition to conifer dominance and delay the availability of stands for harvesting by 40–50 years as stands recover.}
}
@article{FANG2024106230,
title = {Source-free unsupervised domain adaptation: A survey},
journal = {Neural Networks},
volume = {174},
pages = {106230},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106230},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024001540},
author = {Yuqi Fang and Pew-Thian Yap and Weili Lin and Hongtu Zhu and Mingxia Liu},
keywords = {Domain adaptation, Source-free, Unsupervised learning, Survey},
abstract = {Unsupervised domain adaptation (UDA) via deep learning has attracted appealing attention for tackling domain-shift problems caused by distribution discrepancy across different domains. Existing UDA approaches highly depend on the accessibility of source domain data, which is usually limited in practical scenarios due to privacy protection, data storage and transmission cost, and computation burden. To tackle this issue, many source-free unsupervised domain adaptation (SFUDA) methods have been proposed recently, which perform knowledge transfer from a pre-trained source model to the unlabeled target domain with source data inaccessible. A comprehensive review of these works on SFUDA is of great significance. In this paper, we provide a timely and systematic literature review of existing SFUDA approaches from a technical perspective. Specifically, we categorize current SFUDA studies into two groups, i.e., white-box SFUDA and black-box SFUDA, and further divide them into finer subcategories based on different learning strategies they use. We also investigate the challenges of methods in each subcategory, discuss the advantages/disadvantages of white-box and black-box SFUDA methods, conclude the commonly used benchmark datasets, and summarize the popular techniques for improved generalizability of models learned without using source data. We finally discuss several promising future directions in this field.}
}
@article{ZHAO2024109536,
title = {Modification of pectin with high-pressure processing treatment of fresh orange peel before pectin extraction: Part II. The effects on gelling capacity and emulsifying properties of pectin},
journal = {Food Hydrocolloids},
volume = {149},
pages = {109536},
year = {2024},
issn = {0268-005X},
doi = {https://doi.org/10.1016/j.foodhyd.2023.109536},
url = {https://www.sciencedirect.com/science/article/pii/S0268005X23010822},
author = {Wei Zhao and Yixiang Xu and Christina Dorado and Jinhe Bai and Hoa K. Chau and Arland T. Hotchkiss and Madhav P. Yadav and Randall G. Cameron},
keywords = {Pectin gelling property, Calcium-mediated pectin gelation, Sugar-acid-mediated pectin gelation, Rheological analysis, Pectin emulsifying property, Structure-function relationship},
abstract = {The most important functionalities of pectin are gelling and emulsifying properties, which can be tailored by modifying pectin structure through chemical/enzymatic treatments or physical processes. In a companion article (Zhao et al., 2023), the modification of pectin structure with high-pressure processing (HPP) was reported. In this study, the functionalities of the HPP-modified pectins (Hp) were evaluated and compared with commercial low methoxy (LM) and high methoxy (HM) pectins. The calcium sensitivity of Hp was comparable to that of commercial LM pectin, which was remarkably higher than the control pectin extracted from untreated peel and commercial HM pectin. Hp had a dramatically higher gelling capacity for calcium-mediated gelation than the control and commercial HM pectin. The strength and viscoelastic properties of Hp-calcium gels were comparable to that of commercial LM pectin. Meanwhile, most of the Hp also showed a comparable gelling capacity for sugar-acid-mediated gelation to that of the control and commercial HM pectin. Hp also had higher emulsifying stability than the control and commercial pectins. The increased capacity of Hp for calcium-mediated gelation was attributed to the block-wise distribution of non-esterified galacturonic acid (GalA) introduced by the HPP treatment, and the higher emulsifying stability was mainly attributed to the debranching of pectin. The study further confirmed that pectin functionalities were highly correlated with its structural properties. The data reveal the great potential of producing low-cost, high-quality pectins with increased gelling capacity and a broadened scope of applications, by adding a simple HPP pretreatment of fresh source material for pectin extraction.}
}
@article{YIN2025135,
title = {A Bayesian network meta-analysis of EGFR-tyrosine kinase inhibitor treatments in patients with EGFR mutation-positive non-small cell lung cancer},
journal = {Cancer Pathogenesis and Therapy},
volume = {3},
number = {2},
pages = {135-146},
year = {2025},
issn = {2949-7132},
doi = {https://doi.org/10.1016/j.cpt.2024.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S2949713224000454},
author = {Jianqiong Yin and Jing Huang and Min Ren and Rui Tang and Linshen Xie and Jianxin Xue},
keywords = {Non-small cell lung cancer, EGFR-TKI, Network meta-analysis, Survival, Toxicity},
abstract = {Background
To date, no direct comparisons have been performed to compare the effectiveness of all epidermal growth factor receptor-tyrosine kinase inhibitors (EGFR-TKIs) against EGFR mutation-positive non-small cell lung cancer (NSCLC). This study aimed to investigate the efficacy and safety of EGFR-TKIs in patients with EGFR mutation-positive NSCLC.
Methods
We conducted a network meta-analysis of randomized controlled trials comparing osimertinib, lazertinib, aumolertinib, befotertinib, furmonertinib, dacomitinib, afatinib, erlotinib, gefitinib, icotinib, and chemotherapy. Pooled estimations of progression-free survival (PFS), overall survival (OS), objective response rate (ORR), and toxicity (grade ≥ 3 adverse events) were performed within the Bayesian framework.
Results
Twenty-three trials involving 11 treatments were included. All EGFR-TKIs improved PFS relative to chemotherapy, except for icotinib (hazard ratio [HR] = 0.61, 95% confidence interval [CI]: 0.26–1.44). All EGFR-TKIs demonstrated significant ORR benefits over chemotherapy. Osimertinib seemed to prolong PFS compared with icotinib (HR = 0.29, 95% CI: 0.1–0.86), gefitinib (HR = 0.39, 95% CI: 0.21–0.74), and erlotinib (HR = 0.53, 95% CI: 0.29–1.0). In addition, osimertinib showed favorable superiority in improving OS compared with chemotherapy (HR = 0.6, 95% CI: 0.43–0.82), gefitinib (HR = 0.61, 95% CI: 0.45–0.83), erlotinib (HR = 0.65, 95% CI: 0.48–0.89), and afatinib (HR = 0.65, 95% CI: 0.44–0.94). Among these regimens, afatinib showed the highest ORR (cumulative probability: 96.96%). Icotinib was associated with minimal toxicity among the EGFR-TKIs, followed by furmonertinib and osimertinib. Moreover, the toxicity spectra differed among the EGFR-TKIs. Subgroup analyses of patients with two common types of EGFR mutations indicated that furmonertinib possessed the greatest PFS benefit in patients with exon 19 deletion, and lazertinib showed the greatest PFS benefit in patients with Leu858Arg mutation. We also identified differences between EGFR-TKIs in prolonging PFS in patients with brain metastasis.
Conclusions
Osimertinib is the first choice of treatment with considerable efficacy and safety for EGFR mutation-positive NSCLC. The treatments associated with the best PFS in patients with exon 19 deletions and Leu858Arg mutations were furmonertinib and lazertinib, respectively.}
}
@article{GUO2024103716,
title = {Application status of variable-frequency drive in hydrogen fuel cell air compressors from an industrial viewpoint: A review},
journal = {Sustainable Energy Technologies and Assessments},
volume = {64},
pages = {103716},
year = {2024},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2024.103716},
url = {https://www.sciencedirect.com/science/article/pii/S2213138824001127},
author = {Xiaoqiang Guo and Xiaolei Hu and Shiqi Zhang},
keywords = {Hydrogen fuel cell, Hydrogen compressor, Variable-frequency drive, Industrial application, Energy-saving},
abstract = {As an essential accessory of hydrogen fuel cells (HFC), air compressors have the problem of high energy consumption and high cost, which hinders the further development of HFC. Recently, the industry has adopted variable-frequency drive (VFD) to control the operation of HFC air compressors, and the energy-saving effect is remarkable. However, existing research mostly discusses related technologies academically and lacks data review on actual industrial applications, hindering industry standardization and maturity. Therefore, it is meaningful to summarize the current status of the HFC air compressor dedicated VFD industry chain. From the perspective of industrial application, this paper summarizes the current industrial product status of dedicated VFDs, HFC, air compressors, and motors. The purpose of this paper is to provide a useful reference guide for relevant researchers of HFCs and VFDs.}
}
@incollection{CRUZ202525,
title = {Equity, Diversity, and Inclusion and Critical Libraries: A Historical Evolution of Awareness and Informed Practice},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {25-35},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00150-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001504},
author = {Jeffery Cruz},
keywords = {Affirmative action in libraries, Anti-discrimination in libraries, Critical librarianship, Decolonization in libraries, Diversity initiatives in libraries, Equal opportunity in libraries, Equity, diversity, and inclusion, Indigenization in libraries, Library and information services, Queering the library},
abstract = {In principle, libraries have espoused the values of equity, diversity, and inclusion (EDI) as evidenced by the broad range of diversity initiatives, commitments, conferences and literature within the library and information studies sector. More recently, concepts of equity and inclusion have played a role in library diversity initiatives in addition to newer concepts such as critical librarianship and the decolonization, Indigenization and queering of libraries. Starting from affirmative action and anti-discrimination policies and legislation in the 1960s, this entry reviews the historical evolution of EDI awareness and informed practice in the US and Australia over the past 65 years.}
}
@article{EID2025155767,
title = {Using Denoising Diffusion Probabilistic Models to solve the inverse sizing problem of analog Integrated Circuits},
journal = {AEU - International Journal of Electronics and Communications},
volume = {195},
pages = {155767},
year = {2025},
issn = {1434-8411},
doi = {https://doi.org/10.1016/j.aeue.2025.155767},
url = {https://www.sciencedirect.com/science/article/pii/S1434841125001086},
author = {Pedro Eid and Filipe Azevedo and Nuno Lourenço and Ricardo Martins},
keywords = {Artificial neural networks, Denoising diffusion probabilistic models, Electronic design automation},
abstract = {In this paper, we focus on using Artificial Neural Networks (ANNs), particularly diffusion models, to automate the sizing of analog Integrated Circuits (ICs), given the constraints of their performance metrics. Researchers have explored various automation methods, including meta-heuristics and optimization-based approaches, to address this challenge. However, each method presents distinct drawbacks and, at times, yields inefficient results. While studies have made some attempts using ANNs, they commonly face the hurdle of the ill-posed nature of the problem exacerbated by the scarcity of databases for training the models. Therefore, this work introduces a novel approach to automate the design process by leveraging diffusion models to enhance the existing ANNs-based framework and address the limitations of previous methodologies. Specifically, Denoising Diffusion Probabilistic Model (DDPM) to tackle the inverse problem of the analog IC sizing. DDPMs employ a noising and denoising architecture, where they learn to reconstruct input distributions by progressively adding and removing noise. Once trained, the DDPM can generate new data from pure noise. We show that even a simple DDPM can sample sizing solutions in a small amount of time, which is an important steppingstone for future research. Experimental results indicate that our models successfully sized the two tested circuits with an average median error of around 6%, surpassing the state-of-the-art approaches whose error was over 60 to 70% higher. Moreover, by taking advantage of the generative capabilities of our models, we were able to generate points for targets within the dataset, with most of them showing an error below 3%. For the more challenging targets, we managed to find solutions with errors below 10%, while the supervised approaches struggled to achieve errors under 20%.}
}
@article{FU2025200538,
title = {The impact of artificial intelligence on digital enterprise innovation},
journal = {Journal of Strategy & Innovation},
volume = {36},
number = {1},
pages = {200538},
year = {2025},
issn = {3050-7901},
doi = {https://doi.org/10.1016/j.jsinno.2025.200538},
url = {https://www.sciencedirect.com/science/article/pii/S3050790125000087},
author = {Yu Fu and Jiacheng Ni and Mengwen Fang},
keywords = {Artificial intelligence, Enterprise innovation, Digital enterprise, Text analysis},
abstract = {The digital economy is transitioning into a new phase characterized as the intelligent economy, propelled by advancements in artificial intelligence (AI). This shift is poised to inject renewed dynamism into the global economy. This paper develops a theoretical framework that elucidates the relationship between AI adoption and innovation within digital enterprises, drawing upon resource-based theory (RBT) and absorptive capacity theory (ACAP). Utilizing a sample of Chinese digital enterprises listed overseas from 2000 to 2023, this study employs Python to extract keywords associated with “artificial intelligence” from corporate annual reports, thereby constructing a proxy measure for AI adoption. We empirically investigate the impact of AI adoption on innovation within digital enterprises. Our findings demonstrate that AI adoption significantly enhances innovation in digital enterprises, a result that remains robust across a series of robustness tests. Additional analysis reveals that the absorptive capacity of enterprises mediates the relationship between AI adoption and innovation, while slack resources act as a moderating factor, attenuating the link between absorptive capacity and innovation. This paper extends our understanding of the implications of AI on broader innovation entities, elucidating the mechanisms and boundary conditions of AI application in digital enterprises. It also offers insights into how enterprises can effectively integrate AI technology, strategically configure and utilize slack resources, and ultimately enhance their innovation performance.}
}
@article{GREENE2025101949,
title = {Structure and content in the quality levels of ethical argumentation in online discussions},
journal = {Thinking Skills and Creativity},
volume = {58},
pages = {101949},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2025.101949},
url = {https://www.sciencedirect.com/science/article/pii/S1871187125001981},
author = {Marita Seppänen Greene},
keywords = {Ethical argumentation, Computer-supported collaborative learning, Critical thinking, Creative thinking, Higher education},
abstract = {Learning to argue about ethical dilemmas and questions is a vital competency for a democratic society, as one’s thoughts and actions must be morally justified. Ethical argumentation is a combination of facts, emotions, and values that considers the consequences of right and wrong based on the values of individuals and communities. It requires support from both critical thinking to argue clearly, use evidence, and from creative thinking to generate new standpoints and solutions. This study developed a model to analyse undergraduate students’ ethical arguments during role play and free debate (n = 199) in asynchronous online discussions in an authentic, non-scaffolded blended-learning classroom. The instructions include the rules for critical discussion to promote respect for diverse opinions and collaboration, and a model of sound argument structure and common fallacies. Theoretical models of ethical argumentation and iterative data analysis were used to identify and evaluate the argumentation structure and content across different levels of quality. The quality of the content and structural complexity varied between acceptable and unacceptable depending on the topic and students’ engagement to argue. A considerable number of low-quality unsubstantiated arguments ‘other structures’ was also observed. The argumentation was generally fair and without personal attack. The number of fallacies and weak arguments was high for the first four of the six studied topics but significantly decreased thereafter. Students considered the ethical consequences across the topics. The results have implications for integrating critical, creative, and ethical thinking into argumentation in computer-supported collaborative learning environments and citizenship education.}
}
@article{PARASCOS2024111996,
title = {Fabrication of Li7La3Zr2O12 films with controlled porosity and fast Li+ transport},
journal = {Journal of Energy Storage},
volume = {92},
pages = {111996},
year = {2024},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2024.111996},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X24015822},
author = {Kade Parascos and Joshua L. Watts and Jose A. Alarco and Peter C. Talbot},
keywords = {Solid-state electrolyte, Tape casting, Sintering, Garnet, Ceramics},
abstract = {Free-standing Ga-doped Li7La3Zr2O12 (LLZO) films are fabricated by a scalable tape casting technique. The thin (25–50 μm) ceramics exhibit a relative density of 98 ± 1 % and record-high ionic conductivity (1.41 ± 0.05 mS cm−1) for tape cast LLZO. A solution-based approach is used to synthesize the green powders which enable sintering at relatively low temperatures (1050 °C, 30 min). The phase chemistry and microstructure are well-controlled during the sintering process. The tapes are further developed into multilayer porous/dense/porous structures to provide a fast Li+ conducting ceramic framework for all-solid-state batteries (ASSBs). X-ray micro computed tomography (microCT) analysis reveals a fully interconnected pore network in the outer porous layers. In addition, the multilayer structures exhibit a critical current density (CCD) of 2.5 mA cm−2 at room temperature and are stable over 200 cycles at a current density of 0.5 mA cm−2 in symmetric Li cells. Overall, this work offers guidance for the scalable production of thin LLZO ceramics with enhanced physical and electrochemical properties.}
}
@article{HUANG2024617,
title = {Mitigating treatment failure of pulmonary pre-extensively drug-resistant tuberculosis: The role of new and repurposed drugs},
journal = {Journal of Microbiology, Immunology and Infection},
volume = {57},
number = {4},
pages = {617-628},
year = {2024},
issn = {1684-1182},
doi = {https://doi.org/10.1016/j.jmii.2024.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1684118224000768},
author = {Yi-Wen Huang and Ming-Chih Yu and Chih-Bin Lin and Jen-Jyh Lee and Chou-Jui Lin and Shun-Tien Chien and Chih-Hsin Lee and Chen-Yuan Chiang},
keywords = {Bedaquiline, Clofazimine, Carbapenem, Linezolid, Delamanid},
abstract = {Background
Pre-extensively drug-resistant tuberculosis (pre-XDR-TB), defined as multidrug-resistant TB (MDR-TB) with additional resistance to any fluoroquinolone (FQ) is difficult to treat. We assessed whether the use of new or repurposed drugs (bedaquiline, delamanid, linezolid, carbapenem, clofazimine, pretomanid) mitigated treatment failure of pre-XDR-TB.
Methods
MDR-TB patients managed in the Taiwan MDR-TB consortium between July 2009–December 2019 were eligible. Treatment outcomes at 30 months were assessed. Logistic regression models were constructed to investigate factors associated with treatment outcomes.
Results
109 patients with FQ-resistant MDR-TB and 218 patients with FQ-susceptible MDR-TB were included. 60 (55.1%) patients with FQ-resistant MDR-TB and 63 (28.9%) patients with FQ-susceptible MDR-TB have been treated with new or repurposed drugs (p < 0.01). Of the 218 patients with FQ-susceptible MDR-TB, 187 (85.8%) had treatment success, 30 (13.8%) died, no treatment failure, and 1 (0.5%) was loss-to-follow-up; of the 109 patients with FQ-resistant MDR-TB, 78 (71.6%) had treatment success, 21 (19.3%) died, 9 (8.3%) had treatment failure, and 1 (0.9%) was loss-to-follow-up (p < 0.01). The use of new or repurposed drugs was not associated with treatment outcomes among patients with FQ-susceptible MDR-TB. No patients with FQ-resistant MDR-TB treated with ≥2 new or repurposed drugs within 6 months of treatment initiation had treatment failure (p = 0.03). Patients with FQ-resistant MDR-TB treated with 1 new or repurposed drugs was more likely to have treatment failure as compared with patients not treated with new or repurposed drugs (adjOR 7.06, 95% CI 1.72–29.06).
Conclusions
Proper use of new or repurposed anti-TB drugs can mitigate treatment failure in FQ-resistant MDR-TB.}
}
@article{KAY202549,
title = {Lithospheric architecture of the Curnamona Province, Australia},
journal = {Gondwana Research},
volume = {144},
pages = {49-63},
year = {2025},
issn = {1342-937X},
doi = {https://doi.org/10.1016/j.gr.2025.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1342937X25001133},
author = {Ben Kay and Graham Heinson and Goran Boren and Ying Liu and Kate Brand and Stephan Thiel and Helen Williams},
keywords = {Magnetotellurics, Curnamona Province, Adelaide Rift Complex, Gawler Craton, Adelaide Superbasin},
abstract = {The Curnamona Province in southern Australia is a 90,000 km2 Paleoproterozoic to Mesoproterozoic craton that rifted from the Gawler Craton in the Neoproterozoic and is now separated by the Adelaide Rift Complex sediments in the Flinders Ranges. Magnetotelluric (MT) surveys have been conducted across the Curnamona Province to provide insights on lithospheric architecture. A long-period (10–10,000 s) MT inversion of 136 AusLAMP and legacy long-period MT sites, 134 broadband MT, and 31 geomagnetic depth sounding (GDS) sites over an area 500 km by 500 km is used to create a lithospheric-scale framework for the province. In addition, a crustal-scale broadband (0.01 to 1000 s) MT inversion of 134 sites with closer spacing yields higher fidelity of crustal heterogeneity. The long-period Curnamona Province model is combined with other regional models across southern Australia, encompassing the Archean-Mesoproterozoic Gawler Craton in the west to the Phanerozoic Delamerian and Lachlan Orogens in the east. In this wider context, the Curnamona Province is shown to exhibit four distinct regions of low resistivity. (a) In the top 5 km, resistivity is most strongly influenced by the porosity of sedimentary cover. (b) The upper crust to a depth of 20 km has low resistivity < 1 Ω.m along the eastern margin of the province possibly from organic carbon deposited in the Paleoproterozoic. (c) The lower crust to the Moho (∼40 km) has low resistivity (<100 Ω.m) and is coincident with the extent of the Adelaide Rift Complex suggesting it may have resulted during a period of Neoproterozoic rifting. (d) At sub-continental lithospheric depths of 150–250 km, high electrical conductance in the central to northern part of the Curnamona Province is attributed to metasomatism at the base of the lithosphere in the Mesoproterozoic.}
}
@article{YANG2024127811,
title = {Self-support matching networks with multiscale attention for few-shot semantic segmentation},
journal = {Neurocomputing},
volume = {594},
pages = {127811},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127811},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224005824},
author = {Yafeng Yang and Yufei Gao and Lin Wei and Mengyang He and Yucheng Shi and Hailing Wang and Qing Li and Zhiyuan Zhu},
keywords = {Few-shot semantic segmentation, Multiscale, Attention mechanism, Self-support matching},
abstract = {Recent advancements in Few-shot segmentation (FSS) have displayed remarkable capabilities in predicting segmentation masks for unseen class images, using only a limited number of annotated images. However, existing methods have overlooked the influence of contextual information on segmentation and primarily rely on supporting prototypes, with limited research on query prototyping. Effectively utilizing multi-scale features and query information poses a challenging problem in this domain. To address these challenges, this paper proposes a novel approach called the multi-scale and attention-based self-support prototype few-shot semantic segmentation network (MASNet). First, a multi-scale feature enhancement module is designed to obtain features at different scales to enrich global context information. Then, simple and efficient channel attention is utilized to guide the query features related to the target class. Finally, the query prototype is matched with the query features using a self-supporting matching module. This strategy efficiently captures class-based features and addresses the issue of intra-class variance in few-shot segmentation. The experimental results on Pascal-5i, COCO-20i and Abdominal MRI datasets demonstrate that the proposed method achieves remarkable robustness and improved accuracy performance.}
}
@article{FERRARIO2024,
title = {The Role of Humanization and Robustness of Large Language Models in Conversational Artificial Intelligence for Individuals With Depression: A Critical Analysis},
journal = {JMIR Mental Health},
volume = {11},
year = {2024},
issn = {2368-7959},
doi = {https://doi.org/10.2196/56569},
url = {https://www.sciencedirect.com/science/article/pii/S2368795924000696},
author = {Andrea Ferrario and Jana Sedlakova and Manuel Trachsel},
keywords = {generative AI, large language models, large language model, LLM, LLMs, machine learning, ML, natural language processing, NLP, deep learning, depression, mental health, mental illness, mental disease, mental diseases, mental illnesses, artificial intelligence, AI, digital health, digital technology, digital intervention, digital interventions, ethics},
abstract = {Large language model (LLM)–powered services are gaining popularity in various applications due to their exceptional performance in many tasks, such as sentiment analysis and answering questions. Recently, research has been exploring their potential use in digital health contexts, particularly in the mental health domain. However, implementing LLM-enhanced conversational artificial intelligence (CAI) presents significant ethical, technical, and clinical challenges. In this viewpoint paper, we discuss 2 challenges that affect the use of LLM-enhanced CAI for individuals with mental health issues, focusing on the use case of patients with depression: the tendency to humanize LLM-enhanced CAI and their lack of contextualized robustness. Our approach is interdisciplinary, relying on considerations from philosophy, psychology, and computer science. We argue that the humanization of LLM-enhanced CAI hinges on the reflection of what it means to simulate “human-like” features with LLMs and what role these systems should play in interactions with humans. Further, ensuring the contextualization of the robustness of LLMs requires considering the specificities of language production in individuals with depression, as well as its evolution over time. Finally, we provide a series of recommendations to foster the responsible design and deployment of LLM-enhanced CAI for the therapeutic support of individuals with depression.}
}
@article{MOSLEH2024e31952,
title = {Examining the association between emotional intelligence and chatbot utilization in education: A cross-sectional examination of undergraduate students in the UAE},
journal = {Heliyon},
volume = {10},
number = {11},
pages = {e31952},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e31952},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024079830},
author = {Sultan M. Mosleh and Fton Ali Alsaadi and Fatima Khamis Alnaqbi and Meirah Abdullrahman Alkhzaimi and Shamma Waleed Alnaqbi and Waed Mohammed Alsereidi},
keywords = {Chatbot, Artificial intelligence, Emotional intelligence, Undergraduate education},
abstract = {Background
While Emotional Intelligence (EI) demonstrably affects academic success, literature lacks exploration of how implementing chatbot in education might influence both academic performance and students' emotional intelligence, despite the evident potential of such technology.
Aim
To investigate the associations between Emotional Intelligence (EI), chatbot utilization among undergraduate students.
Methods
A cross-sectional approach was employed, utilizing a convenience sample of 529 undergraduate students recruited through online questionnaires. The participants completed the Trait Emotional Intelligence Questionnaire and modified and a modified versions of the unified theory of acceptance and use of technology (UTAUT) model.
Results
of the 529 participants, 83.6 % (n = 440) of participants regularly used chatbot for learning. Students demonstrated a moderate average EI score (129.60 ± 50.15) and an exceptionally high score (89.61 ± 20.70) for chatbot acceptance and usage. A statistically significant (p < 0.001) positive correlation was found between chatbot usage frequency and EI total score. Gender and major emerged as significant factors, with female students (p < 0.05) and health science students (p < 0.05) utilizing chatbot less compared to male and other major students, respectively. A negative correlation (r = −0.111, p = 0.011) was observed between study hours and chatbot usage, suggesting students with higher study hours relied less on chatbot.
Conclusions
The positive correlation between chatbot use and EI in this study sparks promising avenues for enhancing the learning experience. By investing in further research to understand this link and integrate AI tools thoughtfully, policymakers and educators can cultivate a learning environment that prioritizes both academic excellence and student well-being, reflecting the values and perspectives of UAE culture.}
}
@article{WANG20243891,
title = {Differentially Private Support Vector Machines with Knowledge Aggregation},
journal = {Computers, Materials and Continua},
volume = {78},
number = {3},
pages = {3891-3907},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.048115},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824003369},
author = {Teng Wang and Yao Zhang and Jiangguo Liang and Shuai Wang and Shuanggen Liu},
keywords = {Differential privacy, support vector machine, knowledge aggregation, data utility},
abstract = {With the widespread data collection and processing, privacy-preserving machine learning has become increasingly important in addressing privacy risks related to individuals. Support vector machine (SVM) is one of the most elementary learning models of machine learning. Privacy issues surrounding SVM classifier training have attracted increasing attention. In this paper, we investigate Differential Privacy-compliant Federated Machine Learning with Dimensionality Reduction, called FedDPDR-DPML, which greatly improves data utility while providing strong privacy guarantees. Considering in distributed learning scenarios, multiple participants usually hold unbalanced or small amounts of data. Therefore, FedDPDR-DPML enables multiple participants to collaboratively learn a global model based on weighted model averaging and knowledge aggregation and then the server distributes the global model to each participant to improve local data utility. Aiming at high-dimensional data, we adopt differential privacy in both the principal component analysis (PCA)-based dimensionality reduction phase and SVM classifiers training phase, which improves model accuracy while achieving strict differential privacy protection. Besides, we train Differential privacy (DP)-compliant SVM classifiers by adding noise to the objective function itself, thus leading to better data utility. Extensive experiments on three high-dimensional datasets demonstrate that FedDPDR-DPML can achieve high accuracy while ensuring strong privacy protection.}
}
@article{FLOTHER2025101236,
title = {How quantum computing can enhance biomarker discovery},
journal = {Patterns},
volume = {6},
number = {6},
pages = {101236},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101236},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925000844},
author = {Frederik F. Flöther and Daniel Blankenberg and Maria Demidik and Karl Jansen and Raga Krishnakumar and Rajiv Krishnakumar and Nouamane Laanait and Laxmi Parida and Carl Y. Saab and Filippo Utro},
abstract = {Summary
Biomarkers play a central role in medicine’s gradual progress toward proactive, personalized precision diagnostics and interventions. However, finding biomarkers that provide very early indicators of a change in health status, for example, for multifactorial diseases, has been challenging. The discovery of such biomarkers stands to benefit significantly from advanced information processing and means to detect complex correlations, which quantum computing offers. In this perspective, quantum algorithms, particularly in machine learning, are mapped to key applications in biomarker discovery. The opportunities and challenges associated with the algorithms and applications are discussed. The analysis is structured according to different data types—multidimensional, time series, and erroneous data—and covers key data modalities in healthcare—electronic health records, omics, and medical images. An outlook is provided concerning open research challenges.}
}
@article{CHO2025100317,
title = {Spatial Transcriptomics in Inflammatory Skin Diseases Using GeoMx Digital Spatial Profiling: A Practical Guide for Applications in Dermatology},
journal = {JID Innovations},
volume = {5},
number = {1},
pages = {100317},
year = {2025},
issn = {2667-0267},
doi = {https://doi.org/10.1016/j.xjidi.2024.100317},
url = {https://www.sciencedirect.com/science/article/pii/S2667026724000651},
author = {Christina Cho and Nazgol-Sadat Haddadi and Michal Kidacki and Gavitt A. Woodard and Saeed Shakiba and Ümmügülsüm Yıldız-Altay and Jillian M. Richmond and Matthew D. Vesely},
keywords = {Cutaneous lupus, Digital spatial profiling, Lichen planus, Psoriasis, Spatial transcriptomics},
abstract = {The spatial organization of the skin is critical for its function. In particular, the skin immune microenvironment is arranged spatially and temporally, such that imbalances in the immune milieu are indicative of disease. Spatial transcriptomic platforms are helping to provide additional insights into aberrant inflammation in tissues that are not captured by tissue processing required for single-cell RNA sequencing. In this paper, we discuss a technical and user experience overview of NanoString's GeoMx Digital Spatial Profiler to perform in-depth spatial analysis of the transcriptome in inflammatory skin diseases. Our objective is to provide potential pitfalls and methods to optimize RNA capture that are not readily available in the manufacturer’s guidelines. We use concrete examples from our experiments to demonstrate these strategies in inflammatory skin diseases, including psoriasis, lichen planus, and discoid lupus erythematosus. Overall, we hope to illustrate the potential of digital spatial profiling to dissect skin disease pathogenesis in a spatially resolved manner and provide a framework for other skin biology investigators using digital spatial profiling.}
}
@article{CHIEN2025100328,
title = {Creating an Extremely Long-lasting Neuroischemic Wound Model},
journal = {JID Innovations},
volume = {5},
number = {2},
pages = {100328},
year = {2025},
issn = {2667-0267},
doi = {https://doi.org/10.1016/j.xjidi.2024.100328},
url = {https://www.sciencedirect.com/science/article/pii/S2667026724000766},
author = {Sufan Chien and Harshini Sarojini and Arezoo Rajaee and Mohammad Bayat and Samson Chien and Girish Kotwal},
keywords = {Animal model, Neuroischemia, Rabbit, Long term, Wound},
abstract = {In wound study and dressing development, a lack of a suitable animal model that can recapitulate the complex pathophysiology of human chronic wounds has been a major hurdle. Chronic wounds are defined as wounds that heal with a significant delay, usually over a period >2–3 months, but no current animal wound model has such a longischemia. After a longexploration, our group has developed an animal wound model with ischemia and nerve damage lasting for at least 6 months. This model can be easily combined with other conditions such as diabetes and aging for wound mechanistic study and critical testing of dressings. This report presents the method that has significant utility in evaluating therapies that could become the future standard for screening all new wound dressings.}
}
@incollection{ALKHALIFAH2025111,
title = {Chapter 6 - Digital twin in cardiology: Navigating the digital landscape for education, global health, and preventive medicine},
editor = {Miltiadis D. Lytras and Abdulrahman Housawi and Basim S. Alsaywid and Naif Radi Aljohani},
booktitle = {Next Generation eHealth},
publisher = {Academic Press},
pages = {111-126},
year = {2025},
series = {Next Generation Technology Driven Personalized Medicine And Smart Healthcare},
isbn = {978-0-443-13619-1},
doi = {https://doi.org/10.1016/B978-0-443-13619-1.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443136191000064},
author = {Yara Alkhalifah and Dimitrios Lytras},
keywords = {Artificial intelligence, Big data, Cardiology, Computational cardiology, Digital transformation, Digital twin, Global health, Internet of Things (IoT), Preventive medicine},
abstract = {Background: Digital transformation is revolutionizing cardiology through Digital Twins (DTs) technology, a virtual model linking physical entities with their digital counterparts. Despite exponential growth in research, challenges in mathematical foundations and scalable algorithms hinder its realization in healthcare. Methods: A thorough literature review was conducted, searching mainly the databases of MEDLINE/PubMed and IEEE Xplore digital library, using different combinations of the keywords “Digital Twin”, “Cardiology”, “Cardiovascular”, and “Heart” for abstract and title, while applying the Boolean search method. Applications of DT in cardiology are explored, emphasizing transformative potential and challenges. Results: In precision medicine, DT integrates diverse data types through federated learning, transforming clinical trials, and research methodologies. Federated learning addresses privacy concerns, while dynamic bidirectional links offer personalized insights. Digital twins find global applications in hypertension, offering insights into regional prevalence and transforming global cardiovascular health. Their integration in preventive cardiology presents a multidisciplinary approach, predicting cardiovascular risk, personalizing treatment plans, and reshaping patient involvement in healthcare. Conclusion: Although not yet fully realized, Digital Twins' predictive capabilities hold promise in precision medicine, clinical research, and global health. In preventive cardiology, they offer a paradigm shift, providing personalized insights, reshaping education, and overcoming challenges. Digital Twins' vision is facilitated by computational science research and the recruitment of specialized computational cardiologists. As research progresses, attention to privacy, data sharing, computational power, and ethical considerations is crucial for successful implementation and widespread adoption of this transformative technology.}
}
@article{DAWSON2025100238,
title = {Advancing blue-green infrastructure design with synthetic 3D drainage channels: A scenario-based flood model in Nova Scotia, Canada},
journal = {Nature-Based Solutions},
volume = {7},
pages = {100238},
year = {2025},
issn = {2772-4115},
doi = {https://doi.org/10.1016/j.nbsj.2025.100238},
url = {https://www.sciencedirect.com/science/article/pii/S2772411525000278},
author = {Corey Dawson},
keywords = {River builder, Blue-green infrastructure, Synthetic 3D channels, Sustainable urban drainage systems, Nature-based solutions, Flood modelling},
abstract = {Urbanized riverscapes are facing challenges due to hydrological changes. Adjusted flow regimes and imperviousness are contributing to increased flood risks resulting from gray infrastructure and strained subgrade drainage systems. Here a new methodology is presented for designing synthetic 3D drainage channels as blue-green infrastructure to enhance multidisciplinary decision-making for sustainable urban drainage systems planning and elements of nature-based stormwater management. LiDAR derived digital elevation models and River Builder software were used to generate four unique drainage channel scenarios with different surface geometries and vegetative cover types for flood modelling. Flood risks were assessed by fluvial simulation responses to specific channel elements and the design process may translate to real-world applications. Fluvial simulations were compared to evaluate how flood inundation patterns and flow velocities responded to morphology changes and roughness coefficients. Results suggest that incorporating geomorphic principles into open drainage channels can advance blue-green infrastructure design by reflecting more natural morphological elements and improve stakeholder engagement that is well suited for nature-based solutions. By combining high-resolution LiDAR data and process-based River Builder functions, the methodology presents a design tool for interactive investigation, adjustment, and communication of continuous 3D channel design scenarios. Although further site-specific studies are needed and additional metrics may be applied, this paper demonstrates a flexible framework to support sustainable urban drainage systems and nature-based stormwater management approaches in urbanized riverscapes.}
}
@article{ORTEGAQUINTANILLA2025169439,
title = {On the Molecular Basis of the Hypersaline Adaptation of Halophilic Proteins},
journal = {Journal of Molecular Biology},
pages = {169439},
year = {2025},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2025.169439},
url = {https://www.sciencedirect.com/science/article/pii/S0022283625005054},
author = {Gabriel Ortega-Quintanilla and Oscar Millet},
keywords = {haloadaptation, protein, electrostatics, preferential exclusion, hydrophobic solvation},
abstract = {Halophilic organisms have adapted to survive in environments with extremely high salinity, such as saline lakes. To achieve this, they modify their proteome to withstand salt concentrations that inactivate non-adapted mesophilic proteins. The surfaces of halophilic proteins feature a very characteristic amino acid composition, favoring short, polar, and acidic amino acids—such as aspartate, glutamate, and threonine—while disfavoring bulky, hydrophobic amino acids—such as lysine, methionine, and leucine. In this work, we review our understanding of the molecular basis of haloadaptation. We critically examine the role of electrostatic interactions in stabilizing halophilic proteins, while underlining the importance of other contributions from hydrophobic solvation and preferential ion exclusion. Finally, we describe the mechanistic link by which the halophilic amino acid composition optimizes function in hypersaline environments, balancing the trade-off between stability, solubility, and catalytic function.}
}
@article{HE2024858,
title = {Integrating human expertise to optimize the fabrication of parts with complex geometries in WAAM},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {858-868},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524000906},
author = {Fengyang He and Lei Yuan and Haochen Mu and Montserrat Ros and Donghong Ding and Huijun Li and Zengxi Pan},
keywords = {Human motion, Machine learning, Virtual reality (VR), Wire arc additive manufacturing (WAAM)},
abstract = {Wire arc additive manufacturing (WAAM) has emerged as a versatile solution for fabricating parts with complex geometries in recent years. However, the existing deposition parameter planning methods struggle to offer continuous and precise parameters when the part geometry varies dynamically due to the long-term dependence, strong coupling, and hysteresis properties of the WAAM process. To address this challenge, this research introduces an advanced algorithm for generating accurate and continuous deposition parameters by learning and utilizing the welding skills of proficient human welders. The first step involves capturing kinematic and welding parameter data from proficient human welders during practical welding processes. Following this, a human skill learning algorithm is developed based on a combination of the adaptive neuro-fuzzy inference system (ANFIS) architecture and particle swarm optimization (PSO) to analyse and model human motions and bead deposition results. Lastly, a practical backward model is established to generate continuous deposition parameters for weld beads with varying geometry. The effectiveness of the proposed algorithm is validated through the fabrication of two practical WAAM parts. The root mean square error (RMSE) values between the target geometry and the ground truth geometry of the parts are 0.1648 and 0.1805 respectively. The result demonstrates the algorithm's superior ability in optimal deposition parameters planning for fabricating parts with complex geometries.}
}
@article{ELYOSEPH2024,
title = {Comparing the Perspectives of Generative AI, Mental Health Experts, and the General Public on Schizophrenia Recovery: Case Vignette Study},
journal = {JMIR Mental Health},
volume = {11},
year = {2024},
issn = {2368-7959},
doi = {https://doi.org/10.2196/53043},
url = {https://www.sciencedirect.com/science/article/pii/S2368795924000246},
author = {Zohar Elyoseph and Inbar Levkovich},
keywords = {schizophrenia, mental, prognostic, prognostics, prognosis, ChatGPT, artificial intelligence, recovery, vignette, vignettes, outcome, outcomes, large language models, language model, language models, LLM, LLMs, NLP, natural language processing, GPT, Generative Pre-trained Transformers},
abstract = {Background
The current paradigm in mental health care focuses on clinical recovery and symptom remission. This model’s efficacy is influenced by therapist trust in patient recovery potential and the depth of the therapeutic relationship. Schizophrenia is a chronic illness with severe symptoms where the possibility of recovery is a matter of debate. As artificial intelligence (AI) becomes integrated into the health care field, it is important to examine its ability to assess recovery potential in major psychiatric disorders such as schizophrenia.
Objective
This study aimed to evaluate the ability of large language models (LLMs) in comparison to mental health professionals to assess the prognosis of schizophrenia with and without professional treatment and the long-term positive and negative outcomes.
Methods
Vignettes were inputted into LLMs interfaces and assessed 10 times by 4 AI platforms: ChatGPT-3.5, ChatGPT-4, Google Bard, and Claude. A total of 80 evaluations were collected and benchmarked against existing norms to analyze what mental health professionals (general practitioners, psychiatrists, clinical psychologists, and mental health nurses) and the general public think about schizophrenia prognosis with and without professional treatment and the positive and negative long-term outcomes of schizophrenia interventions.
Results
For the prognosis of schizophrenia with professional treatment, ChatGPT-3.5 was notably pessimistic, whereas ChatGPT-4, Claude, and Bard aligned with professional views but differed from the general public. All LLMs believed untreated schizophrenia would remain static or worsen without professional treatment. For long-term outcomes, ChatGPT-4 and Claude predicted more negative outcomes than Bard and ChatGPT-3.5. For positive outcomes, ChatGPT-3.5 and Claude were more pessimistic than Bard and ChatGPT-4.
Conclusions
The finding that 3 out of the 4 LLMs aligned closely with the predictions of mental health professionals when considering the “with treatment” condition is a demonstration of the potential of this technology in providing professional clinical prognosis. The pessimistic assessment of ChatGPT-3.5 is a disturbing finding since it may reduce the motivation of patients to start or persist with treatment for schizophrenia. Overall, although LLMs hold promise in augmenting health care, their application necessitates rigorous validation and a harmonious blend with human expertise.}
}
@article{CHEN2025103032,
title = {Nationalism meets machine heuristics: Investigating the effect of AI’s “nationality” on the perceived credibility of AIGC},
journal = {Technology in Society},
volume = {83},
pages = {103032},
year = {2025},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.103032},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X25002222},
author = {Junyi Chen and Weixi Zeng and Yi Mou},
keywords = {Politicized technology, Credibility, AIGC, Machine heuristic, Nationalism, Large language model},
abstract = {Against the backdrop of politicizing science and technology, it has nevertheless become a common practice to employ AI tools in political communication. This study delves into the tension between the supposedly high credibility of AI-generated content (AIGC) with its perceived objectivity and the potentially devastating impact of politicized tools. Specifically, we investigate the disparities in the perceived credibility of AIGC, stemming from various source types and nationalities. Using an online experiment conducted in China, we found that Chinese AI sources and human-AI hybrid sources with consistent Chinese national identity were perceived as more credible than their foreign (American) counterparts, whose effects were fully mediated by machine heuristics. In contrast, less credibility was directly attributed to human-AI hybrid sources with inconsistent human-AI nationalities (e.g., a Chinese journalist using American AI). Furthermore, although foreign sources were generally seen as less credible, the presence of AI in the source moderated this negative effect. These findings indicated users' perceptions of the political dimensions of AI technology.}
}
@article{JIANG2024202,
title = {Preventing the Immense Increase in the Life-Cycle Energy and Carbon Footprints of LLM-Powered Intelligent Chatbots},
journal = {Engineering},
volume = {40},
pages = {202-210},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924002315},
author = {Peng Jiang and Christian Sonne and Wangliang Li and Fengqi You and Siming You},
keywords = {Large language models, Intelligent chatbots, Carbon emissions, Energy and environmental footprints, Life-cycle assessment, Global cooperation},
abstract = {Intelligent chatbots powered by large language models (LLMs) have recently been sweeping the world, with potential for a wide variety of industrial applications. Global frontier technology companies are feverishly participating in LLM-powered chatbot design and development, providing several alternatives beyond the famous ChatGPT. However, training, fine-tuning, and updating such intelligent chatbots consume substantial amounts of electricity, resulting in significant carbon emissions. The research and development of all intelligent LLMs and software, hardware manufacturing (e.g., graphics processing units and supercomputers), related data/operations management, and material recycling supporting chatbot services are associated with carbon emissions to varying extents. Attention should therefore be paid to the entire life-cycle energy and carbon footprints of LLM-powered intelligent chatbots in both the present and future in order to mitigate their climate change impact. In this work, we clarify and highlight the energy consumption and carbon emission implications of eight main phases throughout the life cycle of the development of such intelligent chatbots. Based on a life-cycle and interaction analysis of these phases, we propose a system-level solution with three strategic pathways to optimize the management of this industry and mitigate the related footprints. While anticipating the enormous potential of this advanced technology and its products, we make an appeal for a rethinking of the mitigation pathways and strategies of the life-cycle energy usage and carbon emissions of the LLM-powered intelligent chatbot industry and a reshaping of their energy and environmental implications at this early stage of development.}
}
@article{CHEN2024104651,
title = {Chat-ePRO: Development and pilot study of an electronic patient-reported outcomes system based on ChatGPT},
journal = {Journal of Biomedical Informatics},
volume = {154},
pages = {104651},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104651},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000698},
author = {Zikang Chen and Qinchuan Wang and Yaoqian Sun and Hailing Cai and Xudong Lu},
keywords = {Patient Reported Outcome Measures, Large Language Model, Knowledge Distillation, Breast Cancer, Mobile Health},
abstract = {Objective
Chatbots have the potential to improve user compliance in electronic Patient-Reported Outcome (ePRO) system. Compared to rule-based chatbots, Large Language Model (LLM) offers advantages such as simplifying the development process and increasing conversational flexibility. However, there is currently a lack of practical applications of LLMs in ePRO systems. Therefore, this study utilized ChatGPT to develop the Chat-ePRO system and designed a pilot study to explore the feasibility of building an ePRO system based on LLM.
Materials and Methods
This study employed prompt engineering and offline knowledge distillation to design a dialogue algorithm and built the Chat-ePRO system on the WeChat Mini Program platform. In order to compare Chat-ePRO with the form-based ePRO and rule-based chatbot ePRO used in previous studies, we conducted a pilot study applying the three ePRO systems sequentially at the Sir Run Run Shaw Hospital to collect patients’ PRO data.
Result
Chat-ePRO is capable of correctly generating conversation based on PRO forms (success rate: 95.7 %) and accurately extracting the PRO data instantaneously from conversation (Macro-F1: 0.95). The majority of subjective evaluations from doctors (>70 %) suggest that Chat-ePRO is able to comprehend questions and consistently generate responses. Pilot study shows that Chat-ePRO demonstrates higher response rate (9/10, 90 %) and longer interaction time (10.86 s/turn) compared to the other two methods.
Conclusion
Our study demonstrated the feasibility of utilizing algorithms such as prompt engineering to drive LLM in completing ePRO data collection tasks, and validated that the Chat-ePRO system can effectively enhance patient compliance.}
}
@article{CARNEIRO2025104645,
title = {A flexible ISO 27701-based framework for assessing cybersecurity maturity: a proposition and a case application},
journal = {Computers & Security},
volume = {158},
pages = {104645},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104645},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825003347},
author = {Fábio Dias Carneiro and Izabela Simon Rampasso and Sidney Luiz {de Matos Mello} and Tiago F.A.C. Sigahi and Hernán Lespay and Rosley Anholon},
keywords = {ISO 27701, Information security, Cybersecurity, Fuzzy TOPSIS, Higher Education Institutions},
abstract = {This study aims to propose a framework for assessing the cybersecurity management level of organizations, based on ISO 27701. To illustrate the proposed framework, and considering the relevance of cybersecurity for Higher Education Institutions (HEIs), an analysis of the reality of Federal HEIs in Brazil is conducted. To develop the proposed framework, the standard ISO 27701 was used to structure a questionnaire. The proposed data analysis combines Hierarchical Cluster Analysis (HCA), frequency analysis, and Fuzzy TOPSIS. The case application considered experts in information security of Federal HEIs in Brazil. The proposed framework presents eight steps: definition of the application focus, analysis of variables and scale proposed, questionnaire structuring, ethics committee submission, data gathering, HCA, frequency analysis, Fuzzy TOPSIS. Regarding the case application, aspects related to internal auditing, asset management and human resources training and analysis were the most critical. This study presents a comprehensive framework for guiding information security assessment in organizations. The proposed framework presents the necessary flexibility to be adjusted according to the requirements of practitioners and researchers. It can be used by companies and the government to assess their current reality and evaluate the impact of changes performed. Researchers can integrate the proposed framework into an Artificial Intelligence mechanism for risk prediction in organizations. The findings from the case application evidence the contributions of this framework to assess the reality of any kind of institution and highlight the insights that can be obtained from its analysis.}
}
@article{SHUKLA2024100605,
title = {An overview of blockchain research and future agenda: Insights from structural topic modeling},
journal = {Journal of Innovation & Knowledge},
volume = {9},
number = {4},
pages = {100605},
year = {2024},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2024.100605},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001446},
author = {Anuja Shukla and Poornima Jirli and Anubhav Mishra and Alok Kumar Singh},
keywords = {Structural topic modeling, Blockchain, Scenario building, Datatopia, Natural language processing, Emerging technologies},
abstract = {As a disruptive technology, blockchain has become a strategic priority for many businesses. A vast amount of research exists on blockchain's innovative nature and immense potential for multiple industries. This study aims to synthesize the existing research to classify the findings into various themes and propose avenues for further research. A total of 2,360 academic articles were analyzed using the text-mining method of structural topic modeling. The identified fifteen topics were mapped to the four quadrants of the Datatopia model, leading to the development of the Datatopia-blockchain (DBlock) framework. The results present future scenarios that provide an understanding of what is known about blockchain, its characteristics, and potential research areas. The contributions to the theory and implications to the practitioners are discussed in detail.}
}
@article{MCKINNEY2024e30106,
title = {Automated vs. manual coding of neuroimaging reports via natural language processing, using the international classification of diseases, tenth revision},
journal = {Heliyon},
volume = {10},
number = {10},
pages = {e30106},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e30106},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024061371},
author = {Alexander M. McKinney and Jessica A. Moore and Kevin Campbell and Thiago A. Braga and Jeffrey B. Rykken and Bharathi D. Jagadeesan and Zeke J. McKinney},
keywords = {ICD-10, Coding, Neuroradiology, Impact, Relevance},
abstract = {Objective
Natural language processing (NLP) can generate diagnoses codes from imaging reports. Meanwhile, the International Classification of Diseases (ICD-10) codes are the United States' standard for billing/coding, which enable tracking disease burden and outcomes. This cross-sectional study aimed to test feasibility of an NLP algorithm's performance and comparison to radiologists' and physicians' manual coding.
Methods
Three neuroradiologists and one non-radiologist physician reviewers manually coded a randomly-selected pool of 200 craniospinal CT and MRI reports from a pool of >10,000. The NLP algorithm (Radnosis, VEEV, Inc., Minneapolis, MN) subdivided each report's Impression into “phrases”, with multiple ICD-10 matches for each phrase. Only viewing the Impression, the physician reviewers selected the single best ICD-10 code for each phrase. Codes selected by the physicians and algorithm were compared for agreement.
Results
The algorithm extracted the reports' Impressions into 645 phrases, each having ranked ICD-10 matches. Regarding the reviewers' selected codes, pairwise agreement was unreliable (Krippendorff α = 0.39-0.63). Using unanimous reviewer agreement as “ground truth”, the algorithm's sensitivity/specificity/F2 for top 5 codes was 0.88/0.80/0.83, and for the single best code was 0.67/0.82/0.67. The engine tabulated “pertinent negatives” as negative codes for stated findings (e.g. “no intracranial hemorrhage”). The engine's matching was more specific for shorter than full-length ICD-10 codes (p = 0.00582x10−3).
Conclusions
Manual coding by physician reviewers has significant variability and is time-consuming, while the NLP algorithm's top 5 diagnosis codes are relatively accurate. This preliminary work demonstrates the feasibility and potential for generating codes with reliability and consistency. Future works may include correlating diagnosis codes with clinical encounter codes to evaluate imaging's impact on, and relevance to care.}
}
@article{PARK2024102941,
title = {Machine learning of metal-organic framework design for carbon dioxide capture and utilization},
journal = {Journal of CO2 Utilization},
volume = {89},
pages = {102941},
year = {2024},
issn = {2212-9820},
doi = {https://doi.org/10.1016/j.jcou.2024.102941},
url = {https://www.sciencedirect.com/science/article/pii/S2212982024002762},
author = {Yang Jeong Park and Sungroh Yoon and Sung Eun Jerng},
keywords = {Metal-organic framework, Carbon capture, Machine learning, High-throughput screening, Generative model},
abstract = {Metal-organic frameworks (MOFs) are attractive materials with easily tunable porous structures. Their selective carbon dioxide (CO2) capture ability can be varied by altering the functionality of the organic ligands. However, rule-based approaches to tuning and developing MOFs with high CO2 capture and conversion abilities are hindered by the numerous possible combinations of metal ions and organic linkers. Recently, machine learning (ML) has been applied to unravel key descriptors in predicting the performance of MOFs. This review summarizes recent advancements in ML models for MOFs in CO2 capture and utilization, including high-throughput screening, neural network interatomic potential, and generative models. The development of sophisticated ML models for designing high-performance MOFs will play a critical role in addressing climate change in the future. Finally, the main challenges and limitations of current approaches in designing high-performance MOFs are discussed.}
}
@article{LI2025121195,
title = {FrameDiffusion: A latent diffusion model for intelligent layout design of steel frame-braced structures},
journal = {Engineering Structures},
volume = {343},
pages = {121195},
year = {2025},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2025.121195},
url = {https://www.sciencedirect.com/science/article/pii/S014102962501586X},
author = {Jiang Li and Wei Wang and Bochao Fu and Yuqing Gao},
keywords = {Steel frame-braced structure, Diffusion model, Component layout design, Fine-tuning method, Intelligent design platform},
abstract = {Steel frame-braced structure is widely used for its superior mechanical performance and efficient construction. However, its layout design often relies on traditional paradigms, leading to significant inefficiencies in manpower and material utilization. To address this issue, this paper proposes a novel framework based on diffusion models, named FrameDiffusion. Leveraging the concept of transfer learning, this framework employs the Kronecker product to perform low-rank decomposition of the parameter matrix in a general large model, enabling cost-effective learning of component layout features on small-scale multimodal datasets. Furthermore, FrameDiffusion integrates the ControlNet model, facilitating the layout design of columns and braces in steel frame-braced structures under varying design conditions. The model’s performance is evaluated through both subjective and objective metrics, including mechanical performance, cost-effectiveness, and perceived quality. Experimental results demonstrate that FrameDiffusion outperforms existing methods, even surpassing engineer-designed solutions in certain aspects. Finally, this study develops an intelligent design platform that streamlines the entire workflow of steel frame-braced structure design, modeling, analysis, and optimization, significantly enhancing the automation and intelligence of structural design.}
}
@article{ROY2024105282,
title = {Transcriptional regulation of suppressors of cytokine signaling during infection with Mycobacterium tuberculosis in human THP-1-derived macrophages and in mice},
journal = {Microbes and Infection},
volume = {26},
number = {3},
pages = {105282},
year = {2024},
issn = {1286-4579},
doi = {https://doi.org/10.1016/j.micinf.2023.105282},
url = {https://www.sciencedirect.com/science/article/pii/S1286457923001958},
author = {Trisha Roy and Anuradha Seth and Hasham Shafi and D.V. Siva Reddy and Sunil Kumar Raman and J.V.U.S. Chakradhar and Sonia Verma and Reena Bharti and Lubna Azmi and Lipika Ray and Amit Misra},
keywords = { H37Rv, SOCS1, SOCS3, Transcription factors, Time kinetics, Host-pathogen interaction},
abstract = {Mycobacterium tuberculosis (Mtb) infection leads to upregulation of Suppressors of Cytokine signaling (SOCS) expression in host macrophages (Mϕ). SOCS proteins inhibit cytokine signaling by negatively regulating JAK/STAT. We investigated this host-pathogen dialectic at the level of transcription. We used phorbol-differentiated THP-1 Mϕ infected with Mtb to investigate preferential upregulation of some SOCS isoforms that are known to inhibit signaling by IFN-γ, IL-12, and IL-6. We examined time kinetics of likely transcription factors and signaling molecules upstream of SOCS transcription, and survival of intracellular Mtb following SOCS upregulation. Our results suggest a plausible mechanism that involves PGE2 secretion during infection to induce the PKA/CREB axis, culminating in nuclear translocation of C/EBPβ to induce expression of SOCS1. Mtb-infected Mϕ secreted IL-10, suggesting a mechanism of induction of STAT3, which may subsequently induce SOCS3. We provide evidence of temporal variation in SOCS isoform exspression and decay. Small-interfering RNA-mediated knockdown of SOCS1 and SOCS3 restored the pro-inflammatory milieu and reduced Mtb viability. In mice infected with Mtb, SOCS isoforms persisted across Days 28–85 post infection. Our results suggest that differential temporal regulation of SOCS isoforms by Mtb drives the host immune response towards a phenotype that facilitates the pathogen's survival.}
}
@article{BAHIRWANI2025501,
title = {Relative treatment effects of first-line chemotherapy and immunotherapy for hepatocellular carcinoma: A systematic review and meta-analysis},
journal = {Cancer Pathogenesis and Therapy},
volume = {3},
number = {6},
pages = {501-513},
year = {2025},
issn = {2949-7132},
doi = {https://doi.org/10.1016/j.cpt.2025.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2949713225000515},
author = {Janak Bahirwani and Suruchi {Jai Kumar Ahuja} and Madhav Changela and Het Patel and Nishit Patel and Maulik Kaneriya and Vishal Patel},
keywords = {Immunotherapy, Chemotherapy, Progression-free survival, Hepatocellular carcinoma},
abstract = {Background
Hepatocellular carcinoma (HCC) is the most common primary liver malignancy and the fourth most common cause of cancer-related mortality worldwide. Despite advances in immunotherapies and targeted treatments for HCC, chemotherapy remains a valuable first-line treatment. However, the efficacy of immunotherapy compared to that of chemotherapy is unknown. This study aimed to provide a comprehensive understanding of the effects of chemotherapy and immunotherapy on survival outcomes, response rates, and adverse effects.
Methods
A thorough literature search of multiple electronic databases, including MEDLINE (PubMed), Embase, Web of Science, Cochrane Central Register of Controlled Trials, and ClinicalTrials.gov was conducted from each database’s inception to February 2024 to identify randomized controlled trials (RCTs) that compared first-line chemotherapy (doxorubicin, cisplatin, sorafenib, and fluorouracil) with immunotherapy (pembrolizumab nivolumab, and tislelizumab) for advanced HCC. Two reviewers independently identified the studies, obtained relevant information, and assessed the possibility of bias. The hazard ratios (HR) for progression-free survival (PFS) and overall survival (OS) were merged using random effects meta-analysis.
Results
Twenty studies with 1183 patients were examined. All studies had a high risk of bias. According to a meta-analysis, immunotherapy was linked to a significantly better PFS than chemotherapy (HR, 1.44, 95% confidence interval [CI], 1.04–2.00, I2 = 32%). OS showed a similar trend, although the difference was not statistically significant (HR, 1.26, 95% CI, 0.96–1.66, I2 = 0%). Sensitivity analysis revealed that immunotherapy continued to improve PFS compared to chemotherapy while having no discernible effect on OS.
Conclusions
First-line immunotherapy may offer PFS advantages over chemotherapy for the treatment of advanced HCC. However, a high risk of bias limits definitive conclusions. Larger, higher-quality RCTs are needed to confirm the potential benefits of OS and minimize bias. Although chemotherapy remains a valuable option in resource-limited settings where access to targeted therapies is restricted, the widespread availability of immunotherapy makes it essential to compare both treatments to determine the most appropriate first-line option for advanced HCC.}
}
@article{WILLIAMS2025111217,
title = {Design and methodology for monitoring the occurrence of foodborne pathogens in meat and poultry in United States: Case study of Campylobacter on chicken},
journal = {International Journal of Food Microbiology},
volume = {437},
pages = {111217},
year = {2025},
issn = {0168-1605},
doi = {https://doi.org/10.1016/j.ijfoodmicro.2025.111217},
url = {https://www.sciencedirect.com/science/article/pii/S016816052500162X},
author = {Michael S. Williams and Eric D. Ebel and Iva Bilanovic and Neal J. Golden and Drew S. Posny and Anant X. Venu and Mark R. Powell},
keywords = {Model-based and design-based inference, Surveillance, Trend estimation},
abstract = {In the United States, the Food Safety and Inspection Service (FSIS), which is part of the U.S. Department of Agriculture, oversees multiple sampling programs that focus on detecting microbial contamination of meat and poultry in slaughter and processing establishments. There have been efforts across scientific disciplines to describe data collection methods and estimation strategies for large scale surveys to allow for the integration of data from multiple sources. The FSIS sample selection methods can be described as a close approximation of either a stratified or two-stage cluster sampling design. These surveys therefore support estimates of pathogen occurrence that are derived under both design- and model-based inferential paradigms. This retrospective study will describe population-level estimation strategies and how changes in pathogen contamination can be monitored across time using a trend analysis. An example based on Campylobacter on broiler chicken carcasses is provided. The example demonstrates that changes in the apparent prevalence during the study period are predominantly the result of changes in laboratory methods.}
}
@article{CAI2025112459,
title = {Assessment of woody breast in broiler breast fillets using structured-illumination reflectance imaging coupled with surface profilometry},
journal = {Journal of Food Engineering},
volume = {391},
pages = {112459},
year = {2025},
issn = {0260-8774},
doi = {https://doi.org/10.1016/j.jfoodeng.2024.112459},
url = {https://www.sciencedirect.com/science/article/pii/S0260877424005259},
author = {Jiaxu Cai and Yuzhen Lu},
keywords = {Sinusoidal illumination, Phase analysis, Machine learning, Poultry, Woody breast},
abstract = {Woody breast (WB) myopathy impairs the quality and marketability of poultry products, leading to significant economic losses for poultry industries worldwide due to product downgrading and consumer complaints. WB-affected broiler breast fillets are characterized by abnormal tissue hardness, muscle rigidity, and irregular shape. Manual evaluation based on tactile palpation and visual examination is the current practice for WB assessment at poultry processing facilities, but it is subjective, labor-intensive, and may induce contamination. Structured-illumination reflectance imaging (SIRI), which is capable of depth-resolved tissue characterization and three-dimensional sample topography, has emerged a potential means for poultry defect detection, as opposed to conventional uniform illumination-based imaging. This study presents a novel effort to assess WB in broiler meat using SIRI coupled with phase-measuring profilometry. An in-house assembled, broadband SIRI platform with phase-shifted sinusoidal illumination patterns at different spatial frequencies was used to acquire images from normal and WB-affected boneless chicken fillets. Acquired pattern images at each spatial frequency were demodulated into 1) one phase difference image that depicts sample surface geometry and 2) two intensity images [i.e., direct component (DC) and amplitude component (AC)]. Hand-crafted geometric and textural features were extracted from the phase difference and intensity images respectively, and used for differentiating between normal and WB-affected samples by regularized linear discriminant analysis models. The features from the phase difference images were more effective than the texture features from either DC or AC images, and the highest overall classification accuracy of nearly 93% was achieved by modeling an optimized set of features from the phase difference images, representing improvements of about 8%–11% over the accuracy obtained using the features from DC and AC images. This study demonstrated the effectiveness of sample surface topographic profiles, revealed by the phase difference images, for WB assessment. SIRI coupled with surface profilometry offers a useful tool for WB assessment of broiler breast meat. The dataset of this study has been made publicly available11https://doi.org/10.5281/zenodo.14558847 to enourage the research into SIRI for poultry quality assessment.}
}
@article{ELBOUANANI2025108106,
title = {On the sum and sum-of-ratio of α−μ distributions and application},
journal = {Journal of the Franklin Institute},
volume = {362},
number = {16},
pages = {108106},
year = {2025},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2025.108106},
url = {https://www.sciencedirect.com/science/article/pii/S0016003225005988},
author = {Faissal {El Bouanani}},
keywords = { distribution, Eavesdropping, Inverse laplace transform, Jamming signal, Multi-user communication, Physical layer security, Secrecy outage probability, Sum-of-ratio, Sum of distributions, Upper moment, Upper moment-generating function},
abstract = {This research presents closed-form statistical functions for three sums and sum-of-ratios of correlated or uncorrelated α/2−μ RVs. We evaluated the upper incomplete moment-generating function (UIMGF) and moment-generating function (MGF) of two sum-ratio distributions. The study offers an Erratum of [7] by extending the MGF and CDF of the sum α−μ RVs to the unanalyzed case α>=2. Using statistical features of signal-to-noise ratio (SNR) and signal-to-interference-to-noise ratio (SINR) at legitimate and wiretapper receiver outputs, we investigated the secrecy outage probability of two RF wireless communication schemes subject to α−μ fading and channel state information. Both receivers are equipped with multiple antennas and use the maximal-ratio combining diversity technique. Additionally, artificial noise is supplied from the primary source or a separate jammer transmitter. Using SINR’s UIMGF and MGF, closed-form secrecy outage probability (SOP) and intercept probability (IP) formulas were derived. The monotonicity of all analytical results vs. system settings has been thoroughly investigated. From a security perspective, the first scenario works better with high artificial noise power. Nevertheless, the output SNR of the legitimate end-user combiner decreases dramatically, reducing channel capacity and the average error rate. The framework might be utilized in full-duplex cooperative or multi-user interference systems. Monte Carlo simulation supports all analytical findings in all scenarios.}
}
@article{SHEHATA202411,
title = {Probiotic potential of lactic acid bacteria isolated from honeybees stomach: Functional and technological insights},
journal = {Annals of Agricultural Sciences},
volume = {69},
number = {1},
pages = {11-18},
year = {2024},
issn = {0570-1783},
doi = {https://doi.org/10.1016/j.aoas.2024.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0570178324000022},
author = {Mohamed G. Shehata and Saad H.D. Masry and Nourhan M. {Abd El-Aziz} and Fouad L. Ridouane and Shaher B. Mirza and Sobhy A. El-Sohaimy},
keywords = {Honeybees stomach, Probiotics, Antioxidant activity, Gut health, Microbiota balance, Fermentation},
abstract = {Probiotic lactic acid bacteria (LAB) have garnered substantial attention for their potential health benefits, particularly in supporting the balance of gut microbiota. This study sought to assess LAB isolates from honeybees stomach as potential probiotics by evaluating their tolerance to acid and bile, autoaggregation, hydrophobicity, co-aggregation with pathogens, antioxidant activity, haemolysis, exopolysaccharide (EPS) production, in vitro cell adherence, and their performance in milk-based fermented products. The LAB isolates exhibited impressive resilience to gastric acid, surviving exposure to simulated gastric juice at pH 2 after 2 h of incubation. Autoaggregation and hydrophobicity, crucial for probiotic adhesion to intestinal epithelial cells, were observed in several LAB isolates. Notably, Ehb3, Ehb5, and Ehb8 displayed the highest values, indicating their potential for effective intestinal adhesion. The antioxidant activities of intracellular and cell-free lactic acid bacteria strain extracts were evaluated using DPPH (2,2-Diphenyl-1-picrylhydrazyl) and ABTS (2,2′-azino-bis(3-ethylbenzothiazoline-6-sulfonic acid)) radical scavenging tests. Ehb3 and Ehb5 demonstrated outstanding antioxidant capabilities, suggesting their potential for enhancing the shelf life and health benefits of probiotic products. These lactic acid bacteria strains were also proficient in fermenting milk, maintaining viability above the technological requirements for probiotic products during storage. Finally, lactic acid bacteria isolate from honey bee stomach exhibit promising characteristics that make them suitable candidates for potential probiotics with health benefits.}
}
@article{CALISKAN2025102549,
title = {Photodynamic and cytotoxic activities of Onosma aucheriana extract: Molecular interactions and antioxidant potential},
journal = {European Journal of Integrative Medicine},
volume = {79},
pages = {102549},
year = {2025},
issn = {1876-3820},
doi = {https://doi.org/10.1016/j.eujim.2025.102549},
url = {https://www.sciencedirect.com/science/article/pii/S1876382025000988},
author = {Metin Caliskan and Sercin Ozlem Caliskan and Emine Incilay Torunoglu and Erdi Can Aytar and Alper Durmaz},
keywords = {Antioxidant, GC–MS analysis, Green light activation, Inositol, Molecular docking, , Stomach cancer},
abstract = {Introduction
This study investigates the anticancer, photodynamic, antioxidant efficiency and molecular interaction profiles of Onosma aucheriana methanol extract.
Method
The cell viability of normal (HEK293) and gastric cancer (MKN28) cell lines was evaluated using IC50 values, including photodynamic activity under green light. Antioxidant potential was assessed through DPPH radical scavenging, while phytochemical contents were quantified spectrophotometrically. Gas chromatography-mass spectrometry (GC–MS) analysis identified major compounds, and molecular docking studies explored their interactions with target proteins.
Results
Cell viability assays showed a dose-dependent reduction in cell viability, with IC50 values of 1036 μg/mL for HEK293 and 951.8 μg/mL for MKN28. To assess photodynamic activity, green light was used, revealing IC50 values of 56.62 μg/mL for normal cells and 32.53 μg/mL for cancer cells. A more pronounced reduction in cell viability was observed in cancer cells compared to normal cells. However, it demonstrates significant photodynamic activity at a concentration approximately 1/18th of the IC50 value for normal cells and about 1/29th of the IC50 value for cancer cells. The extract, with a DPPH radical scavenging IC₅₀ value of 154.15 ± 1.04 µg/mL, exhibits strong antioxidant activity, which is greater than that of BHT (230 ± 10 µg/mL). The total flavanol, flavonoid, phenolic, proanthocyanidin, and tannin contents are quantified as 24.30 ± 1.23 mg QE/g, 32.96 ± 3.23 mg QE/g, 6.31 ± 0.79 mg GAE/g, 83.41 ± 12.95 mg CAE/g, and 27.32 ± 2.30 mg GAE/g, respectively. GC–MS analysis reveals inositol (30.47 %), 4-((1E)‑hydroxy-1-propenyl)-2-methoxyphenol (23.04 %), 3-amino-2,6-dimethylpyridine (7.41 %), and 3-buten-2-one, 4-(4‑hydroxy-2,2,6-trimethyl-7-oxabicyclo[4.1.0] hept‑1-yl) (5.99 %) as major components. Molecular docking studies reveal that 3-buten-2-one, 4-(4‑hydroxy-2,2,6-trimethyl-7-oxabicyclo[4.1.0] hept‑1-yl) exhibits strong binding interactions with BAF complex (6LTJ), inositol shows substantial hydrogen bonding, and 4-((1E)‑hydroxy-1-propenyl)-2-methoxyphenol demonstrates significant binding with several receptors.
Conclusion
The O. aucheriana extract exhibited strong antioxidant activity and a novel photodynamic cytotoxic effect that became prominent under green light exposure. These findings suggest that the extract may possess versatile therapeutic potential.}
}
@article{MAHMOUDIDEHAKI2025100253,
title = {Automated vs. manual linguistic annotation for assessing pragmatic competence in English classes},
journal = {Research Methods in Applied Linguistics},
volume = {4},
number = {3},
pages = {100253},
year = {2025},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2025.100253},
url = {https://www.sciencedirect.com/science/article/pii/S2772766125000746},
author = {Mohsen Mahmoudi-Dehaki and Nasim Nasr-Esfahani},
keywords = {Artificial intelligence, Natural language processing, Pragmalinguistics, Sociopragmatics, Text data mining},
abstract = {Evaluating pragmatic competence remains a complex and critical challenge in applied linguistics, particularly in English as a Foreign Language (EFL) contexts. This study aims to address this gap by examining the potential of automating pragmatic competence assessment using AI-powered text analytics. Employing an explanatory sequential mixed-methods design, the quantitative phase compares the accuracy of automated versus manual linguistic annotation in evaluating the pragmatic skills of EFL learners. In the qualitative phase, factors influencing the accuracy of manual annotation are explored. For automated annotation, ChatGPT-4 Omni (ChatGPT-4o) processed 116 transcriptions representing participants' performances across six verbal discourse completion tasks (DCTs), encompassing prosodic features and pragmatic functions such as requesting favors, apologizing, suggesting, complaining, inviting, and refusing invitations. The AI model was fine-tuned using a human-in-the-loop approach, incorporating ensemble techniques such as few-shot learning and instructional prompts. Manual annotation employed trained EFL teachers using standardized assessment cards. Results indicate that automated annotation surpasses manual accuracy in evaluating most pragmatic components, except cultural norms, where both methods exhibit limitations. Focus group findings reveal that annotator bias, fatigue, technological influences, linguistic background differences, and subjectivity impact manual annotation accuracy. This interdisciplinary investigation expands the methodological toolkit for pragmatic competence evaluation and holds significant implications for fields such as digital humanities, computational pragmatics, language education, machine learning, and natural language processing.}
}
@article{NOCHETE2024106935,
title = {The spatio-temporal distribution of small-scale fisheries along the northern Panay Gulf, Philippines: Implications for management},
journal = {Fisheries Research},
volume = {272},
pages = {106935},
year = {2024},
issn = {0165-7836},
doi = {https://doi.org/10.1016/j.fishres.2023.106935},
url = {https://www.sciencedirect.com/science/article/pii/S0165783623003284},
author = {Charmane B. Nochete and Rex B. Baleña},
keywords = {Spatio-temporal distribution, Marine protected area, Seasonal closure, Small-scale fisheries, Panay Gulf},
abstract = {The small-scale fisheries along the northern region of the Panay Gulf face a distinctive challenge as multiple communities compete for access to the same fishing areas and resources. In response, neighboring coastal municipalities implemented conventional management measures, such as Marine Protected Areas (MPAs) and seasonal closures, aiming to enhance the abundance of fisheries resources. However, these measures only imposed economic hardships on artisanal fishermen. To address this issue, a survey involving 135 informants from coastal barangays was conducted between June to November 2021. The study used semi-structured questionnaires and GPS data loggers to collect essential information on the spatial and temporal distribution of fishing activities and specific target catches. The integration of local knowledge played a crucial role in bridging critical data gaps within the study site, facilitating the identification of more effective management reforms. The findings revealed that traditional harvest locations for various gear types exhibited zonation patterns across the gulf. Fishing activities were strategically conducted in areas with available resources, often disregarding municipal fishing boundaries. Coordinated management strategies are deemed imperative due to the varying timing of seasonal closures among municipalities. On the other hand, the temporal analysis of fishing activities revealed peak seasons of fisheries resources, providing a basis for adjusting the selection of target species and the duration of seasonal closures. Interestingly, the existing management measures failed to reduce the annual fishing activities and were ineffective, contrary to popular claims. This outcome was primarily due to constant shifts in the types of fishing gear used and target catches between seasons, highlighting the displacement of exploitation. In conclusion, this study underscores the importance of aligning fisheries policies with the dynamic nature of small-scale fisheries. Unless these policies adapt to the evolving conditions of the sector, they are likely to remain ineffective in addressing issues faced by artisanal fishermen in the region.}
}
@article{ZENG2025109221,
title = {Poly(Ionic Liquid) matrices embedded with liquid metal particles: A versatile solution for high-power density thermal management},
journal = {Composites Part A: Applied Science and Manufacturing},
volume = {199},
pages = {109221},
year = {2025},
issn = {1359-835X},
doi = {https://doi.org/10.1016/j.compositesa.2025.109221},
url = {https://www.sciencedirect.com/science/article/pii/S1359835X25005159},
author = {Jianhui Zeng and Taoying Rao and Ting Liang and Yimin Yao and Chaoyang Wang and Jian-Bin Xu and Liejun Li and Rong Sun},
keywords = {Thermal interface material, Polymer composite, Poly(ionic liquid), Liquid metal},
abstract = {Amid the global surge in generative AI and the resulting compute revolution, thermal management has emerged to be a pivotal determinant of its success. Innovation in thermal interface materials (TIMs) now represents a strategic frontier in shaping the trajectory of the Fourth Industrial Revolution. Conventional silicone-based TIMs face a performance dilemma comprising thermal cycling-induced interfacial delamination, aging-related increases in interfacial thermal resistance. Building on previous work that introduced poly(ionic liquid)s (PILs) as a novel alternative to silicones, this study further optimizes the molecular structure of PILs. Incorporation of ethoxy groups significantly enhances the mechanical compliance of PIL while maintaining high adhesion strength. Robust hydrogen bonding between ethoxy groups in PIL and liquid metal enables a high loading of 82 vol% without leakage, achieving a thermal conductivity of nearly 5 W m−1 K−1. Meanwhile, strong interfacial adhesion yields a interface contact thermal resistance of 0.74 ± 0.12 × 10-6 m2·K/W between the PIL/LM composite and Si, lower than that of silicone-based TIMs. The noncovalent self-healing of the PIL matrix effectively prevents crack formation in TIMs during aging. This work advances the application of PILs in TIMs and provides strategies for performance optimization, paving the way for their practical deployment as viable matrix alternatives.}
}
@article{OCONNELL2025104736,
title = {Enhancing the trustworthiness of pain research: A call to action.},
journal = {The Journal of Pain},
volume = {28},
pages = {104736},
year = {2025},
issn = {1526-5900},
doi = {https://doi.org/10.1016/j.jpain.2024.104736},
url = {https://www.sciencedirect.com/science/article/pii/S1526590024007144},
author = {Neil E. O’Connell and Joletta Belton and Geert Crombez and Christopher Eccleston and Emma Fisher and Michael C. Ferraro and Anna Hood and Francis Keefe and Roger Knaggs and Emma Norris and Tonya M. Palermo and Gisèle Pickering and Esther Pogatzki-Zahn and Andrew SC Rice and Georgia Richards and Daniel Segelcke and Keith M. Smart and Nadia Soliman and Gavin Stewart and Thomas Tölle and Dennis Turk and Jan Vollert and Elaine Wainwright and Jack Wilkinson and Amanda C.de C. Williams},
keywords = {Trustworthiness, Integrity, Equity, Engagement, Transparency, Rigour},
abstract = {The personal, social and economic burden of chronic pain is enormous. Tremendous research efforts are being directed toward understanding, preventing, and managing chronic pain. Yet patients with chronic pain, clinicians and the public are sometimes poorly served by an evidence architecture that contains multiple structural weaknesses. These include incomplete research governance, a lack of diversity and inclusivity, inadequate stakeholder engagement, poor methodological rigour and incomplete reporting, a lack of data accessibility and transparency, and a failure to communicate findings with appropriate balance. These issues span pre-clinical research, clinical trials and systematic reviews and impact the development of clinical guidance and practice. Research misconduct and inauthentic data present a further critical risk. Combined, they increase uncertainty in this highly challenging area of study and practice, drive the provision of low value care, increase costs and impede the discovery of more effective solutions. In this focus article, we explore how we can increase trust in pain science, by examining critical challenges using contemporary examples, and describe a novel integrated conceptual framework for enhancing the trustworthiness of pain science. We end with a call for collective action to address this critical issue.
Perspective
Multiple challenges can adversely impact the trustworthiness of pain research and health research more broadly. We present ENTRUST-PE, a novel, integrated framework for more trustworthy pain research with recommendations for all stakeholders in the research ecosystem, and make a call to action to the pain research community.}
}
@article{CHARYTAN2025169,
title = {Effects of dialysate potassium concentration of 3.0 mmol/l with sodium zirconium cyclosilicate on dialysis-free days versus dialysate potassium concentration of 2.0 mmol/l alone on rates of cardiac arrhythmias in hemodialysis patients with hyperkalemia},
journal = {Kidney International},
volume = {107},
number = {1},
pages = {169-179},
year = {2025},
issn = {0085-2538},
doi = {https://doi.org/10.1016/j.kint.2024.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0085253824007221},
author = {David M. Charytan and Wolfgang C. Winkelmayer and Christopher B. Granger and John P. Middleton and Charles A. Herzog and Glenn M. Chertow and James M. Eudicone and Jeremy D. Whitson and James A. Tumlin},
keywords = {arrhythmia, atrial fibrillation, end-stage kidney disease, hemodialysis, potassium, sodium zirconium cyclosilicate},
abstract = {The optimal approach towards managing serum potassium (sK+) and hemodialysate potassium concentrations is uncertain. To study this, adults receiving hemodialysis for three months or more with hyperkalemia (pre-dialysis sK+ 5.1–6.5 mmol/l) had cardiac monitors implanted and were randomized to either eight weeks of 2.0 mmol/l potassium/1.25 mmol/l calcium dialysate without sodium zirconium cyclosilicate (SZC) (2.0 potassium/noSZC) or 3.0 mmol/l potassium/1.25 mmol/l calcium dialysate combined with SZC (3.0 potassium/SZC) on non-dialysis days to maintain pre-dialysis sK+ 4.0–5.5 mmol/l, followed by treatment crossover for another eight weeks. The primary outcome was the rate of adjudicated atrial fibrillation (AF) episodes of at least 2 minutes duration. Secondary outcomes included clinically significant arrhythmias (bradycardia, ventricular tachycardia, and/or asystole) and the proportion of sK+ measurements within an optimal window of 4.0–5.5 mmol/l. Among 88 participants (mean age: 57.1 years; 51% male; mean pre-dialysis sK+: 5.5 mmol/l) with 25.5 person-years of follow-up, 296 AF episodes were detected in nine patients. The unadjusted AF rate was lower with 3.0 potassium/SZC versus 2.0 potassium/noSZC; 9.7 vs. 13.4/person-year (modeled rate ratio 0.52; 95% confidence interval 0.41–0.65). Clinically significant arrhythmias were reduced with 3.0 potassium/SZC vs. 2.0 potassium/noSZC (6.8 vs. 10.2/person-year modeled rate ratio 0.47; 0.38; 0.58). Fewer sK+ measurements outside the optimal window occurred with 3.0 potassium/SZC (modeled odds ratio: 0.27; 0.21–0.35). Hypokalemia was less frequent (33 vs. 58 patients) with 3.0 potassium/SZC compared with 2.0 potassium/noSZC. Thus, in patients with hyperkalemia on maintenance hemodialysis, a combination of hemodialysate potassium 3.0 mmol/l and SZC on non-hemodialysis days reduced the rates of AF, other clinically significant arrhythmias, and post-dialysis hypokalemia compared with hemodialysate potassium 2.0/noSZC.}
}
@article{VANMALI2024887,
title = {Cardiovascular Magnetic Resonance-Based Tissue Characterization in Patients With Hypertrophic Cardiomyopathy},
journal = {Canadian Journal of Cardiology},
volume = {40},
number = {5},
pages = {887-898},
year = {2024},
note = {Focus Issue: Hypertrophic Cardiomyopathy in Rapid Evolution},
issn = {0828-282X},
doi = {https://doi.org/10.1016/j.cjca.2024.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0828282X24002022},
author = {Atish Vanmali and Waleed Alhumaid and James A. White},
abstract = {Hypertrophic cardiomyopathy (HCM) is a common hereditable cardiomyopathy that affects between 1:200 to 1:500 of the general population. The role of cardiovascular magnetic resonance (CMR) imaging in the management of HCM has expanded over the past 2 decades to become a key informant of risk in this patient population, delivering unique insights into tissue health and its influence on future outcomes. Numerous mature CMR-based techniques are clinically available for the interrogation of tissue health in patients with HCM, inclusive of contrast and noncontrast methods. Late gadolinium enhancement imaging remains a cornerstone technique for the identification and quantification of myocardial fibrosis with large cumulative evidence supporting value for the prediction of arrhythmic outcomes. T1 mapping delivers improved fidelity for fibrosis quantification through direct estimations of extracellular volume fraction but also offers potential for noncontrast surrogate assessments of tissue health. Water-sensitive imaging, inclusive of T2-weighted dark blood imaging and T2 mapping, have also shown preliminary potential for assisting in risk discrimination. Finally, emerging techniques, inclusive of innovative multiparametric methods, are expanding the utility of CMR to assist in the delivery of comprehensive tissue characterization toward the delivery of personalized HCM care. In this narrative review we summarize the contemporary landscape of CMR techniques aimed at characterizing tissue health in patients with HCM. The value of these respective techniques to identify patients at elevated risk of future cardiovascular outcomes are highlighted.
Résumé
La cardiomyopathie hypertrophique (CMH) est une cardiomyopathie héréditaire courante qui affecte entre 1 : 200 et 1 : 500 de la population générale. Le rôle de l'imagerie par résonance magnétique cardiovasculaire (IRM-C) dans la prise en charge de la CMH s'est développé au cours des deux dernières décennies pour devenir un élément clé informant du risque pour cette population de patients, fournissant des informations uniques relatives à la santé des tissus et son influence sur les pronostics. De nombreuses techniques matures basées sur l'IRM-C sont disponibles en clinique pour examiner l'état de santé des tissus chez les patients atteints de CMH, incluant des méthodes avec et sans contraste. L'imagerie de rehaussement tardif au gadolinium reste une technique de base pour l'identification et la quantification de la fibrose myocardique, avec de nombreuses preuves cumulées soutenant sa valeur pour la prédiction des résultats arythmiques. La cartographie T1 offre une fidélité améliorée pour la quantification de la fibrose grâce à des estimations directes de la fraction du volume extracellulaire, mais elle offre également un potentiel pour des évaluations substitutives sans contraste de l'état de santé du tissu. L'imagerie, sensible au contenu en eau des tissus, y compris l'imagerie à contraste « sang noir » pondérée en T2 et la cartographie du temps de relaxation transversal T2, a également montré un potentiel préliminaire pour aider à la discrimination du risque. Enfin, les techniques émergentes, incluant des méthodes multiparamétriques innovantes, élargissent l'utilité de l'IRM-C pour aider à la caractérisation complète des tissus en vue d'une prise en charge personnalisée de la CMH. Dans cette revue narrative, nous résumons le panorama actuel des techniques d'IRM-C visant à caractériser l'état de santé des tissus chez les patients atteints de CMH. L'intérêt de ces techniques respectives pour identifier les patients présentant un risque élevé d'effets cardiovasculaires futurs est mis en évidence.}
}
@article{ARLEGI2025103745,
title = {Deciphering the correlated evolutionary responses of the hands and feet in modern humans},
journal = {Journal of Human Evolution},
volume = {207},
pages = {103745},
year = {2025},
issn = {0047-2484},
doi = {https://doi.org/10.1016/j.jhevol.2025.103745},
url = {https://www.sciencedirect.com/science/article/pii/S0047248425000983},
author = {Mikel Arlegi and Adrián Pablos and Carlos Lorenzo},
keywords = {Autopods, Evolutionary trajectories, Bayesian analysis},
abstract = {The coevolution of the hands and feet in modern humans has been a subject of significant interest due to their unique morphological features that differentiate humans from other primates and their implications in human evolution. This study aims to investigate the degree of correlated responses to selection between hands and feet and to determine whether one of the autopods has exerted a greater influence on this coevolution, focusing on their homologous elements and morphological traits. We analyzed the 38 long bones of the hands and feet from 96 modern human specimens, employing a comprehensive methodological framework that includes morphological analysis, assessments of modularity, integration, and covariation patterns under random selection. Additionally, Bayesian analyses were conducted to test whether foot morphology drives hand morphology or vice versa. Our findings indicate a high degree of morphological integration between the hands and feet, revealing a trend of increasing correlation from the first to the fifth ray. Consistent with previous studies, our Bayesian model provides robust evidence that the feet drive the morphological coevolution of human autopods, likely in response to functional selection pressures associated with bipedalism. However, our results also highlight that the intertwined evolutionary trajectories of the hands and feet are not a simple unidirectional model, underscoring the complexity of morphological integration and the diverse coevolutionary patterns among different rays, reflecting their specialized functions and evolutionary adaptations.}
}
@article{SHEN2024212902,
title = {A model for evaluating fracture leakage based on variations in bottom-hole temperature and pressure during the fracturing process},
journal = {Geoenergy Science and Engineering},
volume = {238},
pages = {212902},
year = {2024},
issn = {2949-8910},
doi = {https://doi.org/10.1016/j.geoen.2024.212902},
url = {https://www.sciencedirect.com/science/article/pii/S2949891024002720},
author = {Zhaolong Shen and Guofa Ji},
keywords = {Fracturing, Unconventional oil and gas, Fracture leakage, Leakage temperature and pressure},
abstract = {During the large-scale fracturing and exploitation of unconventional reservoirs, artificial fractures progressively widen in reservoirs with natural fracture development, which is highly prone to fracture leakage between segment clusters, thereby impacting the complexity of fracture networks. Nevertheless, the existing research on the evaluation model of fracture leakage in the fracturing process is limited and lacks precise predictive capabilities. The model has been developed to assess the variations in bottom hole pressure and bottom hole temperature during fracture extension by considering the combined effects of pressure loss, temperature exchange, along-track drag, static column pressure, and integrated heat transfer among the fracturing fluid, tubing wall, annulus, casing wall, and formation. The model is capable of evaluating the occurrence of fracture leakage by analyzing the extent of temperature and pressure variations at the base of the wellbore. The study findings compare the temperature and pressure data from 14 out of 144 actual fracture leakage sections of three wells monitored in an oilfield, and it is believed that the situation of fracture leakage can be identified when the predicted temperature changes fall within the range of −2.95 °C–7.53 °C, and the predicted pressure changes range from −0.87 MPa to 6.55 MPa. The model yields an average error of 4.09% for pressure and 5.73% for temperature. Moreover, the model demonstrates high overall prediction accuracy, which holds significant implications for promptly identifying inter-segmental fracture leakage and facilitating the efficient development of oil and gas fields.}
}
@article{JIN2025108538,
title = {Modeling AI-assisted writing: How self-regulated learning influences writing outcomes},
journal = {Computers in Human Behavior},
volume = {165},
pages = {108538},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108538},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224004060},
author = {Fangzhou Jin and Chin-Hsi Lin and Chun Lai},
keywords = {Artificial intelligence, Self-regulated learning, Writing, Chatbot, ChatGPT},
abstract = {Academic writing is essential to academic and professional success, yet remains a challenge for many students. Artificial intelligence (AI) offers a potential solution, but most research on that possibility has focused on final written products rather than on the writing process. This study helps to fill that gap by modeling how key variables interact in generative AI-assisted writing processes, based on survey data from 1073 postgraduate students from 21 countries studying in the UK. We used structural equation modeling to categorize AI use into three levels, from basic to advanced: 1) for technical support, 2) for text development, and 3) for transformation. Self-regulated learning (SRL) strategies positively predicted all three types of AI use. Notably, while the most advanced use of AI (i.e., for writing transformation) significantly enhanced outcomes including critical thinking, motivation, and writing quality, whereas the most basic use (for technical support) did not predict such outcomes. This study further revealed that AI self-efficacy and writing self-efficacy were significant antecedents of self-regulation, suggesting the importance of supporting students’ self-efficacy in boosting self-regulation in AI use. This suggests that the key to writing-outcome improvement may not be to teach students different uses of AI, but to develop their self-regulation to the point that they can independently explore and apply advanced uses of this technology.}
}
@article{HEMALATHA2024138322,
title = {Indole-imidazole hybrid Schiff base for the selective detection of the explosive picric acid via fluorescence “turn-off” process: Experimental and theoretical DFT-D3 study},
journal = {Journal of Molecular Structure},
volume = {1311},
pages = {138322},
year = {2024},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2024.138322},
url = {https://www.sciencedirect.com/science/article/pii/S0022286024008421},
author = {V. Hemalatha and V. Vijayakumar},
keywords = {Indole imidazole hybrid Schiff base, Turn-off sensor, ICT-PET-FRET, Picric acid, TEA treatment},
abstract = {A compound containing a heteroatom and Schiff base, 3-(4,5-Diphenyl-1H-imidazol-2-yl)-1H-indole (INBIL) has been synthesized through the one-pot condensation of indole-3-carbaldehyde with benzil in ethanol and characterized using various analytical parameters like FT-IR, NMR; its thermal and chemical stability also established. The synthesized INBIL showed a 'turn off' luminescent property that has been used to detect picric acid. This luminescent sensor has a broad emission band at 415 nm in acetonitrile due to the conjugation of its double bond. It is worth noting that INBIL can be recycled through TEA treatment. The chemosensor exhibited the highest sensitivity with Ksv = 1.51 × 10−6 M, and a detection limit of 2.91 × 10−7 M, with perfect linearity up to 22 µL. The binding stoichiometry ratio between INBIL and picric acid was predicted to be 1:1 using Job's plot, which was confirmed by 1H NMR analysis. Finally, DFT-D3 calculations were used to calculate the excited molecule's singlet and triplet energy states.}
}
@article{MUTAVHATSINDI2024106173,
title = {Baseline and end-of-treatment host serum biomarkers predict relapse in adults with pulmonary tuberculosis},
journal = {Journal of Infection},
volume = {89},
number = {1},
pages = {106173},
year = {2024},
issn = {0163-4453},
doi = {https://doi.org/10.1016/j.jinf.2024.106173},
url = {https://www.sciencedirect.com/science/article/pii/S0163445324001075},
author = {Hygon Mutavhatsindi and Charles M. Manyelo and Candice I. Snyders and Ilana {Van Rensburg} and Martin Kidd and Kim Stanley and Gerard Tromp and Reynaldo Dietze and Bonnie Thiel and Paul D. {van Helden} and John T. Belisle and John L. Johnson and W. Henry Boom and Gerhard Walzl and Novel N. Chegou},
keywords = {Tuberculosis, Treatment response, Relapse, Biomarkers, Biosignatures},
abstract = {Summary
Background
There is a need for new tools for monitoring of the response to TB treatment. Such tools may allow for tailored treatment regimens, and stratify patients initiating TB treatment into different risk groups. We evaluated combinations between previously published host biomarkers and new candidates, as tools for monitoring TB treatment response, and prediction of relapse.
Methods
Serum samples were collected at multiple time points, from patients initiating TB treatment at research sites situated in South Africa (ActionTB study), Brazil and Uganda (TBRU study). Using a multiplex immunoassay platform, we evaluated the concentrations of selected host inflammatory biomarkers in sera obtained from clinically cured patients with and without subsequent relapse within 2 years of TB treatment completion.
Results
A total of 130 TB patients, 30 (23%) of whom had confirmed relapse were included in the study. The median time to relapse was 9.7 months in the ActionTB study (n = 12 patients who relapsed), and 5 months (n = 18 patients who relapsed) in the TBRU study. Serum concentrations of several host biomarkers changed during TB treatment with IL-6, IP-10, IL-22 and complement C3 showing potential individually, in predicting relapse. A six-marker signature comprising of TTP, BMI, sICAM-1, IL-22, IL-1β and complement C3, predicted relapse, prior to the onset of TB treatment with 89% sensitivity and 94% specificity. Furthermore, a 3-marker signature (Apo-CIII, IP-10 and sIL-6R) predicted relapse in samples collected at the end of TB treatment with sensitivity of 71% and specificity of 74%. A previously identified baseline relapse prediction signature (TTP, BMI, TNF-β, sIL-6R, IL-12p40 and IP-10) also showed potential in the current study.
Conclusion
Serum host inflammatory biomarkers may be useful in predicting relapse in TB patients prior to the initiation of treatment. Our findings have implications for tailored patient management and require prospective evaluation in larger studies.}
}
@article{LIU2025104176,
title = {The mediating effect of user satisfaction and the moderated mediating effect of AI anxiety on the relationship between perceived usefulness and subscription payment intention},
journal = {Journal of Retailing and Consumer Services},
volume = {84},
pages = {104176},
year = {2025},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2024.104176},
url = {https://www.sciencedirect.com/science/article/pii/S0969698924004727},
author = {Yang Liu and Younggeun Park and Huizhong Wang},
keywords = {Perceived usefulness, Generative AI, User satisfaction, Subscription payment intention, AI anxiety},
abstract = {Generative AI applications like ChatGPT from OpenAI help users reach new levels of productivity. ChatGPT has gained countless new users signing up and sharing use cases online since its release. The paid version, GPT 4.0, was also recognized by some subscribers. However, there is a dearth of theoretical understanding regarding the generative AI subscription intentions of users. This study endeavors to explore the influence of ChatGPT users' perceived usefulness on both satisfaction and their intention to pay for a subscription, as well as the moderated mediating effect of artificial intelligence anxiety on the process of user decision-making. To achieve this goal, 274 samples were collected and validated for reliability analysis, validity analysis, regression analysis, and moderated mediation analysis by AMOS 28 and SPSS 27. The findings of this research demonstrate that perceived usefulness influences user satisfaction positively and further affects subscription payment intention. Furthermore, user satisfaction serves as a partial mediator in the connection between perceived usefulness and subscription payment intention. By employing moderated mediation analysis, this paper discovers that AI anxiety amplifies the aforementioned associations, which is counterintuitive yet intriguing. Overall, this study adds to the knowledge of generative AI users’ subscription payment intention and provides valuable insights for designing and popularizing generative AI tools.}
}