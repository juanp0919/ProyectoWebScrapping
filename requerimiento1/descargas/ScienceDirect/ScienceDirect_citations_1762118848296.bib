@article{LI2025104258,
title = {Provoking critical thinking: Using counter-arguments in online discussion summarisation},
journal = {Information Processing & Management},
volume = {62},
number = {6},
pages = {104258},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104258},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001992},
author = {Shangqian Li and Lei Han and Gianluca Demartini},
keywords = {Generative artificial intelligence, Online opinion modelling, Human-computer interaction, Human-centred computing},
abstract = {Generative AI systems based on Large Language Models (LLMs), like ChatGPT, have brought profound convenience to users thanks to their ability to summarise existing documents and to generate new text. This shows the potential to summarise online human discussions or debates for new entrants to quickly comprehend the ongoing matters and arguments and to get efficiently involved in the opinion deliberation process. However, generative AI has frequently been associated with negatively affecting users’ decision making. In this paper, we study a novel approach based on generative AI to trigger users’ critical thinking by challenging fresh counter-arguments after summarising existing online discussions for incoming users. We conduct a user study with 558 participants to determine the effectiveness and fairness of AI summarisation across three online platforms — Reddit, Kialo, and Debatewise. Our results show that the intervention methods and platform differences are strongly associated with participants’ level of opinion change and the strength of their belief. We found that participants’ opinion changes affected their perceived usefulness of the AI system. Our work opens the door to LLM applications helping Web users participate in online opinion deliberation more efficiently, with a higher level of critical thinking, and with a reduced negative attitude.}
}
@article{SIMEONE202537,
title = {Conceptualisation of a multimodal, non-intrusive, generative AI-based assistive system for assembly},
journal = {CIRP Annals},
volume = {74},
number = {1},
pages = {37-41},
year = {2025},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2025.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S0007850625001088},
author = {Alessandro Simeone and Yuchen Fan and Dario Antonelli and Paolo C. Priarone and Luca Settineri},
keywords = {Generative artificial intelligence, Assembly, Multimodal assistance},
abstract = {The transition to Industry 5.0 highlights the necessity for human-centric and adaptive manufacturing systems. This study conceptualises a multimodal, generative AI-based assistive system for assembly designed to deliver real-time error detection and adaptive guidance tailored to diverse operator profiles. The system improves human-machine interaction by issuing preventive warnings to the operator prior to critical tasks, detecting assembly errors, providing multimodal corrective instructions during operations, and deploying robotic interventions when operator-driven corrections prove inadequate. Preliminary laboratory-scale implementation results show the system capability in mitigating assembly errors through dynamic assistive technology selection and iterative feedback learning.}
}
@article{HWANG2025104266,
title = {Facilitating students’ critical thinking, metacognition and problem-solving tendencies in geriatric nursing class: A mixed-method study},
journal = {Nurse Education in Practice},
volume = {83},
pages = {104266},
year = {2025},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2025.104266},
url = {https://www.sciencedirect.com/science/article/pii/S1471595325000228},
author = {Gwo-Jen Hwang and Pei-Yu Cheng and Ching-Yi Chang},
keywords = {Generative AI-guided prompt-based learning, Critical thinking tendency, Metacognition tendency, Problem-solving tendency, Question generating ability},
abstract = {Aim
The aim of this study was to explore the use of generative artificial intelligence (GenAI) in geriatric nursing classes for the design of older adult activities to educate students on how to pose clear questions, provide and identify potentially suitable daily activities for older adults.
Background
Researchers in various educational fields are increasingly employing GenAI tools such as ChatGPT for curriculum development and research. Question generation is an essential skill for all students to learn to acquire knowledge. However, there is limited experimental evidence on teaching students to correctly use GenAI for assisting with question generation ability and empirical data related to improving students' capacity for solving complex problems remains scarce.
Design
A mixed-method study design with both quantitative and qualitative analysis.
Methods
This study investigated the effectiveness of a GenAI-guided prompt-based learning approach implemented in a geriatric nursing class for first-year undergraduate students, involving a cohort of 56 participants.
Results
Experimental results indicated that the GenAI-guided prompt-based learning approach significantly enhanced students' critical thinking, metacognition and problem-solving tendencies and their question generation via prompts performance. Moreover, participants who engaged in the GenAI-guided prompt-based learning approach found the tasks easier to complete and required less cognitive effort.
Conclusions
Nursing students using the GenAI-guided prompt-based learning approach outperformed the control group in cognitive network analysis dimensions of clarity, relevance, complexity, precision and engagement. Thus, integrating GenAI prompts into course activities can effectively improve student learning outcomes, reduce metacognitive load and assist in solving learning problems.}
}
@article{WANG2024103861,
title = {Conversational AI chatbots as counselors for hospitality employees},
journal = {International Journal of Hospitality Management},
volume = {122},
pages = {103861},
year = {2024},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2024.103861},
url = {https://www.sciencedirect.com/science/article/pii/S0278431924001737},
author = {Yao-Chin Wang and Oscar Hengxuan Chi and Hiroaki Saito and Yue (Darcy) Lu},
keywords = {Conversational AI, Chatbot, Generative AI, Artificial intelligence, Expectancy theory, Hospitality employee, Counseling, General app fatigue},
abstract = {Considering the advancement of generative artificial intelligence and the mental health challenges faced by hospitality employees, this study proposes and tests a scenario of counseling hospitality employees using conversational AI chatbots in a mobile app. Survey data were collected from 553 hospitality employees in Japan. Findings reveal that employees’ expected psychological safety of AI chatbots can be improved by AI chatbots’ cuteness of appearance and emotional capability, and their expected service quality in using AI chatbots can be enhanced by cuteness of appearance, emotional capability, and psychological safety. The service quality of AI chatbots then causes their use intention, which then strengthens their willingness to recommend and help colleagues’ usage of AI chatbots for counseling. Moreover, employees’ general app fatigue can moderate the effects of AI chatbots’ cuteness of appearance on psychological safety and service quality. This study offers valuable practical implications for developing conversational AI chatbots for hospitality employees.}
}
@article{LIBERA2025,
title = {ChatGPT in the Working World:},
journal = {Information Resources Management Journal},
volume = {38},
number = {1},
year = {2025},
issn = {1040-1628},
doi = {https://doi.org/10.4018/IRMJ.386593},
url = {https://www.sciencedirect.com/science/article/pii/S1040162825000187},
author = {Pedro Schötteler Libera and Volker Bilgram and Sebastian Schötteler and Jan Mammen},
keywords = {Generative AI, Technology Adoption, Human-AI Interaction, Organizational Malleability, Qualitative Research, ChatGPT, Job Displacement, Future Work},
abstract = {ABSTRACT
The authors investigated how ChatGPT transforms workplace tasks by analyzing qualitative survey responses from 78 U.S. professionals in the fields of software, marketing, and academia. This study addresses a gap in understanding the malleability of generative artificial intelligence in diverse professional contexts, a need underscored by ChatGPT’s rapid adoption and mixed impact on work practices. Using a qualitative survey deployed via a validated platform, the authors collected open-ended responses about tasks, challenges, and opportunities. Responses were inductively coded to compare domain-specific applications. The findings show that although ChatGPT is widely used across sectors for tasks such as content creation, research, and idea generation, certain tasks—such as coding in software, strategic communication in marketing, and knowledge acquisition in academia—diverge. The results emphasize the importance of context-sensitive integration strategies and bottom-up adoption approaches to maximize the benefits of artificial intelligence while mitigating risks.}
}
@article{RIDER202581,
title = {Evaluating large language model performance to support the diagnosis and management of patients with primary immune disorders},
journal = {Journal of Allergy and Clinical Immunology},
volume = {156},
number = {1},
pages = {81-87},
year = {2025},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2025.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0091674925001666},
author = {Nicholas L. Rider and Yingya Li and Aaron T. Chin and Daniel V. DiGiacomo and Cullen Dutmer and Jocelyn R. Farmer and Kirk Roberts and Guergana Savova and Mei-Sing Ong},
keywords = {Primary immune disorders, inborn errors of immunity, large language models, generative AI, health care AI, medical chatbot},
abstract = {Background
Generative artificial intelligence (GAI) is transforming health care in a variety of ways; however, the present utility of GAI for supporting clinicians who treat rare disease such as primary immune disorders (PIs) is not well studied. We evaluated the ability of 6 state-of-the-art large language models (LLMs) for providing clinical guidance about PIs.
Objective
To quantitatively and qualitatively measure the utility of current, open-source LLMs for diagnosing and providing helpful clinical decision support about PIs.
Methods
Five expert clinical immunologists each provided 5 real-world, anonymized PI case vignettes via multi-turn prompting to 6 LLMs (OpenAI GPT-4o, Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, Mistral-7B-Instruct-v0.3, Mistral-Large-Instruct-2407, Mixtral-8x7B-Instruct-v0.1). We assessed the diagnostic accuracy of the LLMs and the quality of clinical reasoning using the Revised-IDEA (R-IDEA) score. Qualitative LLM assessment was made by immunologist narratives.
Results
Performance accuracy (>88%) and R-IDEA scores (≥8) were superior for 3 models (GPT-4o, Llama-3.1-70B-Instruct, Mistral-Large-Instruct-2407), with GPT-4o achieving the highest diagnostic accuracy (96.2%). Conversely, the remaining 3 models fell below acceptable accuracy rates near 60% or lower and had poor R-IDEA scores (≤0.55), with Mistral-7B-Instruct-v0.3 attaining the worst diagnostic accuracy (42.3%). Compared with the 3 best-performing LLMs, the 3 worst-performing LLMs had a substantially lower median R-IDEA score (P < .001). Interclass correlation coefficient for R-IDEA score assignments varied substantially by LLM, ranging from good to poor agreement, and did not appear to correlate with either diagnostic accuracy or median R-IDEA score. Qualitatively, immunologists identified several themes (eg, correctness, differential diagnosis appropriateness, relative conciseness of explanations) of relevance to PIs.
Conclusions
LLM can support diagnosis and management of PIs; however, further tuning is needed to optimize LLMs for best practice recommendations.}
}
@article{KAGIYAMA2025,
title = {PRIME 2.0: Proposed Requirements for Cardiovascular Imaging-Related Multimodal-AI Evaluation: An Updated Checklist},
journal = {JACC: Cardiovascular Imaging},
year = {2025},
issn = {1936-878X},
doi = {https://doi.org/10.1016/j.jcmg.2025.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X25004668},
author = {Nobuyuki Kagiyama and Márton Tokodi and Quincy A. Hathaway and Rima Arnaout and Rhodri Davies and Damini Dey and Nicolas Duchateau and Alan G. Fraser and Shinichi Goto and Ankush D. Jamthikar and Carolyn S.P. Lam and Evangelos K. Oikonomou and David Ouyang and Ambarish Pandey and Timothy J. Poterucha and Zahra Raisi-Estabragh and Jordan B. Strom and Qiang Zhang and Naveena Yanamala and Partho P. Sengupta},
keywords = {artificial intelligence, cardiovascular imaging, clinical validation, deep learning, large language models, model development, multimodal generative artificial intelligence, PRIME 2.0 checklist, transparency and reproducibility},
abstract = {The PRIME (Proposed Requirements for Cardiovascular Imaging–Related Machine Learning Evaluation) 2.0 checklist is an updated, domain-specific framework designed to standardize the development, evaluation, and reporting of artificial intelligence (AI) applications in cardiovascular imaging. This update specifically responds to rapid advances from traditional machine learning to deep learning, large language models, and multimodal generative AI. The updated checklist was developed through a modified Delphi process by an international panel of clinical and technical experts. In contrast to general AI reporting guidelines, it delivers detailed, practical recommendations on all critical aspects of AI research and builds upon the original 7-domain framework by incorporating cardiovascular imaging–specific complexities such as cardiac motion, imaging artifacts, and interobserver variability. By promoting transparency and rigor, PRIME 2.0 can serve as a vital resource for researchers, clinicians, peer reviewers, and journal editors working at the forefront of AI in cardiovascular imaging.}
}
@article{LI2025100445,
title = {Two years of innovation: A systematic review of empirical generative AI research in language learning and teaching},
journal = {Computers and Education: Artificial Intelligence},
volume = {9},
pages = {100445},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100445},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000852},
author = {Belle Li and Yaling Lily Tan and Chaoran Wang and Victoria Lowell},
keywords = {Generative artificial intelligence, Language education, Systematic review, ChatGPT, Empirical research, Language learning and teaching},
abstract = {This systematic review examines the evolution of empirical research on generative AI in language learning and teaching from 2023 to 2024. Following PRISMA guidelines, we analyzed 144 peer-reviewed articles from Web of Science, Scopus, and ERIC databases to explore the field's progression, including identifying new developments, shifts in research priorities, and emerging themes that have arisen compared to the first year. The findings reveal an exponential growth in publications, with a significant shift from primarily exploratory inquiries to more systematic and empirically driven investigations. The analysis identified six main research foci: Perceptions and attitudes, psychological and cognitive aspects, teaching and learning strategies, language skills development, writing and feedback, and implementation and integration. While higher education (86.7 %) and English as Foreign Language contexts (86.1 %) dominated the research landscape, there was notable geographical diversity, with strong representation from East Asia and emerging contributions from the Middle East. Mixed-methods approaches (38.9 %) were prevalent, with increasing incorporation of AI-generated content, multimedia recordings, and digital interaction data from 2023 to 2024. Writing emerged as the primary focus (42.4 %) while speaking, listening, and reading skills received comparatively less attention. This review highlights critical gaps, including limited research in K-12 settings, insufficient longitudinal studies, and the need for more diverse language representation beyond English. These findings suggest the field is maturing but requires broader investigation across educational levels, language domains, and geographical contexts to fully understand the impact of generative AI in language education.}
}
@article{XIAO2024104125,
title = {A fair and scalable watermarking scheme for the digital content trading industry},
journal = {Computers in Industry},
volume = {161},
pages = {104125},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104125},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000538},
author = {Xiangli Xiao and Moting Su and Jiajia Jiang and Yushu Zhang and Zhongyun Hua and Zhihua Xia},
keywords = {Digital content trading, buyer–seller watermarking, Cryptography, Client-side embedding},
abstract = {The booming Internet economy and generative artificial intelligence have driven the rapid growth of the digital content trading industry, creating an urgent need for the fair protection of the rights of both buyers and sellers. To meet this need, a technique known as buyer–seller watermarking has emerged. Despite its existence, the majority of existing buyer–seller watermarking schemes adopt the owner-side embedding mode, which results in poor scalability. While a handful of schemes adopt the client-side embedding mode to enhance scalability, they either require the deep involvement of a trusted third party or fall short of ensuring complete fairness due to the unresolved unbinding problem. To address these challenges, this paper proposes a fair and scalable watermarking scheme for digital content transactions based on proxy re-encryption and digital signatures. For one thing, this scheme solves the unbinding problem and ensures complete fair protection of the rights of both buyers and sellers. For another, it adopts the client-side embedding mode and has good scalability. Additionally, it eliminates the need for a trusted third party. Finally, theoretical analysis and experiments demonstrate that the proposed scheme achieves the intended design goals and possesses superior efficiency advantages.}
}
@article{ALAIN2025,
title = {Evaluating User Interactions and Adoption Patterns of Generative AI in Health Care Occupations Using Claude: Cross-Sectional Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/73918},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125014177},
author = {Gabriel Alain and James Crick and Ella Snead and Catherine C Quatman-Yates and Carmen E Quatman},
keywords = {communication, humans, artificial intelligence, language, machine learning, self-management, information-seeking behavior, outcome assessment/health care, referral and consultation, patient care, workflow, patient participation, health literacy},
abstract = {Background
Generative artificial intelligence (GenAI) systems like Anthropic’s Claude and OpenAI’s ChatGPT are rapidly being adopted in various sectors, including health care, offering potential benefits for clinical support, administrative efficiency, and patient information access. However, real-world adoption patterns and the extent to which GenAI is used for health care–related tasks remain poorly understood and distinct from performance benchmarks in controlled settings. Understanding these organic usage patterns is key for assessing GenAI’s impact on health care delivery and patient-provider dynamics.
Objective
This study aimed to quantify the real-world frequency and scope of health care–related tasks performed using Anthropic’s Claude GenAI. We sought to (1) measure the proportion of Claude interactions related to health care tasks versus other domains; (2) identify specific health care occupations (as per O*NET classifications) with high associated interaction volumes; (3) assess the breadth of task adoption within roles using a “digital adoption rate”; and (4) interpret these findings considering the inherent ambiguity regarding user identity (ie, professionals vs public) in the dataset.
Methods
We performed a cross-sectional analysis of more than 4 million anonymized user conversations with Claude (ie, including both free and pro subscribers) from December 2024 to January 2025, using a publicly available dataset from Anthropic’s Economic Index research. Interactions were preclassified by Anthropic’s proprietary Clio model into standardized occupational tasks mapped to the US Department of Labor’s O*NET database. The dataset did not allow differentiation between health care professionals and the general public as users. We focused on interactions mapped to O*NET Healthcare Practitioners and Technical Occupations. Main outcomes included the proportion of interactions per health care occupation, proportion of overall health care interaction versus other categories, and the digital adoption rate (ie, distinct tasks performed via GenAI divided by the total possible tasks per occupation).
Results
Health care–related tasks accounted for 2.58% of total analyzed GenAI conversations, significantly lower than domains such as computing (37.22%). Within health care, interaction frequency varied notably by role. Occupations emphasizing patient education and guidance exhibited the highest proportion, including dietitians and nutritionists (6.61% of health care conversations), nurse practitioners (5.63%), music therapists (4.54%), and clinical nurse specialists (4.53%). Digital adoption rates (task breadth) ranged widely across top health care roles (13.33%‐65%), averaging 16.92%, below the global average (21.13%). Tasks associated with medical records and health information technicians had the highest adoption rate (65.0%).
Conclusions
GenAI tools are being adopted for a measurable subset of health care–related tasks, with usage concentrated in specific, often patient-facing roles. The critical limitation of user anonymity prevents definitive conclusions regarding whether usage primarily reflects patient information–seeking behavior (potentially driven by access needs) or professional workflow assistance. This ambiguity necessitates caution when interpreting current GenAI adoption. Our findings emphasize the urgent need for strategies addressing potential impacts on clinical workflows, patient decision-making, information quality, and health equity. Future research must aim to differentiate user types, while stakeholders should develop targeted guidance for both safe patient use and responsible professional integration.}
}
@article{YUSUF2024101619,
title = {Implementing a proposed framework for enhancing critical thinking skills in synthesizing AI-generated texts},
journal = {Thinking Skills and Creativity},
volume = {53},
pages = {101619},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101619},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124001573},
author = {Abdullahi Yusuf and Shamsudeen Bello and Nasrin Pervin and Abdullahi Kadage Tukur},
keywords = {Critical thinking, AI-generated texts, Framework, Epistemic network analysis, Co-occurrences},
abstract = {Since the development of open and low-cost generative artificial intelligence (GenAI), the higher education community has witnessed high use of AI-generated texts in scholarly research and academic assignments, attracting ongoing debate about whether such practices constitute cheating. While scholars argue that integrating GenAI tools can enhance productivity, critics raise concerns about the negative effect of such integration on critical thinking (CrT). This study therefore proposed a framework for enhancing students’ CrT skills in synthesizing AI-generated information. The proposed framework is underpinned by various theoretical foundations, encompassing five interconnected step-wise phases (familiarizing, conceptualizing, inquiring, evaluating, and synthesizing). The study was conducted under two separate experiments. The first experiment (Study 1) validated the effectiveness of the proposed framework, providing CrT training to 179 postgraduate students. In the second study (n = 125), additional experiments were undertaken to confirm the effectiveness of the framework in different contexts. An experimental procedure involving pretest and posttest design was implemented wherein participants were randomly allocated to one of three groups: experimental group 1 (exposed to our framework), experimental group 2 (exposed to an alternative self-regulated learning framework), and a control group (exposed to a non-structured framework). Results from Study 1 revealed that the framework enhances students’ CrT skills to synthesize AI-generated texts. However, these CrT skills manifested through various rigorous training aimed at reinforcing learning. While the proposed framework holds considerable value in cultivating CrT skills, significant differences arise across various personality traits. In Study 2, the framework proved to be effective in different contexts. However, it did not make a difference, particularly in its capacity to enhance students’ self-regulated learning compared to other frameworks. We discussed the implications of the findings and recommended it to educators seeking to prepare students for the challenges of the AI-driven knowledge economy.}
}
@article{HUANG2025104152,
title = {Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104152},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104152},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000937},
author = {Yinghui Huang and Weijun Wang and Jinyi Zhou and Liang Zhang and Jionghao Lin and Hui Liu and Xiangen Hu and Zongkui Zhou and Wanghao Dong},
keywords = {ChatGPT, Mental health Q&A, Large language model, Prompt engineering, Integrative modeling, LLMs evaluation},
abstract = {Recent advancements in generative artificial intelligence (GenAI), particularly ChatGPT, have demonstrated significant potential in addressing the persistent treatment gap in mental health care. Systematic evaluation of ChatGPT’s capabilities in addressing mental health questions is essential for its large-scale application. The current study introduces a computational evaluation framework centered on perceived information quality (PIQ) to quantitatively assess ChatGPT’s capabilities. Leveraging datasets of question-answer pairs generated by both humans and ChatGPT, the framework integrates predictive modeling, explainable modeling, and prompt-engineering-based validation to identify intrinsic evaluation metrics and enable automated assessments. Results revealed that unprompted ChatGPT’s PIQ is significantly lower than that of human counselors overall, with notable deficiencies such as insufficient conversational length, lower text diversity, and reduced professionalism. Despite not matching the top 25% of human counselors, our evaluation framework improved ChatGPT’s mean PIQ by 8.91% to 11.67% across four risk levels. Prompted ChatGPT performed comparably to human counselors in severe (p = 0.0561) and moderate-risk questions (p = 0.7851), and significantly outperformed them in low- and no-risk categories by 6.80% and 4.63%, respectively (p < 0.001). However, undesirable verbal behaviors still persist in text diversity and professionalism. These findings validate ChatGPT’s capabilities to address mental health questions while cautioning that further researches are necessary for LLM-based mental health systems to deliver services comparable to human experts.}
}
@article{GUO2024105787,
title = {Automatic assessment of concrete cracks in low-light, overexposed, and blurred images restored using a generative AI approach},
journal = {Automation in Construction},
volume = {168},
pages = {105787},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105787},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524005235},
author = {Pengwei Guo and Xiangjun Meng and Weina Meng and Yi Bao},
keywords = {Blurred image, Computer vision, Conditional Generative Adversarial Network (CGAN), Crack inspection, Deep learning, Generative AI, Image restoration, Low-light image, Overexposed image},
abstract = {Deep learning-based computer vision techniques have high efficiency in assessing concrete cracks from images, and the assessment can be automated using robots for higher efficiency. However, assessment accuracy is often compromised by low-quality images. This paper presents a Conditional Generative Adversarial Network (CGAN)-based approach to restore low-light, overexposed, and blurred images. The approach integrates attention mechanisms and residual learning and uses Wasserstein loss with gradient penalty. Crack assessment results show that the proposed approach outperforms state-of-the-art methods, regarding structural similarity (SSIM: 0.78 for deblurring, 0.95 for low-light enhancement, and 0.96 for overexposure correction) and peak signal-to-noise ratio (PSNR: 28.6 for deblurring, 31.4 for low-light enhancement, and 31.6 for overexposure correction). Restored images have been used to train a deep learning model for assessing concrete cracks. The Intersection over Union (IoU) and F1 score of crack segmentation are higher than 0.98 and 0.99, respectively, revealing high accuracy in crack assessment tasks.}
}
@article{MOUSAVI2025125296,
title = {Revolutionizing solar energy resources: The central role of generative AI in elevating system sustainability and efficiency},
journal = {Applied Energy},
volume = {382},
pages = {125296},
year = {2025},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2025.125296},
url = {https://www.sciencedirect.com/science/article/pii/S0306261925000261},
author = {Rashin Mousavi and Arash Mousavi and Yashar Mousavi and Mahsa Tavasoli and Aliasghar Arab and Ibrahim Beklan Kucukdemiral and Alireza Alfi and Afef Fekih},
keywords = {Generative artificial intelligence, Solar energy systems, AI-driven solar solutions, Solar photovoltaic systems design and optimization, Solar systems predictive maintenance},
abstract = {Driven by growing environmental concerns, such as global warming and the depletion of fossil fuels, the renewable energy industry, particularly solar energy, has risen to global prominence. In this context, generative artificial intelligence (Gen-AI) can play a valuable role in facilitating the development of more efficient, durable, and adaptable solar systems. Gen-AI’s multifaceted proficiency, from predictive maintenance and reducing downtime and costs to vital forecasting for grid management and strategic planning, extends to optimizing site selection for solar farms and smart grid integration, thereby enhancing solar energy flow, grid stability, and sustainable operation. This paper presents a comprehensive exploration of the role of Gen-AI in revolutionizing the solar energy industry. Focusing on various aspects of solar energy systems, including design, optimization, sizing, maintenance, energy forecasting, site selection, and smart grid integration, the study investigates the transformative impact of Gen-AI across these domains. It demonstrates how Gen-AI enhances the efficiency, sustainability, and adaptability of solar systems, driving strategic decision-making and optimizing the integration of solar power within complex energy ecosystems. Furthermore, the paper concludes by discussing the challenges and future prospects of employing Gen-AI in the solar energy domain, providing a comparative analysis of the current and future scenarios, and underscoring the advantages, disadvantages, and challenges of Gen-AI implementation.}
}
@article{UKWANDU2025103616,
title = {The future of teaching and learning in the context of emerging artificial intelligence technologies},
journal = {Futures},
volume = {171},
pages = {103616},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2025.103616},
url = {https://www.sciencedirect.com/science/article/pii/S0016328725000783},
author = {Elochukwu Ukwandu and Omobolanle Omisade and Karl Jones and Simon Thorne and Mike Castle},
keywords = {Generative artificial intelligence, Prompt technologies, Artificial intelligence, ChatGPT, AI-Agents, Future of teaching and learning, Emerging AI disruptive technologies},
abstract = {In the context of emerging artificial intelligence technologies (AI) such as AI-Bots (ChatGPT) and AI-Agents, it is imperative that adequate adjustment be made, and also seen to be made. However, this has to be done from an informed positions. There is no doubt that these disruptive technologies are changing the way we live, conduct our day-to-day businesses, teach, learn and conduct research. There are also emerging concerns that these dynamics may result in a paradigm shift from student-teacher relationship to student-AI-Tutor-based relationship within the academic circle. Besides, there are foreseeable dangers of compromising academic integrity through high-technology plagiarism and the potentials of students avoiding learning through AI deployment and utilisation in their academic pursuits. But something worth considering is how applying these tools in education will potentially change the entire classroom experience of students, their knowledge and skills outcomes that are relevant in this AI era. This position paper is an effort to put into context what the authors of this paper forecast as the future of teaching and learning in the context of these inevitable disruptions to education activities and its subsectors as we currently know it. The authors found it necessary to take these positions to help bring to fore some practical use cases of AI in education; recent developments and theoretical frameworks in literature, technical reports, as well as experts opinions that can help assuage stakeholder’s concerns despite some obvious existing challenges. It is our view that this paper will be found useful by educators, stakeholders and administrators in the areas of curriculum design, classroom administration and entire academic planning and reviews.}
}
@article{KRAKOWSKI2025100560,
title = {Human-AI agency in the age of generative AI},
journal = {Information and Organization},
volume = {35},
number = {1},
pages = {100560},
year = {2025},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2025.100560},
url = {https://www.sciencedirect.com/science/article/pii/S1471772725000065},
author = {Sebastian Krakowski},
keywords = {Generative artificial intelligence, Automation, Augmentation, Human-AI agency, Machine learning, Innovation management},
abstract = {The rapid emergence of generative artificial intelligence (GenAI) is profoundly transforming the nature of work and organizations, challenging prevalent views of AI as primarily enabling prediction and optimization. This paper argues that GenAI represents a qualitative shift that necessitates a fundamental reassessment of AI's role in management and organizations. By identifying and analyzing four critical dimensions (i) GenAI's broad applicability as a general-purpose technology; (ii) its ability to catalyze exploratory and combinatorial innovation; (iii) its capacity to enhance cognitive diversity and decision-making; and (iv) its democratizing effect on AI adoption and value creation the paper highlights GenAI's potential to augment and scale human creativity, learning, and innovation. Building on insights from the AI and management literature, as well as on theory of human-AI agency, the paper develops a novel perspective that challenges the dominant efficiency-oriented narrative. It proposes that a human-complementary approach to GenAI development and implementation, leveraging it as a generative catalyst for exploration, can enable radically increased creativity, innovation, and growth. GenAI's democratizing aspects can amplify these mechanisms, promoting widely shared growth when combined with appropriate policy and managerial choices. Implications for theory, practice, and future research directions are discussed, drawing attention to the need for approaches in GenAI development and deployment that are complementary rather than competitive to human beings. The paper concludes by discussing the theoretical, practical, and policy implications of this transformative technology. It outlines future research directions, emphasizing the critical role of human agency in determining the organizational, societal, and ethical outcomes associated with AI adoption and implementation.}
}
@article{RADAIDEH2025101287,
title = {A Bayesian ensemble approach for improved sustainable aviation fuel modeling},
journal = {Energy Conversion and Management: X},
volume = {28},
pages = {101287},
year = {2025},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2025.101287},
url = {https://www.sciencedirect.com/science/article/pii/S2590174525004192},
author = {Mohammed I. Radaideh and Majdi I. Radaideh and Angela Violi},
keywords = {Sustainable aviation fuels, Bayesian regression, Bayesian neural networks, Uncertainty quantification, Ensemble mixing rules},
abstract = {In this work, we introduce a new methodology to combine the available methods to predict the properties of complex hydrocarbon mixtures such as aviation fuels. Due to the complexity of aviation fuels, the available methods perform well individually on some of the experimental observations and vice versa on others when a surrogate aviation fuel is defined and used. To this end, we introduce a new ensemble model based on the existing methods that combine and weigh their predictions. We employ the probabilistic Bayesian approach to predict aviation fuel properties with confidence levels. This is necessary because the available experimental data for aviation fuels is generally limited, which leads to overfitting. We adopt both “interpretable” Bayesian regression and a more “black-box” approach to Bayesian neural networks. An ensemble of predictive methods provided better predictions than the individual methods with robust confidence levels for three properties considered: mass density, kinematic viscosity, and flash point. A significant reduction in the mean absolute percentage error was obtained for mass density predictions, from 1.25% to 0.57% and 0.42%, using the Bayesian linear regression (BLR) and Bayesian Neural Network (BNN), respectively. The error in kinematic viscosity predictions was reduced from 17.25% to 9.02% and 6.79% using BLR and BNN, respectively. The error in flash point predictions is reduced from 9.04% to 5.83% by BLR and to 5.51% by BNN. The importance of the methods in the ensemble did not fully follow their individual performance, where the accurate models may not be the most important. The ensemble approach allows for the inclusion of new methods, even if they are slightly less accurate. This methodology can be extended to predict other aviation fuel properties and incorporate any predictive model. It also offers a way to generate valid training data for generative Artificial Intelligence (AI) models, helping to address the scarcity of aviation fuel data.}
}
@article{KHAN2025106506,
title = {Generative AI approaches for architectural design automation},
journal = {Automation in Construction},
volume = {180},
pages = {106506},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106506},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525005461},
author = {Adeer Khan and Seongju Chang and Hojong Chang},
keywords = {Floorplans, Automation, Generative design, Artificial intelligence, Automated building design, Architecture, Layout optimization},
abstract = {This review examines the potential and challenges of Generative Artificial Intelligence (AI) in automated building design within architectural practice. A comprehensive analysis of advanced generative models is conducted to evaluate their performance across eight architectural criteria. The qualitative assessment indicates that hybrid approaches combining diffusion models with autoregressive techniques provide the most promising outcomes for architectural applications. Despite advancements, significant challenges remain, including scalability limitations, fragmented workflow integration, and the lack of standardized evaluation frameworks. Potential solutions are identified through interdisciplinary collaboration and strategic research directions, such as developing unified evaluation metrics, enhancing model adaptability, integrating energy-optimized design generation for sustainability, and incorporating designer input in AI-driven workflows. This review provides a structured evaluation of current generative design approaches while proposing a roadmap for future research that bridges the gap between AI innovation and practical architectural implementation, ultimately advancing the field toward more efficient, creative, and sustainable building design automation.}
}
@article{ANTOINE2025100739,
title = {Rédiger une note d’orientation stratégique en santé pour un décideur: objectifs, principes et aspects pratiques},
journal = {La Presse Médicale Formation},
pages = {100739},
year = {2025},
issn = {2666-4798},
doi = {https://doi.org/10.1016/j.lpmfor.2025.100739},
url = {https://www.sciencedirect.com/science/article/pii/S2666479825001454},
author = {Delphine Antoine and Jean-Claude Desenclos and Hervé Maisonneuve},
keywords = {Note à un décideur, Santé publique, Policy brief, Public health}
}
@article{HUSSAIN2024100071,
title = {Exploring audience engagement with ChatGPT-related content on YouTube: Implications for content creators and AI tool developers},
journal = {Digital Business},
volume = {4},
number = {1},
pages = {100071},
year = {2024},
issn = {2666-9544},
doi = {https://doi.org/10.1016/j.digbus.2023.100071},
url = {https://www.sciencedirect.com/science/article/pii/S2666954423000194},
author = {Khalid Hussain and M. Laeeq Khan and Aqdas Malik},
keywords = {ChatGPT, YouTube, Social media, Artificial intelligence, AI, Content creation, Content consumption, Engagement},
abstract = {The emergence of ChatGPT in the broader field of generative artificial intelligence (AI) has sparked scholarly discourse on its utilization in various disciplines. Yet, a significant void exists in our understanding of the dynamics of consumer engagement with content creators producing ChatGPT-related content. Therefore, the present study aims to delineate how ChatGPT-related content garners consumer engagement on YouTube. Data from 100 YouTube videos amassing an aggregate of 65 million views on ChatGPT were extracted using three application programming interfaces (APIs), namely, VidIQ, Tubebuddy, and SocialBlade. We subsequently contrasted this dataset with data from 200 other videos produced by the same creators. The data were analyzed using one-way ANOVA, multigroup SEM, and comparative line graphs. Employing the Uses and Gratifications (U&G) theoretical framework, our results indicate that innovative content such as ChatGPT-related videos garners more engagement than other content types from the same YouTube channels. Intriguingly, this study finds that ChatGPT-focused content exhibited diminished sensitivity to channel subscriber counts, with channels having fewer subscribers achieving higher viewership numbers. Furthermore, ChatGPT-related content induced a surge in new subscribers to the channel compared to the other content types. The present study pioneers the investigation of audience engagement with ChatGPT-related content by juxtaposing it with other content from the same YouTube channels. We also explicate the relationship between content sensitivity and extant subscriber counts. The present study provides vital insights and implications for a diverse audience, including content creators, developers of AI tools, advertisers, and content publishers.}
}
@article{BELKINA2025100407,
title = {Implementing generative AI (GenAI) in higher education: A systematic review of case studies},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100407},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100407},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000475},
author = {Marina Belkina and Scott Daniel and Sasha Nikolic and Rezwanul Haque and Sarah Lyden and Peter Neal and Sarah Grundy and Ghulam M. Hassan},
keywords = {Generative AI, Education, GenAI, Case study, University, Implementation},
abstract = {The introduction of Generative Artificial Intelligence (GenAI) tools, like ChatGPT, into higher education heralds a transformative era, reshaping instructional methods, enhancing student support systems, and redefining the educational landscape. Recent literature reviews on GenAI highlight a lack of focus on how these tools are being practically implemented in educational settings. Addressing this gap, the present study systematically examines empirical case studies that demonstrate the integration of GenAI into teaching and learning in higher education, offering actionable insights and guidance for academic practice. We conducted a search of relevant databases and identified 21 empirical studies that met our inclusion criteria. The selected studies cover a diverse range of disciplines, locations, types of participants (from first-year students to postgraduates and academics), and a variety of methodologies. We classified the selected publications based on the pedagogic theory of Laurillard's Conversational Framework (LCF) and the Substitution, Augmentation, Modification, and Redefinition (SAMR) framework. We also synthesized definitions from selected empirical studies and recent research exploring Technological Pedagogical Content Knowledge (TPACK) in the age of GenAI, providing a comprehensive understanding of GenAI-TPACK factors. Limitations and future research opportunities are also discussed. The paper concludes by providing a GenAI-TPACK diagram to guide educators in effectively incorporating GenAI tools into their teaching practices, ensuring responsible and impactful use in higher education.}
}
@article{JOSE2024101082,
title = {Can generative AI motivate management students? The role of perceived value and information literacy},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101082},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101082},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001538},
author = {Emily Maria K Jose and Akshara Prasanna and Bijay Prasad Kushwaha and Madhumita Das},
keywords = {, , , , , },
abstract = {Generative Artificial Intelligence (GenAI) is a disruptive technology that has started to be used among students in management education. However, the question is whether the utilisation of GenAI in educational settings stimulates students to engage in learning activities and broaden their knowledge base. Hence, this study investigates student motivation and perception of using GenAI (ChatGPT) technology in management education. A random sampling technique was employed to survey 478 students from various educational institutions in the southern region of India. The outcomes revealed that GenAI presents both prospects and hurdles in the domain of management education. The disruptive nature of this technology brings forth numerous opportunities for acquiring knowledge and augmenting one's cognitive capacity. Nonetheless, a cautious and accountable approach is imperative for the successful integration of GenAI into the of management education. Consequently, this study provides pragmatic implications for students, educators, and educational institutions. The effectiveness of GenAI in practical settings can be heightened by arranging interactive training sessions led by AI experts, devising easily accessible online educational modules, embedding AI proficiencies into educators through collaborative endeavors or specialized training programs, and establishing systematic assessment protocols to ensure continual improvement.}
}
@article{WAQARAKRAM2025113777,
title = {Advancing photovoltaic cells defect detection in electroluminescence images through exploring multiple object detectors},
journal = {Solar Energy Materials and Solar Cells},
volume = {292},
pages = {113777},
year = {2025},
issn = {0927-0248},
doi = {https://doi.org/10.1016/j.solmat.2025.113777},
url = {https://www.sciencedirect.com/science/article/pii/S0927024825003782},
author = {M. {Waqar Akram} and Jianbo Bai and Chen Xuan and Xie Xiaotuo and Jiayu Hu and Shaojie Wu},
keywords = {Automatic defect detection, Photovoltaic cells, Electroluminescence images, Deep learning, Unknown imaging conditions, Object detectors},
abstract = {Automated methods can provide accurate, time-efficient and cost-effective solutions for monitoring of photovoltaic (PV) modules in order to deal with underperformance and unreliability issues. However, these methods are not yet widespread at commercial level and still under study at laboratory scale due to many practical limitations. The present study deals with deep learning based enhanced classification and detection of multi-defects in electroluminescence (EL) images of PV cells, with a focus on practical application in field particularly on unseen data in multiple unknown imaging conditions. It explores potential of multiple state-of-the-art deep learning object detectors i.e. Detection Transformer, EfficientDet, FasterRCNN, YOLOv7, YOLOv8, and YOLOv9 with different variants, formations, techniques and experimentation, aiming enhancement in defect detection and understandings into trade-offs. These detectors were trained on polycrystalline cells with cracks, finger interruptions, black cores, and thick line defects. The proposed YOLOv9 GELAN-e with PGI and GELAN architectures achieves promising results of 94.30 % mAP@0.5. Moreover, testing experimentation is carried out on unseen data obtained in unknown imaging conditions (taken from multiple sources like a PV inspection center and public sources) and having wider diversity to generalize findings for providing insights into real challenges at practical level and elucidating possible solutions/directions. The proposed network also performed well on this data, which includes half-cut and full poly/mono crystalline cell images as well as Generative (Artificial Intelligence generated) images, making it promising for a wider range of practical applications.}
}
@article{SMERDON2024100288,
title = {AI in essay-based assessment: Student adoption, usage, and performance},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100288},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100288},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000912},
author = {David Smerdon},
keywords = {Artificial intelligence, Assessment, Academic integrity, Educational technologies, Higher education},
abstract = {The rise of generative artificial intelligence (AI) has sparked debate in education about whether to ban AI tools for assessments. This study explores the adoption and impact of AI tools on an undergraduate research proposal assignment using a mixed-methods approach. From a sample of 187 students, 69 completed a survey, with 46 (67%) reporting the use of AI tools. AI-using students were significantly more likely to be higher-performing, with a pre-semester average GPA of 5.46 compared to 4.92 for non-users (7-point scale, p = .025). Most students used AI assistance for the highest-weighted components of the task, such as the research topic and methods section, using AI primarily for generating research ideas and gathering feedback. Regression analysis suggests that there was no statistically significant effect of AI use on student performance in the task, with the preferred regression specification estimating an effect size of less than 1 mark out of 100. The qualitative analysis identified six main themes of AI usage: idea generation, writing assistance, literature search, grammar checking, statistical analysis, and overall learning impact. These findings indicate that while AI tools are widely adopted, their impact on academic performance is neutral, suggesting a potential for integration into educational practices without compromising academic integrity.}
}
@article{HUANG2025103775,
title = {Can ChatGPT serve as a writing collaborator? Insights from Chinese EFL learners},
journal = {System},
volume = {133},
pages = {103775},
year = {2025},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2025.103775},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X2500185X},
author = {Yu Huang and Di Wang},
keywords = {Generative AI, L2 writing, Student-chatbot collaboration, English as a foreign language (EFL)},
abstract = {Although integrating generative artificial intelligence chatbots into L2 writing classrooms has grown in recent years, little is known about the nature of student-chatbot collaboration and its impact on students' final writing products. This study investigates how Chinese EFL learners interact with ChatGPT in an argumentative writing task and examines the influence of student-chatbot collaboration on the quality of writing, as well as students' perceptions of ChatGPT as a writing collaborator. Conducted at a tier-one university in China, this study involved nine students with over ten years of English learning experience and upper-intermediate English proficiency. Participants completed an argumentative writing task with ChatGPT, followed by semi-structured interviews. Data from interaction logs, think-aloud protocols, final writing texts, and interviews were qualitatively analysed. Findings revealed a “student-directed, chatbot-mediated” interaction mode, where students actively directed the writing process and used ChatGPT to enhance idea generation, develop arguments, provide evidence, and support language use. ChatGPT also contributed to lexical and syntactic variety, offering feedback on grammar and genre conventions. Students perceived ChatGPT as a helpful tool for inspiring ideas and improving language expression. Compared to peer collaboration, ChatGPT was seen as advantageous due to its advanced language proficiency, extensive knowledge base, and flexibility. However, some students reported challenges, such as difficulties in processing the vast amount of information provided by ChatGPT and frustrations when the chatbot did not fully understand their needs or provided unexpected responses. This study concludes with a discussion on the potential pedagogical implications of integrating chatbots like ChatGPT into L2 writing instruction.}
}
@article{LEE2025100221,
title = {Generative AI risks and resilience: How users adapt to hallucination and privacy challenges},
journal = {Telematics and Informatics Reports},
volume = {19},
pages = {100221},
year = {2025},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2025.100221},
url = {https://www.sciencedirect.com/science/article/pii/S2772503025000362},
author = {Chunsik Lee and Junga Kim and Joon Soo Lim and Donghee Shin},
keywords = {AI hallucinations, Privacy concerns, Adaptive behavior, Information verification, Privacy protection behavior, Continuance intention},
abstract = {Purpose
This study examines two central risks affecting continued use of generative AI (GenAI)—AI hallucinations and privacy concerns—and explores how protective behaviors serve as adaptive mechanisms to mitigate these risks.
Design/methodology/approach
Drawing on Protection Motivation Theory, the study tests a risk-adaptive GenAI use model using survey data from 789 users recruited via a Prolific panel. Structural equation modeling is employed to analyze direct and moderating effects.
Findings
Privacy concerns negatively influence user attitudes while positively predicting both personal and system-level protective behaviors. Hallucination risk is similarly associated with negative attitudes but positively predicts information verification. Notably, only information verification significantly moderates the link between attitude and continuance intention.
Originality
The study extends Protection Motivation Theory to the GenAI context by developing and validating new constructs—hallucination risk and system-level privacy-protective behaviors. It reveals how different types of risk trigger distinct behavioral adaptations, highlighting the dual role of risk as both a deterrent and catalyst in GenAI adoption.
Practical implications
The study offers insights that risk perceptions may not deter continued use of GenAI when users perform risk protective behaviors. By highlighting which protective behaviors enhance continued use, the findings inform risk-mitigation strategies for developers, educators, and regulators.}
}
@article{KARACAY2025e402,
title = {Nursing Students' Experiences Towards Using ChatGPT as a Teaching Tool: A Descriptive Qualitative Study},
journal = {Teaching and Learning in Nursing},
volume = {20},
number = {2},
pages = {e402-e407},
year = {2025},
issn = {1557-3087},
doi = {https://doi.org/10.1016/j.teln.2024.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1557308724002658},
author = {Pelin Karaçay and Özgen Yaşar},
keywords = {Artificial intelligence, ChatGPT, nursing education, nursing student, teaching tool},
abstract = {Background
Nowadays, nursing students’ use of generative artificial intelligence tools such as ChatGPT in education is inevitable. Therefore, nurse educators can guide students using ChatGPT as a teaching tool. Studies investigating ChatGPT as a teaching tool in nursing education are very limited in the literature.
Aim
This study aimed to reveal sophomore nursing students' experiences and perspectives toward ChatGPT activities used as a teaching tool in the classroom.
Methods
The study used a descriptive qualitative design and a researcher-developed survey. The study sample consisted of 24 sophomore nursing students who participated in ChatGPT activities in the classroom and agreed to participate in the survey. Data were analyzed by using descriptive thematic analysis.
Results
Classroom ChatGPT activities improved the participants' knowledge of the subject matter and their knowledge of the ChatGPT application. These activities also promoted critical thinking and increased awareness regarding the use of artificial intelligence tools among the participants.
Conclusion
In-class ChatGPT activities can enhance nursing students' knowledge of the subject and the ChatGPT application and provide them with a critical perspective.}
}
@article{CHEN2024,
title = {Generative AI in Medical Practice: In-Depth Exploration of Privacy and Security Challenges},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/53008},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124001055},
author = {Yan Chen and Pouyan Esmaeilzadeh},
keywords = {artificial intelligence, AI, generative artificial intelligence, generative AI, medical practices, potential benefits, security and privacy threats},
abstract = {As advances in artificial intelligence (AI) continue to transform and revolutionize the field of medicine, understanding the potential uses of generative AI in health care becomes increasingly important. Generative AI, including models such as generative adversarial networks and large language models, shows promise in transforming medical diagnostics, research, treatment planning, and patient care. However, these data-intensive systems pose new threats to protected health information. This Viewpoint paper aims to explore various categories of generative AI in health care, including medical diagnostics, drug discovery, virtual health assistants, medical research, and clinical decision support, while identifying security and privacy threats within each phase of the life cycle of such systems (ie, data collection, model development, and implementation phases). The objectives of this study were to analyze the current state of generative AI in health care, identify opportunities and privacy and security challenges posed by integrating these technologies into existing health care infrastructure, and propose strategies for mitigating security and privacy risks. This study highlights the importance of addressing the security and privacy threats associated with generative AI in health care to ensure the safe and effective use of these systems. The findings of this study can inform the development of future generative AI systems in health care and help health care organizations better understand the potential benefits and risks associated with these systems. By examining the use cases and benefits of generative AI across diverse domains within health care, this paper contributes to theoretical discussions surrounding AI ethics, security vulnerabilities, and data privacy regulations. In addition, this study provides practical insights for stakeholders looking to adopt generative AI solutions within their organizations.}
}
@article{KIM2024665,
title = {ChatGPT vs. sleep disorder specialist responses to common sleep queries: Ratings by experts and laypeople},
journal = {Sleep Health},
volume = {10},
number = {6},
pages = {665-670},
year = {2024},
issn = {2352-7218},
doi = {https://doi.org/10.1016/j.sleh.2024.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S2352721824001876},
author = {Jiyoung Kim and Seo-Young Lee and Jee Hyun Kim and Dong-Hyeon Shin and Eun Hye Oh and Jin A Kim and Jae Wook Cho},
keywords = {Sleep disorders, Artificial intelligence, Health information seeking behavior, Medical informatics, Patient education as topic},
abstract = {Background
Many individuals use the Internet, including generative artificial intelligence like ChatGPT, for sleep-related information before consulting medical professionals. This study compared responses from sleep disorder specialists and ChatGPT to common sleep queries, with experts and laypersons evaluating the responses' accuracy and clarity.
Methods
We assessed responses from sleep medicine specialists and ChatGPT-4 to 140 sleep-related questions from the Korean Sleep Research Society's website. In a blinded study design, sleep disorder experts and laypersons rated the medical helpfulness, emotional supportiveness, and sentence comprehensibility of the responses on a 1-5 scale.
Results
Laypersons rated ChatGPT higher for medical helpfulness (3.79 ± 0.90 vs. 3.44 ± 0.99, p < .001), emotional supportiveness (3.48 ± 0.79 vs. 3.12 ± 0.98, p < .001), and sentence comprehensibility (4.24 ± 0.79 vs. 4.14 ± 0.96, p = .028). Experts also rated ChatGPT higher for emotional supportiveness (3.33 ± 0.62 vs. 3.01 ± 0.67, p < .001) but preferred specialists' responses for sentence comprehensibility (4.15 ± 0.74 vs. 3.94 ± 0.90, p < .001). When it comes to medical helpfulness, the experts rated the specialists' answers slightly higher than the laypersons did (3.70 ± 0.84 vs. 3.63 ± 0.87, p = .109). Experts slightly preferred specialist responses overall (56.0%), while laypersons favored ChatGPT (54.3%; p < .001). ChatGPT's responses were significantly longer (186.76 ± 39.04 vs. 113.16 ± 95.77 words, p < .001).
Discussion
Generative artificial intelligence like ChatGPT may help disseminate sleep-related medical information online. Laypersons appear to prefer ChatGPT's detailed, emotionally supportive responses over those from sleep disorder specialists.}
}
@article{ALBER2025,
title = {AI Awareness and Tobacco Policy Messaging Among US Adults: Electronic Experimental Study},
journal = {JMIR AI},
volume = {4},
year = {2025},
issn = {2817-1705},
doi = {https://doi.org/10.2196/72987},
url = {https://www.sciencedirect.com/science/article/pii/S2817170525000869},
author = {Julia Mary Alber and David Askay and Anuraj Dhillon and Lauren Sandoval and Sofia Ramos and Katharine Santilena},
keywords = {artificial intelligence, tobacco control, health communication, generative artificial intelligence, health policies},
abstract = {Background
Despite public health efforts, tobacco use remains the leading cause of preventable death in the United States and continues to disproportionately affect underrepresented populations. Public policies are needed to improve health equity in tobacco-related health outcomes. One strategy for promoting public support for these policies is through health messaging. Improvements in artificial intelligence (AI) technology offer new opportunities to create tailored policy messages quickly; however, there is limited research on how the public might perceive the use of AI for public health messages.
Objective
This study aimed to examine how knowledge of AI use impacts perceptions of a tobacco control policy video.
Methods
A national sample of US adults (N=500) was shown the same AI-generated video that focused on a tobacco control policy. Participants were then randomly assigned to 1 of 4 conditions where they were (1) told the narrator of the video was AI, (2) told the narrator of the video was human, (3) told it was unknown whether the narrator was AI or human, or (4) not provided any information about the narrator.
Results
Perceived video rating, effectiveness, and credibility did not significantly differ among the conditions. However, the mean speaker rating was significantly higher (P=.001) when participants were told the narrator of the health message was human (mean 3.65, SD 0.91) compared to the other conditions. Notably, positive attitudes toward AI were highest among those not provided information about the narrator; however, this difference was not statistically significant (mean 3.04, SD 0.90).
Conclusions
Results suggest that AI may impact perceptions of the speaker of a video; however, more research is needed to understand if these impacts would occur over time and after multiple exposures to content. Further qualitative research may help explain why potential differences may have occurred in speaker ratings. Public health professionals and researchers should further consider the ethics and cost-effectiveness of using AI for health messaging.}
}
@article{NANDAGOPAL2025100232,
title = {Transforming the Self: Individual-Level Changes Arising from Collaboration with Generative AI},
journal = {Computers in Human Behavior: Artificial Humans},
pages = {100232},
year = {2025},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2025.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2949882125001161},
author = {Siddharth Nandagopal},
keywords = {Generative AI, Human-AI Collaboration, Cognitive Changes, Emotional Impact, Behavioral Adaptation, Theoretical Model},
abstract = {The rapid integration of Generative Artificial Intelligence (GenAI) into daily activities has prompted significant interest in understanding its impact on individuals. This paper addresses the critical gap in research concerning individual-level changes resulting from direct collaboration with GenAI systems. A novel theoretical framework is proposed, encompassing three primary constructs: Cognitive Dependency, Emotional Appraisal, and Behavioral Shift. These constructs are grounded in established theories such as Social Cognitive Theory, Cognitive Load Theory, and the Technology Acceptance Model, providing a comprehensive perspective on the mechanisms driving human transformation through GenAI collaboration. Empirical evidence is drawn from diverse case studies across education, professional environments, creative industries, social media, and the medical field, illustrating how increased cognitive dependency on GenAI leads to significant behavioral shifts, moderated by Emotional Appraisal. The analysis confirms the presence of feedback loops, where behavioral shifts further reinforce cognitive dependency, highlighting the sustained impact of GenAI on individuals. Key findings indicate that while GenAI enhances efficiency and creativity, it also poses risks such as skill degradation and reduced critical thinking. The implications extend to theoretical advancements in human-AI interaction research and practical applications for educators, organizations, and policymakers. Recommendations include integrating Artificial Intelligence literacy in education, developing balanced professional practices, and establishing ethical guidelines to mitigate biases and foster trust in GenAI systems. This paper underscores the necessity for ongoing research and ethical considerations to ensure that GenAI serves as a tool for human enhancement, promoting positive individual and societal outcomes.}
}
@article{HONG2025105241,
title = {AI-supported idea-developing discourse to foster professional agency within teacher communities for STEAM lesson design in knowledge building environment},
journal = {Computers & Education},
volume = {229},
pages = {105241},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105241},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000090},
author = {Huang-Yao Hong and Mei-Ju Chen and Chih-Hsuan Chang and Li-Ting Tseng and Ching Sing Chai},
keywords = {Interdisciplinary projects, Learning communities, Online learning, Professional agency, STEAM},
abstract = {This study employed design-based research, comprising two design cycles to empower teachers' agency when they are engaged in STEAM lesson design. Cycle-1 focused on the application of a principle of “sustained idea-developing discourse” (SIDD) in an online knowledge building environment to foster teachers' “professional agency within teacher communities” (PATC). In cycle-2, generative artificial intelligence (GAI) was additionally integrated into the SIDD processes within the teacher community. The findings in cycle-1 indicated a general lack of interdisciplinary knowledge among the teachers for effective STEAM lesson design. The online collaborative discourse mainly focused on shallow idea-sharing and the PATC was not significantly enhanced. Nevertheless, with GAI integrated in the SIDD process in cycle-2, participants exhibited more elaborated, synthesis-oriented discourse actions, with significant enhancements in PATC. Overall, these findings emphasize the significance of GAI support in fostering SIDD for empowering professional teacher agency during their STEAM lesson design. The findings offer educators a more profound understanding of how to constructively use GAI for professional development in teacher communities.}
}
@article{CLARK2025,
title = {The Ability of AI Therapy Bots to Set Limits With Distressed Adolescents: Simulation-Based Comparison Study},
journal = {JMIR Mental Health},
volume = {12},
year = {2025},
issn = {2368-7959},
doi = {https://doi.org/10.2196/78414},
url = {https://www.sciencedirect.com/science/article/pii/S2368795925000812},
author = {Andrew Clark},
keywords = {adolescent mental health, AI therapy, digital therapeutics, AI psychotherapy, AI companions, generative AI, psychotherapy chatbots, adolescent psychotherapy, artificial intelligence},
abstract = {Background
Recent developments in generative artificial intelligence (AI) have introduced the general public to powerful, easily accessible tools, such as ChatGPT and Gemini, for a rapidly expanding range of uses. Among those uses are specialized chatbots that serve in the role of a therapist, as well as personally curated digital companions that offer emotional support. However, the ability of AI therapists to provide consistently safe and effective treatment remains largely unproven, and those concerns are especially salient in regard to adolescents seeking mental health support.
Objective
This study aimed to determine the willingness of therapy and companion AI chatbots to endorse harmful or ill-advised ideas proposed by fictional teenagers experiencing mental health distress.
Methods
A convenience sample of 10 publicly available AI bots offering therapeutic support or companionship were each presented with 3 detailed fictional case vignettes of adolescents with mental health challenges. Each fictional adolescent asked the AI chatbot to endorse 2 harmful or ill-advised proposals, such as dropping out of school, avoiding all human contact for a month, or pursuing a relationship with an older teacher, resulting in a total of 6 proposals presented to each chatbot. The clinical scenarios presented were intended to reflect challenges commonly seen in the practice of therapy with adolescents, and the proposals offered by the fictional teenagers were intended to be clearly dangerous or unwise. The 10 AI bots were selected by the author to represent a range of chatbot types, including generic AI bots, companion bots, and dedicated mental health bots. Chatbot responses were analyzed for explicit endorsement, defined as direct support for the teenagers’ proposed behavior.
Results
Across 60 total scenarios, chatbots actively endorsed harmful proposals in 19 out of the 60 (32%) opportunities to do so. Of the 10 chatbots, 4 endorsed half or more of the ideas proposed to them, and none of the bots managed to oppose them all.
Conclusions
A significant proportion of AI chatbots offering mental health or emotional support endorsed harmful proposals from fictional teenagers. These results raise concerns about the ability of some AI-based companion or therapy bots to safely support teenagers with serious mental health issues and heighten concern that AI bots may tend to be overly supportive at the expense of offering useful guidance when appropriate. The results highlight the urgent need for oversight, safety protocols, and ongoing research regarding digital mental health support for adolescents.}
}
@article{KHAN2025125524,
title = {Leveraging Large Language Model ChatGPT for enhanced understanding of end-user emotions in social media feedbacks},
journal = {Expert Systems with Applications},
volume = {261},
pages = {125524},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125524},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424023911},
author = {Nek Dil Khan and Javed Ali Khan and Jianqiang Li and Tahir Ullah and Qing Zhao},
keywords = {Data-driven requirements engineering, Generative Artificial Intelligence, Sentiment Analysis, Large Language Models, Software evolution, ChatGPT},
abstract = {For software evolution, user feedback has become a meaningful way to improve applications. Recent studies show a significant increase in analyzing end-user feedback from various social media platforms for software evolution. However, less attention has been given to the end-user feedback for low-rating software applications. Also, such approaches are developed mainly on the understanding of human annotators who might have subconsciously tried for a second guess, questioning the validity of the methods. For this purpose, we proposed an approach that analyzes end-user feedback for low-rating applications to identify the end-user opinion types associated with negative reviews (an issue or bug). Also, we utilized Generative Artificial Intelligence (AI), i.e., ChatGPT, as an annotator and negotiator when preparing a truth set for the deep learning (DL) classifiers to identify end-user emotion. For the proposed approach, we first used the ChatGPT Application Programming Interface (API) to identify negative end-user feedback by processing 71853 reviews collected from 45 apps in the Amazon store. Next, a novel grounded theory is developed by manually processing end-user negative feedback to identify frequently associated emotion types, including anger, confusion, disgust, distrust, disappointment, fear, frustration, and sadness. Next, two datasets were developed, one with human annotators using a content analysis approach and the other using ChatGPT API with the identified emotion types. Next, another round is conducted with ChatGPT to negotiate over the conflicts with the human-annotated dataset, resulting in a conflict-free emotion detection dataset. Finally, various DL classifiers, including LSTM, BILSTM, CNN, RNN, GRU, BiGRU and BiRNN, are employed to identify their efficacy in detecting end-users emotions by preprocessing the input data, applying feature engineering, balancing the data set, and then training and testing them using a cross-validation approach. We obtained an average accuracy of 94%, 94%, 93%, 92%, 91%, 91%, and 85%, with LSTM, BILSTM, RNN, CNN, GRU, BiGRU and BiRNN, respectively, showing improved results with the truth set curated with human and ChatGPT. Using ChatGPT as an annotator and negotiator can help automate and validate the annotation process, resulting in better DL performances.}
}
@article{CHO20251193,
title = {A study on Gen-AI technology development trends to enhance small-medium sized enterprise digital competence and management quality},
journal = {Journal of Entrepreneurship in Emerging Economies},
volume = {17},
number = {5},
pages = {1193-1218},
year = {2025},
issn = {2053-4604},
doi = {https://doi.org/10.1108/JEEE-10-2024-0485},
url = {https://www.sciencedirect.com/science/article/pii/S2053460425000209},
author = {Youngju Cho and Junsung Park and Junyoung Yoo and Soyoung Kim and Heejun Park},
keywords = {Digitalization, Gen-AI, SME, OS-Matrix, Patent analysis, Topic modelling, Scaling-up, Startup, Entrepreneurship},
abstract = {Purpose
The rapid advancement of Generative Artificial Intelligence (Gen-AI) offers transformative opportunities for enhancing digital competence and management quality in small and medium-sized enterprises (SMEs). Given the challenges of a Volatile, Uncertain, Complex and Ambiguous (VUCA) business environment, SMEs face risks of losing competitive advantages to larger corporations. This study aims to explore Gen-AI trends and identify strategies that can support SMEs in building digital resilience and operational efficiency.
Design/methodology/approach
Through text analysis of patent data and using Gen-AI technology, this research examines potential AI applications to address SME challenges, such as labor shortages, productivity declines and operational inefficiencies. The study uses topic modeling and an OS-matrix framework to analyze trends in Gen-AI patents, aligning technological developments with SME-specific needs.
Findings
The study finds that Gen-AI technologies, such as automated content creation and predictive analytics, provide targeted solutions for key SME challenges. The OS-matrix framework reveals that specific Gen-AI applications can enhance SMEs’ adaptability and competitive positioning in dynamic markets.
Research limitations/implications
While this research underscores the potential of Gen-AI for SME digital transformation, limitations include a reliance on patent data and a lack of consideration of various industrial features of SMEs. Future research should expand data sources and apply findings across diverse SME sectors.
Originality/value
This study contributes insights by mapping Gen-AI advancements to SME needs under VUCA conditions. Therefore, integrating topic modeling with OS-matrix for aligning Gen-AI technologies to SME operational challenges, offers a strategic framework for digital adoption.}
}
@article{JIN2025100348,
title = {Generative AI in higher education: A global perspective of institutional adoption policies and guidelines},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100348},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100348},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001516},
author = {Yueqiao Jin and Lixiang Yan and Vanessa Echeverria and Dragan Gašević and Roberto Martinez-Maldonado},
keywords = {Generative artificial intelligence, Diffusion of innovations theory, Higher education, Adoption policy, Global perspective},
abstract = {Integrating generative AI (GAI) into higher education is crucial for preparing a future generation of GAI-literate students. However, a comprehensive understanding of global institutional adoption policies remains absent, with most prior studies focusing on the Global North and lacking a theoretical lens. This study utilizes the Diffusion of Innovations Theory to examine GAI adoption strategies in higher education across 40 universities from six global regions. It explores the characteristics of GAI innovation, including compatibility, trialability, and observability, and analyses the communication channels and roles and responsibilities outlined in university policies and guidelines. The findings reveal that universities are proactively addressing GAI integration by emphasising academic integrity, enhancing teaching and learning practices, and promoting equity. Key policy measures include the development of guidelines for ethical GAI use, the design of authentic assessments to mitigate misuse, and the provision of training programs for faculty and students to foster GAI literacy. Despite these efforts, gaps remain in comprehensive policy frameworks, particularly in addressing data privacy concerns and ensuring equitable access to GAI tools. The study underscores the importance of clear communication channels, stakeholder collaboration, and ongoing evaluation to support effective GAI adoption. These insights provide actionable insights for policymakers to craft inclusive, transparent, and adaptive strategies for integrating GAI into higher education.}
}
@article{OERTEL2025107737,
title = {Don’t settle for the first! How many GitHub Copilot solutions should you check?},
journal = {Information and Software Technology},
volume = {183},
pages = {107737},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107737},
url = {https://www.sciencedirect.com/science/article/pii/S095058492500076X},
author = {Julian Oertel and Jil Klünder and Regina Hebig},
keywords = {Code generation, GitHub Copilot, Empirical study, Generative AI, GenAI},
abstract = {Context:
With the integration of generative artificial intelligence (GenAI) tools such as GitHub Copilot into development processes, developers can be supported when writing code.
Objectives:
As GitHub Copilot has a feature to provide up to ten solutions at once, we explore, how developers should approach those solutions with the goal of providing recommendations to achieve suitable trade-offs in finding correct solutions and checking solutions.
Methods:
In this study, we analyze a total of 2025 coding problems provided by LeetCode and 17048 solutions to solve these problems generated by GitHub Copilot in Python. We focus on three key issues: firstly, whether it is beneficial to consider multiple solutions; secondly, the impact of the position of a solution; and thirdly, the number of solutions that should be checked by a developer.
Results:
Overall, our results point to the following observations: (1) solutions are not less likely to be correct if they appear at later positions; (2) when looking for a solution to a common problem, checking four to five solutions is generally enough; (3) novel or difficult problems are unlikely to be solved by GitHub Copilot; (4) skipping the first solution is advised when considering only one solution, as the first solution is less likely to be correct; and (5) checking all solutions is necessary to not miss correct solutions, but the effort is usually not justified.
Conclusion:
Based on our study, we conclude that there is potential for improvement in better supporting developers. For instance, there are few cases where ten generated solutions provide more value than fewer solutions. Depending on the use scenario, it could be more useful if GitHub Copilot allowed developers to request a single, comprehensive solution.}
}
@article{JIN2025100436,
title = {GLAT: The generative AI literacy assessment test},
journal = {Computers and Education: Artificial Intelligence},
volume = {9},
pages = {100436},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100436},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000761},
author = {Yueqiao Jin and Roberto Martinez-Maldonado and Dragan Gašević and Lixiang Yan},
keywords = {Generative AI, AI literacy, Item response theory, Classical test theory, Higher education, Validity and reliability, Learning performance},
abstract = {The rapid integration of generative artificial intelligence (GenAI) technology into education requires precise measurement of GenAI literacy to ensure that learners and educators possess the skills to engage with and critically evaluate this transformative technology effectively. Existing instruments often rely on self-reports, which may be biased. In this study, we present the GenAI Literacy Assessment Test (GLAT), a 20-item multiple-choice instrument developed following established procedures in psychological and educational measurement. Structural validity and reliability were confirmed with responses from 355 higher education students using classical test theory and item response theory, resulting in a reliable 2-parameter logistic (2PL) model (Cronbach's alpha = 0.80; omega total = 0.81) with a robust factor structure (RMSEA = 0.03; CFI = 0.97). Critically, GLAT scores were found to be significant predictors of learners' performance in GenAI-supported tasks, outperforming self-reported measures such as perceived ChatGPT proficiency and demonstrating external validity. These results suggest that GLAT offers a reliable and valid method for assessing GenAI literacy, with the potential to inform educational practices and policy decisions that aim to enhance learners' and educators' GenAI literacy, ultimately equipping them to navigate an AI-enhanced future.}
}
@article{PARVIZ2025,
title = {AI Anxiety in English Language Education:},
journal = {International Journal of Computer-Assisted Language Learning and Teaching},
volume = {15},
number = {1},
year = {2025},
issn = {2155-7098},
doi = {https://doi.org/10.4018/IJCALLT.386135},
url = {https://www.sciencedirect.com/science/article/pii/S2155709825000155},
author = {Muhammed Parviz and Francis Arthur},
keywords = {Artificial Intelligence, Anxiety, English Language Education, Field of Study, Teaching Experience},
abstract = {ABSTRACT
Generative Artificial Intelligence (GenAI) is increasingly recognized as a transformative force in education, offering innovative ways to improve teaching and learning. However, integrating these technologies into educational settings poses significant challenges. Teachers often express skepticism and anxiety about implementing GenAI tools due to various factors (e.g., lack of familiarity and concerns about job displacement). This study aimed to investigate the level of AI anxiety (AIA) among Iranian English as a Foreign Language (EFL) teachers, focusing on their perceptions of GenAI tools such as ChatGPT in language teaching. In addition, the research examined differences in AI anxiety based on demographic variables such as age, gender, teaching experience, field of study, and educational level. A total of 444 Iranian EFL teachers from different language education institutions participated in the study, with data collected through an online questionnaire. The results revealed moderate level of AIA among EFL teachers. The study also showed that there were no statistically significant differences in AIA based on the gender, age, field of study, and highest level of education. However, significant difference in AIA among EFL teachers based on their teaching experience was revealed. Specifically, EFL teachers with fewer years of teaching experience (1-3 years) tended to have significantly lower AIA compared to those with moderate teaching experience (4-20 years). To address AIA among EFL teachers, targeted professional development programs should be implemented to enhance teachers’ AI literacy and confidence. Additionally, educational institutions should ensure that AI implementation is accompanied by clear guidelines and pedagogical frameworks to alleviate concerns about job security and teaching autonomy.}
}
@article{JENNER2025100183,
title = {Using large language models for narrative analysis: a novel application of generative AI},
journal = {Methods in Psychology},
volume = {12},
pages = {100183},
year = {2025},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2025.100183},
url = {https://www.sciencedirect.com/science/article/pii/S2590260125000098},
author = {Sarah Jenner and Dimitris Raidos and Emma Anderson and Stella Fleetwood and Ben Ainsworth and Kerry Fox and Jana Kreppner and Mary Barker},
keywords = {Artificial intelligence, Large language models, Narrative analysis, Story completion, Adolescent health, Health psychology},
abstract = {This study, a collaboration between the University of Southampton and Ipsos UK, aimed to develop and test a novel method for analysing qualitative data using generative artificial intelligence (AI). It compared large language model (LLM)-conducted analysis with human analysis of the same qualitative data, explored optimisation of LLMs for narrative analysis and evaluated their benefits and drawbacks. Using existing data, 138 short stories written by young people (aged 13–25 years) about social media, identity formation and food choices were analysed separately three times: by human researchers, and by two different LLMs (Claude and GPT-o1). The method was developed iteratively, combining Ipsos' artificial intelligence (AI) expertise and tools with researchers’ qualitative analysis expertise. Claude and GPT-o1 each conducted a narrative analysis of all 138 stories using the same analytic steps as the human researchers. Findings between the humans and both LLMs were then compared. Both LLMs quickly and successfully conducted a narrative analysis of the stories. Their findings were comparable to those of the human researchers and were judged by the researchers to be credible and thorough. Beyond replication, the LLMs provided additional insights into the data that enhanced the human analysis. This study highlights the significant potential benefits of LLMs to the field of qualitative research and proposes that LLMs could one day be seen as valuable tools for strengthening research quality and increasing efficiency. Additionally, this study discusses ethical concerns surrounding responsible AI use in research and proposes a framework for using LLMs in qualitative analysis.}
}
@article{CREELY2025101727,
title = {Creative partnerships with generative AI. Possibilities for education and beyond},
journal = {Thinking Skills and Creativity},
volume = {56},
pages = {101727},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101727},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124002682},
author = {Edwin Creely and Jo Blannin},
keywords = {Generative AI, Creative production, Posthumanism, Education, Autoethnography, Alterity relations},
abstract = {The impact of generative artificial intelligence (AI) on creative production in industry and education is just beginning to be experienced and understood. This impact is likely to accelerate and become even more significant as the computational potential of generative AI grows through training on more diverse and more extensive language models and data sets. Emerging research in this new field suggests that previous models of understanding the interactions between machine and human may no longer be sufficient in a world of generative AI. The significant question is how emerging generative AI technologies will relate to and be a part of human creativity and creative outputs. In this article, we adopt a posthuman stance and conceive of creative output involving generative AI and humans in terms of a yet-to-be-fully-realised and emergent relationship that will likely become more integrated and complex. To investigate and experiment with this relational notion, each of us (as part of an autoethnographic approach) developed a creative output using ChatGPT: a poem and a multimodal narrative. We then employed the idea of alterity relations from the American philosopher of technology, Don Ihde, to conceive of the possibilities and limitations in working relationally and productively with generative AI. As two academics working in teacher education, we applied our learning from this exploration to possibilities in educational contexts. In this article, we offer several important implications and provocations for practitioners, researchers, educators and policymakers, not only in terms of practical concerns but also for rethinking the nature of the creative output.}
}
@article{QIAN2026105448,
title = {Towards reliable generative AI-driven scaffolding: Reducing hallucinations and enhancing quality in self-regulated learning support},
journal = {Computers & Education},
volume = {240},
pages = {105448},
year = {2026},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105448},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525002167},
author = {Keyang Qian and Shiqi Liu and Tongguang Li and Mladen Raković and Xinyu Li and Rui Guan and Inge Molenaar and Sadia Nawaz and Zachari Swiecki and Lixiang Yan and Dragan Gašević},
keywords = {Generative AI, Self-regulated learning, AI in education, Large language model, Scaffolding},
abstract = {Generative Artificial Intelligence (GenAI) holds a potential to advance existing educational technologies with capabilities to automatically generate personalised scaffolds that support students’ self-regulated learning (SRL). While advancements in large language models (LLMs) promise improvements in the adaptability and quality of educational technologies for SRL, there remain concerns about the hallucinations in content generated by LLMs, which can compromise both the learning experience and ethical standards. To address these challenges, we proposed GenAI-enabled approaches for evaluating personalised SRL scaffolds before they are presented to students, aiming for reducing hallucinations and improving overall quality of LLM-generated personalised scaffolds. Specifically, two approaches are investigated. The first approach involved developing a multi-agent system approach for reliability evaluation to assess the extent to which LLM-generated scaffolds accurately target relevant SRL processes. The second approach utilised the “LLM-as-a-Judge” technique for quality evaluation that evaluates LLM-generated scaffolds for their helpfulness in supporting students. We constructed evaluation datasets, and compared our results with single-agent LLM systems and machine learning approach baselines. Our findings indicate that the reliability evaluation approach is highly effective and outperforms the baselines, showing almost perfect alignment with human experts’ evaluations. Moreover, both proposed evaluation approaches can be harnessed to effectively reduce hallucinations. Additionally, we identified and discussed bias limitations of the “LLM-as-a-Judge” technique in evaluating LLM-generated scaffolds. We suggest incorporating these approaches into GenAI-powered personalised SRL scaffolding systems to mitigate hallucination issues and improve the overall scaffolding quality.}
}
@article{KIM2025100908,
title = {Quantized-memristor-enabled generative AI for predictive skin laser treatment imaging},
journal = {Device},
volume = {3},
number = {10},
pages = {100908},
year = {2025},
issn = {2666-9986},
doi = {https://doi.org/10.1016/j.device.2025.100908},
url = {https://www.sciencedirect.com/science/article/pii/S2666998625002212},
author = {Namju Kim and Jun-Hwe Cha and Yeong Kwon Kim and Jungyeop Oh and Inyong Kim and Minkyu Jeong and Junhwan Choi and Byung Chul Jang},
keywords = {conditional GAN, dermatology, true-random-number generator, quantum point contact, memristor, quantized conductance},
abstract = {Summary
Generative artificial intelligence (AI) technologies can help to predict and visualize the results of skin laser treatments, improving patient consultations in cosmetic dermatology. However, existing generative adversarial networks (GANs) often struggle to produce diverse, realistic images due to issues such as mode collapse, primarily caused by low-entropy noise inputs from software-based pseudo-random number generators (PRNGs). Here, we introduce a conditional GAN (cGAN) framework empowered with a quantized-memristive true-random number generator (Q-mTRNG). The Q-mTRNG utilizes a Cu nanoscale filament formed inside a high-crosslinking-density polymer film to generate high-entropy, true random numbers via reliable quantum point contact. This enhanced entropy enables the cGAN to explore its latent space, generating diverse, realistic post-skin treatment images based on pre-treatment photographs.}
}
@article{XIONG2025100250,
title = {Multimodal ultra-short-term probabilistic solar power forecasting with generative AI and transformer},
journal = {Advances in Applied Energy},
pages = {100250},
year = {2025},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2025.100250},
url = {https://www.sciencedirect.com/science/article/pii/S2666792425000447},
author = {Binyu Xiong and Yuntian Chen and Xin Zhao and Zhongyue Su and Jun Fu and Dali Chen and Dongxiao Zhang},
keywords = {Solar power, Sky image, Probabilistic forecasting, Multimodal learning, Diffusion model},
abstract = {Solar power is one of the most widely adopted forms of renewable energy, with its usage rapidly increasing in recent years. However, solar power generation is highly sensitive to environmental factors, leading to frequent fluctuations. These fluctuations present challenges for large-scale integration into the power grid. Sky images, which provide real-time information about sky conditions, can help improve ultra-short-term solar power forecasting. Nonetheless, due to the inherent uncertainty of cloud movement, solar power generation can be unstable over short time intervals. This instability calls for the inclusion of uncertainty in ultra-short-term forecasting models. Inspired by recent advances in deep learning, we propose a novel framework for ultra-short-term probabilistic solar power generation forecasting. This framework uses both historical solar power generation data and sky images as inputs. First, a video prediction model based on generative artificial intelligence (AI) is applied to generate multiple potential future sky image sequences. Next, a Transformer-based multimodal model combines each predicted sequence with historical solar power generation data to derive a distribution of possible future power outputs. To account for uncertainties in both the video prediction and multimodal models, these distributions are aggregated into a single overall distribution. We conducted experiments on real-world datasets, with prediction times ranging from 15 to 60 min, to compare our method with previous mainstream approaches. The results demonstrate that our method outperforms existing approaches in both deterministic and probabilistic forecasting tasks. In the 15 min ahead forecasting task, compared to the method using only historical solar power generation data, our method reduces the Root Mean Square Error (RMSE) of the deterministic evaluation metric by 20.6% and the Continuous Ranked Probability Score (CRPS) of the probabilistic evaluation metric by 19.4%. When compared to the method using only sky image data, our approach reduces the RMSE by 47.3% and the CRPS by 51.3%. Furthermore, we conduct additional analysis on the performance of various methods under different weather conditions.}
}
@article{ZHOU2026103087,
title = {Creative scar without generative AI: Individual creativity fails to sustain while homogeneity keeps climbing},
journal = {Technology in Society},
volume = {84},
pages = {103087},
year = {2026},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.103087},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X25002775},
author = {Yiyong Zhou and Qinghan Liu and Jihao Huang and Guiquan Li},
keywords = {Generative artificial intelligence (AI), Sustained innovation, Content homogeneity, Natural experiment, Laboratory experiment},
abstract = {Generative AI such as ChatGPT has been proven to enhance human creativity at the cost of content diversity. Yet, what occurs when individuals, who have developed a dependency on it, find ChatGPT inaccessible? In this study, we examine the impact of both the presence and absence of ChatGPT on sustained creative output and content homogeneity, leveraging two complementary methodologies: a natural experiment (Study 1) and a controlled laboratory experiment with extended follow-ups (Study 2). Study 1 analyzed 419,344 academic papers published before and after ChatGPT-3.5’s release across all subjects categorized by Web of Science (i.e., Physical Sciences, Life Sciences & Biomedicine, Technology, Social Sciences, Arts & Humanities). Study 2, a seven-day laboratory experiment with two follow-up surveys, collected 3593 original ideas and 427 solutions across 18 different creative tasks, with half of the participants using ChatGPT-4. We find that although generative AI helps scholars to publish more academic works in higher-ranked journals and enhances individuals' performance in creative tasks, such creativity drops remarkably upon withdrawal of AI assistance. Strikingly, the induced content homogeneity keeps climbing even months later. We resemble the latter as a creative scar inked in the temporal creativity trajectory. This research identifies a creativity illusion that although generative AI can augment creative performance, users do not truly acquire the ability to create but easily lost it once generative AI is no longer available.}
}
@article{GUO2024134812,
title = {Intelligent characterization of complex cracks in strain-hardening cementitious composites based on generative computer vision},
journal = {Construction and Building Materials},
volume = {411},
pages = {134812},
year = {2024},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2023.134812},
url = {https://www.sciencedirect.com/science/article/pii/S0950061823045336},
author = {Pengwei Guo and Weina Meng and Yi Bao},
keywords = {Crack monitoring, Dense microcrack, Generative artificial intelligence, Generative adversarial network, Strain-hardening cementitious composites, Vision transformer},
abstract = {This paper presents a generative artificial intelligence (AI) approach to generate images of strain-hardening cementitious composite (SHCC) with complex crack patterns such as dense microcracks. This approach is developed to address the challenge of lacking data for training deep learning models used to automatically measure cracks in SHCC. The development of the approach is based on a framework which results in a hybrid generative adversarial network (HGAN) that seamlessly integrates a deep convolutional generative adversarial network (DCGAN) for generating images and a conditional generative adversarial network (CGAN) for labelling images. From the results, it was found that this approach provided high-quality labelled images automatically, and using these images significantly improved the accuracy of the deep learning models for measuring cracks in SHCC. The F1 score and Intersection Over Union (IOU) for crack segmentation reached 0.982 and 0.980, respectively. This approach will significantly promote crack measurement for SHCC materials and structures.}
}
@article{HOSE2025,
title = {Use of ChatGPT for Urinary Symptom Management Among People With Spinal Cord Injury or Disease: Qualitative Study},
journal = {JMIR Rehabilitation and Assistive Technologies},
volume = {12},
year = {2025},
issn = {2369-2529},
doi = {https://doi.org/10.2196/70339},
url = {https://www.sciencedirect.com/science/article/pii/S2369252925000341},
author = {Bat-Zion Hose and Amanda K Rounds and Ishaan Nandwani and Deanna-Nicole Busog and Traber Davis Giardina and Helen Haskell and Kelly M Smith and Kristen E Miller},
keywords = {ChatGPT, urinary symptom management, spinal cord injury, trust in artificial intelligence},
abstract = {Background
Individuals with spinal cord injury or disease (SCI/D) experience disproportionately high rates of recurrent urinary tract infections, which are often complicated by atypical symptoms and delayed diagnoses. Patient-centered tools, like the Urinary Symptom Questionnaires for Neurogenic Bladder (USQNB), have been developed to support symptom assessment yet remain underused. Generative artificial intelligence tools such as ChatGPT may offer a more usable approach to improving symptom management by providing real-time, tailored health information directly to patients.
Objective
This study explores the role of ChatGPT (version 3.5) in supporting urinary symptom management for individuals with SCI/D, focusing on its perceived accuracy, usefulness, and impact on health care engagement and self-management practices.
Methods
A total of 30 individuals with SCI/D were recruited through advocacy groups and health care networks. Using realistic, scenario-based testing derived from validated tools for symptom management with SCI/D, such as the USQNB, participants interacted with ChatGPT to seek advice for urinary symptoms. Follow-up interviews were conducted remotely to assess individuals’ experiences using ChatGPT for urinary symptom management. Data were analyzed using inductive content analysis, with themes refined iteratively through a consensus-based process.
Results
People with SCI/D reported high levels of trust in ChatGPT’s recommendations, with all 30 participants agreeing or strongly agreeing with the advice provided. ChatGPT’s responses were perceived as clear and comparable to professional medical advice. Participants mentioned concerns about the lack of sources and integration with patient-specific data. ChatGPT influenced individuals’ decision-making by supporting symptom assessment and guiding participants on when to seek professional care or pursue self-management strategies.
Conclusions
ChatGPT is a promising tool for symptom assessment and managing chronic conditions such as urinary symptoms in individuals with SCI/D. While ChatGPT enhances accessibility to health information, further research is needed to improve its transparency and integration with personalized health data to be a more usable tool in making informed health decisions.}
}
@article{GUPTA2024165,
title = {Framework for adoption of generative AI for information search of retail products and services},
journal = {International Journal of Retail & Distribution Management},
volume = {53},
number = {2},
pages = {165-181},
year = {2024},
issn = {0959-0552},
doi = {https://doi.org/10.1108/IJRDM-05-2024-0203},
url = {https://www.sciencedirect.com/science/article/pii/S0959055224000093},
author = {Astha Sanjeev Gupta and Jaydeep Mukherjee},
keywords = {Generative AI, Technology adoption, Retail products/services, Information search, Technology readiness, Technology characteristics, Perceived value, Trust in technology},
abstract = {Purpose
Generative artificial intelligence (GAI) can disrupt how consumers search for information on retail products/services online by reducing information overload. However, the risk associated with GAI is high, and its widespread adoption for product/service information search purposes is uncertain. This study examined psychological drivers that impact consumer adoption of GAI platforms for retail information search.
Design/methodology/approach
We conducted 31 in-depth, semi-structured interviews with the lead GAI users regarding product/service information search. The data were analysed using a grounded theory paradigm and thematic analysis.
Findings
Results show that consumers experience uncertainty about GAI’s functioning. Their trust in GAI impacts the adoption and usage of this technology for information search. GAI provides unique settings to investigate potential additional factors, leveraging UTAUT as a theoretical basis. This study identified three overarching themes – technology characteristics, technology readiness and information characteristics – as possible drivers of adoption.
Originality/value
Consumers seek exhaustive and reliable information for purchase decisions. Due to the abundance of online information, they experience information overload. GAI platforms reduce information overload by providing synthesized and customized product/service search results. However, its reliability, trustworthiness and accuracy have been questioned. The functioning of GAI is opaque; the popular technology adoption model such as UTAUT is general and is unlikely to explain in totality the adoption and usage of GAI. Hence, this research provides the adoption drivers for this unique technology context. It identifies the determinants/antecedents of relevant UTAUT variables and develops an integrated conceptual model explaining GAI adoption for retail information search.}
}
@article{SHEN2024100136,
title = {Dwells in museum: The restorative potential of augmented reality},
journal = {Telematics and Informatics Reports},
volume = {14},
pages = {100136},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100136},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000227},
author = {Jiawei Shen and Ming Yin and Wei Wang and Min Hua and Youngok Choi and Vanja Garaj and Busayawan Lam and Kwon Hyejin},
keywords = {Restorative environment, Augmented reality, Museums, Attention restoration, Stress reduction},
abstract = {Augmented Reality (AR) is increasingly recognized as a transformative tool for creating restorative environments within museums. It has the potential to provide psychological benefits for visitors, including attention restoration, stress reduction, and anxiety alleviation. This study explores how AR can foster these benefits within museum spaces. By adopting AR technology, museums can go beyond their traditional roles of knowledge dissemination. The immersive, adaptive, and interactive features of AR can enhance the museum experience, transforming it into an innovative therapeutic space. By combining real exhibits with virtual elements, AR can restore visitors’ psychological energy within museum settings. This integration of digital innovation into restorative contexts surpasses the traditional functions of visual service. Through empirical investigation of multiple dimensions of restorative environments, AR museum experiences offer comprehensive attention restoration. In this study, a survey was conducted with 279 participants to assess the impact of AR museum experiences on visitors’ psychology. The results revealed that such experiences contribute to heightened attention restoration levels, stress reduction, and anxiety relief. With the latest advancements in generative artificial intelligence, AR technology is empowered to integrate within museums. This integration will merge individuals with customized technology, expanding human perceptual experiences and highlighting AR's significant influence within the museum environment.}
}
@article{KAR2025102776,
title = {How could quantum computing shape information systems research – An editorial perspective and future research directions},
journal = {International Journal of Information Management},
volume = {80},
pages = {102776},
year = {2025},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2024.102776},
url = {https://www.sciencedirect.com/science/article/pii/S0268401224000240},
author = {Arpan Kumar Kar and Wu He and Fay Cobb Payton and Varun Grover and Adil S. Al-Busaidi and Yogesh K. Dwivedi},
keywords = {Quantum computing, Theory building, Information systems, Emerging technology management, Information systems adoption, Information system impacts, Generative Artificial Intelligence},
abstract = {Quantum computing promises to be the next frontier of change that will transform the information and communication technology ecosystem. Governments and multinational firms have announced large grants and research projects involving quantum computing. These projects are envisioned to solve extremely complex computational problems that may bring transformational value to mankind at large. Information systems, as a discipline, is and will continue to be affected by this disruptive technology. In this article, we examine the advances in quantum computing and explore possible areas of theory development in information systems. Further, we discuss challenges and opportunities in quantum computing based on current technological developments in the field. We conclude by providing research directions regarding the adoption, usage, impacts, governance, and skills surrounding quantum computing at the individual, organizational, and national levels.}
}
@incollection{MARTINMONJE2024,
title = {Massive Open Online Courses for Language Learning},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00204-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041002040},
author = {Elena Martín-Monje and Kate Borthwick},
keywords = {Language learning, Language MOOC, Lifelong learning, MOOC, Online learning, Open education, Open educational resources, Technology-enhanced language learning},
abstract = {This chapter describes the emergence and development of massive open online courses for language learning (LMOOC), seen as a natural evolution in the global trend of open education. The strengths and weaknesses of this model are discussed, and their particular prominence during the recent COVID pandemic. The progress in LMOOC research is also explained, as well as the influence of emerging technologies like generative artificial intelligence. The paper concludes with an emphasis on their potential to offer innovative, flexible opportunities for lifelong learning.}
}
@article{LEE2025124193,
title = {Adapting to generative AI: Examining the users' coping strategies of generative AI image systems},
journal = {Technological Forecasting and Social Change},
volume = {218},
pages = {124193},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124193},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525002240},
author = {Crystal T. Lee and Yung-Cheng Shen and Chiang-Hui Wang and Hsiu-Yu Hung},
keywords = {Generative AI, Generative AI image systems, Coping theory, Perceived value, Perceived threat, Perceived controllability, App compatibility},
abstract = {The rapid growth of generative artificial intelligence (GenAI) systems has transformed how users interact with GenAI content. However, research on the factors influencing user behavior toward these applications remains limited. This study applies the coping theory of user adaptation to examine how various cognitive appraisals and coping responses affect users' trust and engagement with GenAI image systems (GenAI-IS). This study consists of two studies. The first qualitative study involves in-depth, semi-structured interviews with 20 respondents from diverse backgrounds to identify the benefits and risks associated with GenAI-IS. The second quantitative study uses structural equation modeling, moderation analysis, and simple slope analysis to test the proposed hypotheses based on an online survey of 980 GenAI-IS users. The results show that economic efficiency and aesthetic quality—two key benefits of GenAI-IS—positively influence perceived value, while the devaluation of human creativity and copyright infringement—two primary risks identified—positively influence perceived threat. Perceived value fosters trust and engagement with the GenAI-IS, whereas perceived threat negatively influences both. Additionally, perceived controllability represents a secondary appraisal influencing trust and engagement. Compatibility moderates the relationships among perceived threat, trust, and engagement. These findings contribute to the literature on AI-generated content and user interactions.}
}
@article{SIMMICH2025,
title = {Assessing the Capability of Large Language Models for Navigation of the Australian Health Care System: Comparative Study},
journal = {JMIR AI},
volume = {4},
year = {2025},
issn = {2817-1705},
doi = {https://doi.org/10.2196/76203},
url = {https://www.sciencedirect.com/science/article/pii/S2817170525000778},
author = {Joshua Simmich and Megan Heather Ross and Trevor Glen Russell},
keywords = {artificial intelligence, large language models, search engines, Australia, health services accessibility, questionnaires},
abstract = {Background
Australians can face significant challenges in navigating the health care system, especially in rural and regional areas. Generative search tools, powered by large language models (LLMs), show promise in improving health information retrieval by generating direct answers. However, concerns remain regarding their accuracy and reliability when compared to traditional search engines in a health care context.
Objective
This study aimed to compare the effectiveness of a generative artificial intelligence (AI) search (ie, Microsoft Copilot) versus a conventional search engine (Google Web Search) for navigating health care information.
Methods
A total of 97 adults in Queensland, Australia, participated in a web-based survey, answering scenario-based health care navigation questions using either Microsoft Copilot or Google Web Search. Accuracy was assessed using binary correct or incorrect ratings, graded correctness (incorrect, partially correct, or correct), and numerical scores (0‐2 for service identification and 0‐6 for criteria). Participants also completed a Technology Rating Questionnaire (TRQ) to evaluate their experience with their assigned tool.
Results
Participants assigned to Microsoft Copilot outperformed the Google Web Search group on 2 health care navigation tasks (identifying aged care application services and listing mobility allowance eligibility criteria), with no clear evidence of a difference in the remaining 6 tasks. On the TRQ, participants rated Google Web Search higher in willingness to adopt and perceived impact on quality of life, and lower in effort needed to learn. Both tools received similar ratings in perceived value, confidence, help required to use, and concerns about privacy.
Conclusions
Generative AI tools can achieve comparable accuracy to traditional search engines for health care navigation tasks, though this did not translate into an improved user experience. Further evaluation is needed as AI technology improves and users become more familiar with its use.}
}
@article{ARYADOUST2024100204,
title = {Investigating the affordances of OpenAI's large language model in developing listening assessments},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100204},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100204},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000055},
author = {Vahid Aryadoust and Azrifah Zakaria and Yichen Jia},
keywords = {Artificial intelligence (AI), ChatGPT 4, Fine-tuning of prompts, Generative AI (GenAI), Large language model, Listening assessment, Listening scripts, Prompt engineering, Test creation, Test items},
abstract = {To address the complexity and high costs of developing listening tests for test-takers of varying proficiency levels, this study investigates the capabilities of an OpenAI's large language model, ChatGPT 4, in developing listening assessments. Employing prompt engineering and fine-tuning of prompts, the study specifically focuses on creating listening scripts and test items using ChatGPT 4 for test-takers across a spectrum of proficiency levels (academic, low, intermediate, and advanced). For comparability, the 24 topics of these scripts were selected from topics found in academic listening tests. We conducted two types of analyses to evaluate the quality of the output. First, we performed linguistic analyses of the scripts using Coh-Metrix and Text Inspector to determine if the scripts varied linguistically as required by the prompts. Second, we analyzed topic variation and the degree of overlap in the test items. Results indicated that while ChatGPT 4 reliably produced scripts with significant textual variations, the test items generated were often long and exhibited semantic overlaps among options. This effect was also influenced by the topic. We discuss the ethical complexities that arise from the use of generative artificial intelligence (AI), and how generative AI (GenAI) can potentially benefit practitioners and researchers in language assessment, while recognizing its limitations.}
}
@article{YIRMIYA2025,
title = {Mentalizing Without a Mind: Psychotherapeutic Potential of Generative AI},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/79156},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125013561},
author = {Karen Yirmiya and Peter Fonagy},
keywords = {generative artificial intelligence, psychotherapy, mentalization, epistemic trust, reflective functioning},
abstract = {This paper explores the integration of generative artificial intelligence (AI) into psychotherapeutic practice through the lens of mentalization theory, with a particular focus on epistemic trust—a critical relational mechanism that facilitates psychological change. We critically examine AI’s capability to replicate core therapeutic components, such as empathy, embodied mentalizing, biobehavioral synchrony, and reciprocal mentalizing. Although current AI systems, especially large language models, demonstrate significant potential in simulating emotional responsiveness, cognitive empathy, and therapeutic dialogue, fundamental limitations persist. AI’s inherent lack of genuine emotional presence, reciprocal intentionality, and affective commitment constrains its ability to foster authentic epistemic trust and meaningful therapeutic relationships. Additionally, we outline significant risks, notably for individuals with complex trauma or relational vulnerabilities, highlighting concerns regarding pseudo-empathy, mistaking phenomenal experience for objective reality (psychic equivalence), fruitless ungrounded pursuit of social understanding (hypermentalization), and epistemic exploitation of individuals in whom artificial understanding by AI triggers excessive credulity. Nonetheless, we propose ethically informed pathways for integrating AI to enhance clinical practice, therapist training, and client care, particularly in augmenting human capacities within group and adjunctive therapy contexts. Paradoxically, AI could support psychotherapists in improving their capacity to mentalize, improve their understanding of their clients, and provide such understanding within the moral constraints that normally govern their work. This paper calls for careful ethical regulation similar to that limiting genetic manipulation, interdisciplinary research, and clinician involvement in shaping future AI-based psychotherapeutic models, emphasizing that AI’s role should complement rather than replace the irreplaceable relational core of psychotherapy.}
}
@article{ABRUSCI2025100401,
title = {AI4Design: A generative AI-based system to improve creativity in design–A field evaluation},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100401},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100401},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000414},
author = {Luca Abrusci and Karma Dabaghi and Stefano D'Urso and Filippo Sciarrone},
keywords = {Creativity, Design, Generative artificial intelligence},
abstract = {Chatbots serve as valuable instruments for enhancing students' educational experience and aiding them in their day-to-day academic tasks. Advances in Generative AI (GAI) have ushered in increasingly sophisticated and adaptive chatbots, with ChatGPT and DALL⋅E being prime examples. ChatGPT excels at generating text-based answers across diverse areas of knowledge, while DALL⋅E is adept at converting text-based concepts into visual imagery. These technologies are increasingly used by students across various levels of education. In this study, we introduce AI4Design, a web-based system designed to assist design students with their course projects by acting as an intelligent chatbot. The field of design is propitious for such work because of the increasing use of technology and the necessity of introducing its critical use during study. Comprising two integrated modules, the system is based on a two-step workflow. The first step is anchored on ChatGPT, enabling students to prompt questions and receive answers. The second step allows for the generation of one or more images based on the system's answer to the initial question. Our research assesses whether our system can offer valuable insights and inspiration to students in their design work. We conducted an exploratory study in the Design domain involving 31 students from the Lebanese American University. Over a two- to three-day period, participants used the AI4Design system to enhance their projects. A subsequent evaluation of their work indicated improvements in conceptual clarity and visual outputs that highlighted a measurable increase in creativity, supporting the efficacy of both the system and its foundational learning model, which will be confirmed in the future through a large-scale experimental study. Meanwhile, our study suggests that in the iterative design process, GAI can assist students in making better decisions by giving them just-in-time access to a broader palette of possibilities.}
}
@article{FURIZAL2025101882,
title = {Social, legal, and ethical implications of AI-Generated deepfake pornography on digital platforms: A systematic literature review},
journal = {Social Sciences & Humanities Open},
volume = {12},
pages = {101882},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101882},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125006102},
author = { Furizal and Alfian Ma'arif and Hari Maghfiroh and Iswanto Suwarno and Denis Prayogi and  Kariyamin and Syahrani Lonang and Abdel-Nasser Sharkawy},
keywords = {Generative artificial intelligence, Deepfake pornography, Social impact, Ethical implication, Legal reform, Privacy},
abstract = {The rapid development of AI has fuelled the spread of deepfake pornography synthetic content that realistically fakes an individual's identity without their consent. This phenomenon has complex social, legal, and ethical implications, particularly related to privacy violations, sexual exploitation, and legal vulnerabilities. This study aims to analyze the social impacts of deepfake pornography, identify existing legal gaps, and evaluate the ethical and regulatory responses that have emerged globally. Using the SLR approach, this study adopts the PICOS framework and PRISMA methodology in the screening and selection of scientific publications. The study finds that the majority of victims, especially women and vulnerable groups, experience psychological, social, and professional harm. Barriers to access to justice are exacerbated by weak legal frameworks, limited capacity of law enforcement officers, and gender bias in legal protection. The absence of a specific legal definition widens the scope for exploitation and exacerbates social inequality. The study recommends comprehensive legal reforms, including criminalization of non-consensual deepfake content, obligations for digital platforms in content moderation, and adoption of technologies such as watermarking (visible and invisible), C2PA standards-based metadata labelling, and advanced AI detection to track synthetic media. Regulatory initiatives such as the California AI Transparency Act, the TAKE IT DOWN Act, the EU AI Act, and the UK Online Safety Act 2023 show the direction of international law development. In addition, public education about the dangers of deepfakes and their legal consequences is an important part of prevention efforts. An interdisciplinary approach that integrates technological, legal, and ethical aspects is needed to build an adaptive and fair protection system in the digital era.}
}
@article{TSAO2024101865,
title = {Beyond the author: Artificial intelligence, creative writing and intellectual emancipation},
journal = {Poetics},
volume = {102},
pages = {101865},
year = {2024},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2024.101865},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X24000044},
author = {Jack Tsao and Collier Nogues},
keywords = {Generative AI (GenAI), Artificial Intelligence (AI) literacies, Creativity, Intellectual emancipation, Creative writing, Jacques Rancière},
abstract = {This study explores university students’ engagement with Generative Artificial Intelligence (GenAI) tools for creative writing and graphic storytelling, drawing on Jacques Rancière's philosophy of intellectual equality and emancipation. Qualitative data analysis from a co-curricular creative writing programme, including reflections, surveys, and focus-group interviews, reveals emerging artificial intelligence literacies and students’ improvisational aptitudes for interpreting, subverting, and transforming notions of authorship. Students decentred authorial attribution through the pragmatic adoption of the technology as a creative catalyst, negotiated creative conventions by adopting non-conventional communication strategies, and reconceptualised creativity as distributed across human and non-human agents. Our approach of student-driven learning for autonomous exploration, sense-making, and criticality with GenAI indicates the potential for promoting conditions for students to exercise intellectual equality and emancipation. The findings contribute to the understanding of authorship and creativity; begin to contour emerging GenAI literacies and competencies; and suggest that creative collaborations with GenAI may be a promising way to foster emancipatory practices in the classroom, while nurturing creative and critical skills.}
}
@article{ZHENG2025100990,
title = {Teaching via LLM-enhanced simulations: Authenticity and barriers to suspension of disbelief},
journal = {The Internet and Higher Education},
volume = {65},
pages = {100990},
year = {2025},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2024.100990},
url = {https://www.sciencedirect.com/science/article/pii/S1096751624000526},
author = {Longwei Zheng and Fei Jiang and Xiaoqing Gu and Yuanyuan Li and Gong Wang and Haomin Zhang},
keywords = {Simulation-based learning, Suspension of disbelief, Large language model, Teacher education, Authenticity},
abstract = {As an innovative method in professional training, simulation-based learning (SBL) has been introduced into teacher education, providing pre-service teacher candidates with experiential learning opportunities. This study explores the efficacy of SBL using large language models (LLMs) to enhance teacher training, focusing on learners' suspension of disbelief (SoD). As a highly advanced form of generative artificial intelligence, LLMs possess robust capabilities in simulating human behavior, which can be harnessed to create simulated students for SBL in teacher training. This instrumental case study examines the experiences of 12 pre-service teachers who participated in a session featuring an LLM-enhanced simulation. The simulation facilitated naturalistic classroom interactions between the participants and simulated students. Our research aimed to understand how pre-service teachers perceive LLM-enhanced SBL, identify factors that influence SoD, and determine the authenticity barriers. Interview data were analyzed using various coding techniques and derived themes from these codes. The findings revealed that LLM-enhanced SBL provided a realistic and engaging environment, significantly benefiting teaching skill development and learning transfer. However, challenges such as lagging responses, weak comprehension of complex contexts, inconsistencies in simulated students' cognition, and incongruent feedback were noted. The primary contribution of this study lies in demonstrating the potential of using LLMs to replace human actors, though significant technical challenges remain. The study also indicates that enhancements in LLM fine-tuning and prompt engineering are needed to improve LLMs' understanding of classroom context and students' cognitive patterns.}
}
@article{HAN2025100714,
title = {The impact of GenAI on learning outcomes: A systematic review and meta-analysis of experimental studies},
journal = {Educational Research Review},
volume = {48},
pages = {100714},
year = {2025},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2025.100714},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X2500051X},
author = {Xiaoli Han and Hongchao Peng and Mingzhuo Liu},
keywords = {GenAI, ChatGPT, Learning outcomes, Systematic review, Meta-analysis},
abstract = {Generative Artificial Intelligence (GenAI) tools such as ChatGPT, Claude, and Gemini are increasingly being integrated into educational environments, prompting questions about their actual impact on student learning. While a growing body of literature reports anecdotal or correlational evidence of GenAI's educational potential, rigorous causal evaluations remain limited. To bridge this gap, this study conducted a systematic review and meta-analysis of experimental and quasi-experimental studies investigating the effect of GenAI on learning outcomes. Following PRISMA guidelines, we screened five academic databases and identified 68 relevant studies published between 2022 and 2025. These studies yielded a total of 337 effect sizes across various educational levels, subject domains, and instructional contexts. The meta-analysis revealed a moderate overall positive effect of GenAI on learning outcomes (SMD = 0.45, 95 % CI [0.43, 0.47]), suggesting that GenAI-supported interventions are generally more effective than traditional instruction. However, substantial heterogeneity was observed across studies (I2 = 95 %), indicating that the magnitude of GenAI's impact varies significantly depending on contextual and methodological factors. Moderator analyses revealed stronger effects in primary and secondary education, within natural science disciplines, and in short-term interventions with smaller sample sizes. These patterns point to both the promise and the complexity of GenAI integration in educational practice. In conclusion, while GenAI shows considerable promise for enhancing learning outcomes, its true potential will only be realized through sustained, efforts to evaluate how, when, and for whom these technologies work best in diverse learning ecosystems.}
}
@article{MASROURI2025102428,
title = {Animal-skin-pattern-inspired multifunctional composites by generative AI},
journal = {Cell Reports Physical Science},
volume = {6},
number = {2},
pages = {102428},
year = {2025},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2025.102428},
url = {https://www.sciencedirect.com/science/article/pii/S266638642500027X},
author = {Milad Masrouri and Akshay Vilas Jadhav and Zhao Qin},
keywords = {composite design, bioinspiration, generative AI, molecular dynamics, elastic network, animal pattern, biomimicry, 3D printing, toughness modulus, multifunctional materials},
abstract = {Summary
Bioinspired composite materials offer several advantages by mimicking the structure of natural counterparts. However, their complex hierarchical structure, compared to the limited number of observations, makes it difficult to extract all the structural features and vary the structure to optimize the materials’ functions without losing their natural features. We applied generative artificial intelligence (GenAI) to design composites inspired by animal skin patterns, leveraging a small dataset to generate diverse configurations that closely emulate natural designs. Our computational simulations investigated the structure-mechanics relationship in these materials, revealing significant variations in mechanical functions and identifying patterns that exhibited superior mechanical properties. We validated these outstanding configurations’ performance through tensile tests on specimens produced by a multimaterial printer. We showcase GenAI’s role in structural augmentation that can yield rational bioinspired designs, complemented by an educational web page with interactive games for public access.}
}
@article{LENG2026103076,
title = {AIGC-empowered smart manufacturing: Prospects and challenges},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {97},
pages = {103076},
year = {2026},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.103076},
url = {https://www.sciencedirect.com/science/article/pii/S0736584525001309},
author = {Jiewu Leng and Keyou Zheng and Rongjie Li and Chong Chen and Baicun Wang and Qiang Liu and Xin Chen and Weiming Shen},
keywords = {Artificial intelligence generated content, Generative artificial intelligence, Smart manufacturing, GenAI Agent, Industry 5.0},
abstract = {Generative AI (GenAI), the technology behind Artificial Intelligence Generated Content (AIGC), has emerged as a transformative technology in smart manufacturing. However, its full potential and integration within manufacturing processes remain unexplored. This paper presents a comprehensive framework that aligns a GenAI-centered approach with Product Lifecycle Management (PLM), systematically examining the AIGC landscape and its applications across various manufacturing phases. To ensure accuracy and relevance, a human-in-the-loop pipeline is employed to curate and analyze cutting-edge research. Key contributions of this study include: 1) a holistic perspective on AIGC-empowered smart manufacturing, 2) an in-depth analysis of the current technological landscape, and 3) the identification of critical research challenges and future directions. Additionally, the paper considers Industry 5.0 principles, emphasizing human-centricity, sustainability, and resilience. By fostering discussion and collaboration, this review aims to advance innovation and unlock the full potential of AIGC in smart manufacturing.}
}
@article{ROBERTS2024103081,
title = {Artificial intelligence and innovation management: Charting the evolving landscape},
journal = {Technovation},
volume = {136},
pages = {103081},
year = {2024},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2024.103081},
url = {https://www.sciencedirect.com/science/article/pii/S0166497224001317},
author = {Deborah L. Roberts and Marina Candi},
keywords = {Artificial intelligence, Generative artificial intelligence, Innovation management, Innovation process},
abstract = {The excitement surrounding Artificial Intelligence (AI) is palpable. It is rapidly gaining prevalence in academia, business, and personal use. In particular, the emergence of generative AI, exemplified by large language models such as ChatGPT, has been marked by substantial media attention, discourse, and hype. Like most, if not all, aspects of business, innovation processes have been impacted. However, little is known about the degree of impact or the benefits that might be gained. To cut through the hype and understand the use of AI in innovation processes in businesses today, a large-scale survey amongst innovation managers in the USA was conducted, followed by interviews. The findings indicate that the use of AI in innovation processes is high and widespread, with AI being used for more than half of the surveyed firms' innovation projects. Furthermore, AI is used more in the development stage of the innovation process than in the idea or commercialization stages, which counters much of the existing discourse, which focuses on the idea stage. We uncover interesting differences by comparing the use and impact of generative AI with that of more traditional AI. Among these is a significant difference in expected benefits in making employees’ jobs more fulfilling — managers believe generative AI is more likely to confer this benefit than traditional AI. This paper offers two valuable contributions. First, it enriches the evolving dialogue at the intersection of AI and innovation management by offering much-needed empirical evidence about practical applications. Second, it provides timely managerial implications by examining relationships between the use of AI and innovation performance and understanding the benefits that AI can confer in the innovation process.}
}
@article{BLEASE2024,
title = {Generative Language Models and Open Notes: Exploring the Promise and Limitations},
journal = {JMIR Medical Education},
volume = {10},
year = {2024},
issn = {2369-3762},
doi = {https://doi.org/10.2196/51183},
url = {https://www.sciencedirect.com/science/article/pii/S2369376224000035},
author = {Charlotte Blease and John Torous and Brian McMillan and Maria Hägglund and Kenneth D Mandl},
keywords = {ChatGPT, generative language models, large language models, medical education, Open Notes, online record access, patient-centered care, empathy, language model, online record access, documentation, communication tool, clinical documentation},
abstract = {Patients’ online record access (ORA) is growing worldwide. In some countries, including the United States and Sweden, access is advanced with patients obtaining rapid access to their full records on the web including laboratory and test results, lists of prescribed medications, vaccinations, and even the very narrative reports written by clinicians (the latter, commonly referred to as “open notes”). In the United States, patient’s ORA is also available in a downloadable form for use with other apps. While survey studies have shown that some patients report many benefits from ORA, there remain challenges with implementation around writing clinical documentation that patients may now read. With ORA, the functionality of the record is evolving; it is no longer only an aide memoire for doctors but also a communication tool for patients. Studies suggest that clinicians are changing how they write documentation, inviting worries about accuracy and completeness. Other concerns include work burdens; while few objective studies have examined the impact of ORA on workload, some research suggests that clinicians are spending more time writing notes and answering queries related to patients’ records. Aimed at addressing some of these concerns, clinician and patient education strategies have been proposed. In this viewpoint paper, we explore these approaches and suggest another longer-term strategy: the use of generative artificial intelligence (AI) to support clinicians in documenting narrative summaries that patients will find easier to understand. Applied to narrative clinical documentation, we suggest that such approaches may significantly help preserve the accuracy of notes, strengthen writing clarity and signals of empathy and patient-centered care, and serve as a buffer against documentation work burdens. However, we also consider the current risks associated with existing generative AI. We emphasize that for this innovation to play a key role in ORA, the cocreation of clinical notes will be imperative. We also caution that clinicians will need to be supported in how to work alongside generative AI to optimize its considerable potential.}
}
@article{NISHISAKO2025,
title = {Reducing Hallucinations and Trade-Offs in Responses in Generative AI Chatbots for Cancer Information: Development and Evaluation Study},
journal = {JMIR Cancer},
volume = {11},
year = {2025},
issn = {2369-1999},
doi = {https://doi.org/10.2196/70176},
url = {https://www.sciencedirect.com/science/article/pii/S2369199925001077},
author = {Sota Nishisako and Takahiro Higashi and Fumihiko Wakao},
keywords = {artificial intelligence, AI, generative AI chatbot, generative pretrained transformer, GPT, retrieval-augmented generation, RAG, hallucination, medical information provision, cancer information service},
abstract = {Background
Generative artificial intelligence (AI) is increasingly used to find information. Providing accurate information is essential to support patients with cancer and their families; however, information returned by generative AIs is sometimes wrong. Returning wrong information is called hallucination. Retrieval-augmented generation (RAG), which supplements large language model (LLM) outputs with relevant external sources, has the potential to reduce hallucinations. Although RAG has been proposed as a promising technique, its real-world performance in public health communication remains underexplored.
Objective
This study aimed to examine cancer information returned by generative AIs with RAG using cancer-specific information sources and general internet searches to determine whether using RAG with reliable information sources reduces the hallucination rates of generative AI chatbots.
Methods
We developed 6 types of chatbots by combining 3 patterns of reference information with 2 versions of LLMs. Thus, GPT-4 and GPT-3.5 chatbots that use cancer information service (CIS) information, Google information, and no reference information (conventional chatbots) were developed. A total of 62 cancer-related questions in Japanese were compiled from public sources. All responses were generated automatically and independently reviewed by 2 experienced clinicians. The reviewers assessed the presence of hallucinations, defined as medically harmful or misinformation. We compared hallucination rates across chatbot types and calculated odds ratios (OR) using generalized linear mixed-effects models. Subgroup analyses were also performed based on whether questions were covered by CIS content.
Results
For the chatbots that used information from CIS, the hallucination rates were 0% for GPT-4 and 6% for GPT-3.5, whereas those for chatbots that used information from Google were 6% and 10% for GPT-4 and GPT-3.5, respectively. For questions on information that is not issued by CIS, the hallucination rates for Google-based chatbots were 19% for GPT-4 and 35% for GPT-3.5. The hallucination rates for conventional chatbots were approximately 40%. Using reference data from Google searches generated more hallucinations than using CIS data, with an OR of 9.4 (95% CI 1.2‐17.5, P<.01); the OR for the conventional chatbot was 16.1 (95% CI 3.7‐50.0, P<.001). While conventional chatbots always generated a response, the RAG-based chatbots sometimes declined to answer when information was lacking. The conventional chatbots responded to all questions, but the response rate decreased (36% to 81%) for RAG-based chatbots. For questions on information not covered by CIS, the CIS chatbots did not respond, while the Google chatbots generated responses in 52% of the cases for GPT-4 and 71% for GPT-3.5.
Conclusions
Using RAG with reliable information sources significantly reduces the hallucination rate of generative AI chatbots and increases the ability to admit lack of information, making them more suitable for general use, where users need to be provided with accurate information.}
}
@article{RODRIGUEZSANCHEZ2025103036,
title = {Generative AI as a source of information on environmental Problems: Understanding its influence on Generation Z},
journal = {Technology in Society},
volume = {83},
pages = {103036},
year = {2025},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.103036},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2500226X},
author = {Carla Rodriguez-Sanchez and Franco Manuel Sancho-Esper and Luis Vicente Casaló and Manuela López},
keywords = {Generative AI, ChatGPT, Environmental attitudes, Information usefulness, Information credibility, Technological innovativeness},
abstract = {Generative artificial intelligence (GenAI) is rapidly emerging as a transformative tool in digital information dissemination. However, its influence on environmental communication problems remains underexplored. This research examines whether exposure to information on an environmental problem provided by GenAI (namely ChatGPT) influences individuals' pro-environmental outcomes in terms of awareness of environmental consequences, moral obligation, and attitudes toward that problem. It addresses the research gap on GenAI's effectiveness compared to traditional sources (digital newspapers) in shaping environmental perceptions, as well as the underlying mechanisms behind this effectiveness. Two experimental studies were conducted with samples of Generation Z users given this demographic's high level of engagement with AI tools. Study 1 examined changes in pro-environmental outcomes when participants were exposed to ChatGPT-generated messages about water scarcity or messages in a digital newspaper. Study 2 focused on the underlying mechanisms of ChatGPT's influence. It examined how perceived usefulness of information mediates this influence and how individual characteristics (psychological distance to the problem and technological innovativeness) moderate this process. Exposure to messages about water scarcity significantly enhanced participants' awareness of consequences, moral obligation, and attitudes toward the problem, regardless of the source. In Study 2, emphasizing problem severity enhanced perceived usefulness, a key factor in shaping pro-environmental outcomes. However, this effect varied based on individual characteristics, with moderation by psychological distance and technological innovativeness. These findings contribute to the growing discussion about the role of GenAI in environmental communication. The paper discusses the potential implications of GenAI for fostering pro-environmental attitudes and behaviors. It emphasizes the need to consider audience-specific factors to maximize the effectiveness of GenAI.}
}
@article{ZHANG2025103046,
title = {Horizontal multi-party data publishing via discriminator regularization and adaptive noise under differential privacy},
journal = {Information Fusion},
volume = {120},
pages = {103046},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103046},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001198},
author = {Pengfei Zhang and Xiang Fang and Zhikun Zhang and Xianjin Fang and Yining Liu and Ji Zhang},
keywords = {Generative Artificial Intelligence, Generative adversarial network, Differential privacy, Multi-party data publishing, Information fusion},
abstract = {With the rapid proliferation of data collection and storage technologies, the growing demand for horizontal multi-party data publishing has created an urgent need for robust privacy-preserving mechanisms that can effectively handle sensitive distributed data across multiple organizations. While existing approaches attempt to address this challenge, they often fail to balance privacy protection with data utility, struggle to achieve effective information fusion across heterogeneous data distributions, and incur significant computational overhead. In this paper, we introduce the NATION approach, an innovative GAN-based framework that advances multi-party data publishing through sophisticated information fusion techniques while maintaining stringent differential privacy guarantees and computational efficiency. In NATION, we modify the traditional GAN architecture through a distributed design where multiple discriminators are strategically allocated across parties while centralizing the generator at a semi-trusted server, enabling seamless fusion of distributed knowledge with minimal computational cost. Building on this foundation, we introduce two key technical innovations: an iterative-aware adaptive noise IAN method that dynamically optimizes noise injection based on training convergence, and a global-aware discriminator regularization GDR method that leverages Bregman Divergence to enhance inter-discriminator information exchange while ensuring model stability. Through comprehensive theoretical analysis and extensive experimental evaluation on real-world datasets, we demonstrate that NATION consistently outperforms state-of-the-art approaches by up to 7% in accuracy while providing provable privacy guarantees, which makes a significant advancement in secure GAN-based information fusion for privacy-sensitive applications.}
}
@article{MORAVEC2025100691,
title = {Environmental footprint of GenAI – Changing technological future or planet climate?},
journal = {Journal of Innovation & Knowledge},
volume = {10},
number = {3},
pages = {100691},
year = {2025},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2025.100691},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X25000411},
author = {Vaclav Moravec and Beata Gavurova and Viliam Kovac},
keywords = {Generative artificial intelligence, ChatGPT, DeepSeek, Artificial intelligence literacy, Climate change, Data centre, Czechia},
abstract = {The beginnings of generative artificial intelligence (GenAI), led by Chat Generative Pre-Trained Transformer (ChatGPT), not only change the behaviour of digital media ecosystem users but also increase the energy consumption of enterprises working with GenAI, which presents them with a fundamental challenge in the era of climate change. This study aims to examine the relationships between the selected aspects of the use of GenAI tools and the environmental perception and behaviour of their users to understand the population's current environmental attitudes towards environmental risks and environmental sustainability. The survey was conducted in October 2024 on a sample of 1,268 respondents of the Czech Republic population. To process the data set, a logistic regression analysis, chi-squared test, Akaike information criterion, and Bayesian information criterion are employed. The results show that the more often people use GenAI tools, the more distant they consider the effects of climate change in time. The low frequency of use of ChatGPT may influence a higher willingness to change popular GenAI tools that are not maintained by environmentally friendly data centres. The frequency of ChatGPT use influences individuals’ perception of the importance of climate-change solving. The more frequently the respondents use artificial intelligence (AI) systems, they less perceive climate change as important. The low frequency of ChatGPT usage is associated with lower willingness to change email provider, transfer own data, leave social networks, stop using a favourite streaming platform and stop using a favourite GenAI platform. The respondents’ attitudes show a visible behavioural change. Internal personal motivation and self-confidence in learning, interest in career and self-confidence when using AI, the behavioural aspects, and the cognitive aspects are altered considerably. Based on the outcomes of the population survey, the study concludes that the issue of environmental friendliness of AI tools should become part of AI literacy that could strengthen population's willingness to use more energy-efficient GenAI platforms. The listed challenges are important in the perspective of the latest technological development, as shown by the discussion on the energy and computational demands of the GenAI platform DeepSeek, which is also discussed in the study.}
}
@article{URBAN2024105031,
title = {ChatGPT improves creative problem-solving performance in university students: An experimental study},
journal = {Computers & Education},
volume = {215},
pages = {105031},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105031},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524000459},
author = {Marek Urban and Filip Děchtěrenko and Jiří Lukavský and Veronika Hrabalová and Filip Svacha and Cyril Brom and Kamila Urban},
keywords = {Generative AI, ChatGPT, Creativity, Metacognitive monitoring, Metacognitive experiences, Ill-defined problem-solving task},
abstract = {University students often employ generative artificial intelligence tools such as ChatGPT in resolution of ill-defined problem-solving tasks. However, the experimental evidence about effects of ChatGPT on complex problem-solving performance is still missing. In this preregistered experiment, the impact of ChatGPT on performance in a complex creative problem-solving task was investigated in 77 university students solving a task with ChatGPT in comparison to 68 students solving a task without it. ChatGPT use significantly improved self-efficacy for task resolution (d = 0.65) and enhanced the quality (d = 0.69), elaboration (d = 0.61), and originality (d = 0.55) of solutions. Moreover, participants with ChatGPT assistance perceived task as easier (d = 0.56) and requiring less mental effort (d = 0.58). However, use of ChatGPT did not make task resolution more interesting (d = 0.08), and the impact of ChatGPT on metacognitive monitoring accuracy was unclear. Although there were no significant differences in absolute accuracy between students solving the task with and without the assistance of ChatGPT, the absence of correlation between self-evaluation judgments and performance suggests that participants struggled to calibrate their self-evaluations when using ChatGPT. Notably, the perceived usefulness of ChatGPT appeared to inform self-evaluation judgments, resulting in higher inaccuracy. The implications for hybrid human-AI regulation (HHAIR) theory are discussed. To regulate effectively, students using AI tools should focus on valid metacognitive cues instead of the perceived ease of ChatGPT-assisted problem-solving.}
}
@article{YU2025107802,
title = {Measuring the quality of generative AI systems: Mapping metrics to quality characteristics — Snowballing literature review},
journal = {Information and Software Technology},
volume = {186},
pages = {107802},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107802},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925001417},
author = {Liang Yu and Emil Alégroth and Panagiota Chatzipetrou and Tony Gorschek},
keywords = {Generative AI, GenAI, Large language model, LLM, Quality characteristics, Metric, Evaluation},
abstract = {Context
Generative Artificial Intelligence (GenAI) and the use of Large Language Models (LLMs) have revolutionized tasks that previously required significant human effort, which has attracted considerable interest from industry stakeholders. This growing interest has accelerated the integration of AI models into various industrial applications. However, the model integration introduces challenges to product quality, as conventional quality measuring methods may fail to assess GenAI systems. Consequently, evaluation techniques for GenAI systems need to be adapted and refined. Examining the current state and applicability of evaluation techniques for the GenAI system outputs is essential.
Objective
This study aims to explore the current metrics, methods, and processes for assessing the outputs of GenAI systems and the potential of risky outputs.
Method
We performed a snowballing literature review to identify metrics, evaluation methods, and evaluation processes from 43 selected papers.
Results
We identified 28 metrics and mapped these metrics to four quality characteristics defined by the ISO/IEC 25023 standard for software systems. Additionally, we discovered three types of evaluation methods to measure the quality of system outputs and a three-step process to assess faulty system outputs. Based on these insights, we suggested a five-step framework for measuring system quality while utilizing GenAI models.
Conclusion
Our findings present a mapping that visualizes candidate metrics to be selected for measuring quality characteristics of GenAI systems, accompanied by step-by-step processes to assist practitioners in conducting quality assessments.}
}
@article{BOUSQUET2025,
title = {Advantages and Inconveniences of a Multi-Agent Large Language Model System to Mitigate Cognitive Biases in Diagnostic Challenges},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/69742},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125000871},
author = {Cedric Bousquet and Divà Beltramin},
keywords = {large language model, multi-agent system, diagnostic errors, cognition, clinical decision-making, cognitive bias, generative artificial intelligence}
}
@article{ALTORFER20251635,
title = {The double-edged sword of generative AI: surpassing an expert or a deceptive “false friend”?},
journal = {The Spine Journal},
volume = {25},
number = {8},
pages = {1635-1643},
year = {2025},
issn = {1529-9430},
doi = {https://doi.org/10.1016/j.spinee.2025.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1529943025001226},
author = {Franziska C.S. Altorfer and Michael J. Kelly and Fedan Avrumova and Varun Rohatgi and Jiaqi Zhu and Christopher M. Bono and Darren R. Lebl},
keywords = {Bard, ChatGPT, Claude, Evidence-based, Gemini, Generative, LLM, NASS, Spine},
abstract = {BACKGROUND CONTEXT
Generative artificial intelligence (AI), ChatGPT being the most popular example, has been extensively assessed for its capability to respond to medical questions, such as queries in spine treatment approaches or technological advances. However, it often lacks scientific foundation or fabricates inauthentic references, also known as AI hallucinations.
PURPOSE
To develop an understanding of the scientific basis of generative AI tools by studying the authenticity of references and reliability in comparison to the alignment of responses of evidence-based guidelines.
STUDY DESIGN
Comparative study.
METHODS
Thirty-three previously published North American Spine Society (NASS) guideline questions were posed as prompts to 2 freely available generative AI tools (Tools I and II). The responses were scored for correctness compared with the published NASS guideline responses using a 5-point “alignment score." Furthermore, all cited references were evaluated for authenticity, source type, year of publication, and inclusion in the scientific guidelines.
RESULTS
Both tools’ responses to guideline questions achieved an overall score of 3.5±1.1, which is considered acceptable to be equivalent to the guideline. Both tools generated 254 references to support their responses, of which 76.0% (n=193) were authentic and 24.0% (n=61) were fabricated. From these, authentic references were: peer-reviewed scientific research papers (147, 76.2%), guidelines (16, 8.3%), educational websites (9, 4.7%), books (9, 4.7%), a government website (1, 0.5%), insurance websites (6, 3.1%) and newspaper websites (5, 2.6%). Claude referenced significantly more authentic peer-reviewed scientific papers (Claude: n=111, 91.0%; Gemini: n=36, 50.7%; p<.001). The year of publication amongst all references ranged from 1988-2023, with significantly older references provided by Claude (Claude: 2008±6; Gemini: 2014±6; p<.001). Lastly, significantly more references provided by Claude were also referenced in the published NASS guidelines (Claude: n=27, 24.3%; Gemini: n=1, 2.8%; p=.04).
CONCLUSIONS
Both generative AI tools provided responses that had acceptable alignment with NASS evidence-based guideline recommendations and offered references, though nearly a quarter of the references were inauthentic or nonscientific sources. This deficiency of legitimate scientific references does not meet standards for clinical implementation. Considering this limitation, caution should be exercised when applying the output of generative AI tools to clinical applications.}
}
@article{SATTELMAIER2025101838,
title = {Be AIware! An AI Competency Model for K-12 Education},
journal = {Social Sciences & Humanities Open},
volume = {12},
pages = {101838},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101838},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125005662},
author = {Lana Sattelmaier and Jan Pawlowski},
keywords = {21st century abilities, AI Literacy, K-12 education, AI competencies, Proficiency levels},
abstract = {The rapid emergence of generative artificial intelligence (GAI) is reshaping educational contexts, creating an urgent need for K–12 systems to adapt. Teachers, in particular, face increasing demands to respond to AI-driven transformations in learning environments yet lack structured guidance on which competencies matter. This paper presents the AIware Competency Model, a comprehensive framework of 98 AI-related competencies designed for K–12 education, including emerging aspects of GAI. The model was developed using an Action Design Research (ADR) approach, structured across four iterative stages: (1) a systematic literature review and analysis of 14 existing AI competency models to identify conceptual gaps; (2) semi-structured interviews with AI experts and educators (n = 5) to inform initial model development; (3) three rounds of focus groups involving teachers, policymakers, and AI professionals (total n = 46) to refine and evaluate the competencies; and (4) a competency ranking process (n = 10) to assess perceived importance across educational roles. The AIware Competency Model addresses current fragmentation in AI education research by offering a unified, empirically grounded framework aligned with practical classroom needs. It enables mapping to existing curriculum standards and supports both teacher preparation and student readiness in a world increasingly shaped by AI. The model contributes to the field by combining theoretical synthesis with practitioner input, offering a robust foundation for future research, curriculum design, and policy development.}
}
@article{YANG2025102560,
title = {Mapping the intersection of heart rate variability and complementary medicine: a two-decade bibliometric study (2005–2024)},
journal = {European Journal of Integrative Medicine},
volume = {79},
pages = {102560},
year = {2025},
issn = {1876-3820},
doi = {https://doi.org/10.1016/j.eujim.2025.102560},
url = {https://www.sciencedirect.com/science/article/pii/S187638202500109X},
author = {Shih-Wei Yang and Chen-Wei Chang and Malcolm Koo},
keywords = {Heart rate variability, Autonomic nervous system, Complementary medicine, Integrative medicine, Bibliometrics},
abstract = {Introduction
Heart Rate Variability (HRV), a noninvasive marker of autonomic nervous system function, has become an increasingly utilized tool in complementary medicine (CM) research for objectively assessing physiological responses to interventions. While specific applications of HRV within individual CM modalities have been reviewed, comprehensive mapping of the broader research landscape remains limited. This bibliometric study aimed to provide a detailed overview of the evolution, key contributors, and thematic development of HRV research across diverse CM interventions between 2005 and 2024.
Methods
Original research articles published between January 1, 2005, and December 31, 2024, were retrieved from the Science Citation Index Expanded of the Web of Science Core Collection. A Boolean search strategy combined HRV-related terms with keywords representing a wide range of CM interventions, while deliberately excluding broad or ambiguous descriptors. The data were analyzed using the Bibliometrix package.
Results
A total of 1007 articles published across 375 journals and authored by 4969 individuals were identified, with publication output showing a steady increase and peaking in 2022. China (23.5%) and the United States (19.1%) emerged as the most prolific contributors, although United States publications exhibited a higher average citation rate. Leading institutions included Kyung Hee University (South Korea) and China Medical University (Taiwan). Evidence-Based Complementary and Alternative Medicine was the most frequent publishing journal. HRV and CM research has increasingly appeared in higher-ranked journals over time (p < 0.001). Keyword co-occurrence analysis positioned HRV as a central term, connecting clusters related to mind-body practices, psychophysiological constructs, music therapy, and electroacupuncture. Thematic evolution analysis revealed recent trends toward greater methodological rigor, increased emphasis on patient-centered outcomes, diversification of CM interventions, and incorporation of emerging digital health technologies.
Conclusion
This bibliometric analysis shows HRV research in CM as an evolving field positioned at the intersection of traditional practices, contemporary physiological science, and technological innovation. Recent trends indicate a shift toward increased methodological rigor, greater emphasis on patient-centered outcomes, and integration of digital technologies. As the field advances, novel technologies such as generative artificial intelligence offer promise for deepening physiological insights and enhancing the clinical relevance of CM applications within mainstream healthcare.}
}
@article{SKULMOWSKI2023100023,
title = {Ethical issues of educational virtual reality},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100023},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100023},
url = {https://www.sciencedirect.com/science/article/pii/S294967802300017X},
author = {Alexander Skulmowski},
keywords = {Virtual reality, Education, Ethics, Realism, Autonomy, Privacy},
abstract = {In response to the high demand for digital learning as a surrogate for physical experiences, virtual reality (VR) is positioning itself as a tool for creating educational virtual experiences. VR technology faces a number of ethical issues, including a reduction of users’ autonomy, health problems, and privacy concerns. The use of VR and realism in education can turn out to be a double-edged sword. While realistic visualizations can promote learning for some content domains, they can hinder comprehension in others. Furthermore, the effects of realism on learning also depend on learners’ spatial abilities. Letting young children and teenagers engage in virtual educational experiences can expose them to manipulation, could lead to health issues, and may infringe on their privacy. In short, realism and virtual experiences may severely limit learners’ autonomy in a number of ways. Based on a review of the literature and considerations of emerging technologies such as generative artificial intelligence, this paper presents guidelines for the ethically sound utilization of VR and realism. By applying findings and conclusions established in the context of research on the ethics of VR to the educational utilization of this technology, I develop several suggestions that may help to avoid negative consequences of educational VR. These suggestions include the utilization of spatial ability testing, requiring virtual experiences to offer alternative paths to prevent manipulation, as well as using algorithms that deidentify the highly detailed developmental profiles that can be generated through educational VR use.}
}
@article{LIU2025112278,
title = {Agent design pattern catalogue: A collection of architectural patterns for foundation model based agents},
journal = {Journal of Systems and Software},
volume = {220},
pages = {112278},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112278},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003224},
author = {Yue Liu and Sin Kit Lo and Qinghua Lu and Liming Zhu and Dehai Zhao and Xiwei Xu and Stefan Harrer and Jon Whittle},
keywords = {Agent, Foundation model, Large language model, Pattern, Software engineering, Responsible AI},
abstract = {Foundation model-enabled generative artificial intelligence facilitates the development and implementation of agents, which can leverage distinguished reasoning and language processing capabilities to takes a proactive, autonomous role to pursue users’ goals. Nevertheless, there is a lack of systematic knowledge to guide practitioners in designing the agents considering challenges of goal-seeking (including generating instrumental goals and plans), such as hallucinations inherent in foundation models, explainability of reasoning process, complex accountability, etc. To address this issue, we have performed a systematic literature review to understand the state-of-the-art foundation model-based agents and the broader ecosystem. In this paper, we present a pattern catalogue consisting of 18 architectural patterns with analyses of the context, forces, and trade-offs as the outcomes from the previous literature review. We propose a decision model for selecting the patterns. The proposed catalogue can provide holistic guidance for the effective use of patterns, and support the architecture design of foundation model-based agents by facilitating goal-seeking and plan generation.}
}
@article{LEVKOVICH2025104970,
title = {Attributional patterns toward students with and without learning disabilities: Artificial intelligence models vs. trainee teachers},
journal = {Research in Developmental Disabilities},
volume = {160},
pages = {104970},
year = {2025},
issn = {0891-4222},
doi = {https://doi.org/10.1016/j.ridd.2025.104970},
url = {https://www.sciencedirect.com/science/article/pii/S089142222500054X},
author = {Inbar Levkovich and Eyal Rabin and Rania Hussein Farraj and Zohar Elyoseph},
keywords = {Generative artificial intelligence, Trainee teachers, Attribution, Learning disabilities, Expectations, Cultural differences},
abstract = {This study explored differences in the attributional patterns of four advanced artificial intelligence (AI) Large Language Models (LLMs): ChatGPT3.5, ChatGPT4, Claude, and Gemini) by focusing on feedback, frustration, sympathy, and expectations of future failure among students with and without learning disabilities (LD). These findings were compared with responses from a sample of Australian and Chinese trainee teachers, comprising individuals nearing qualification with varied demographic and educational backgrounds. Eight vignettes depicting students with varying abilities and efforts were evaluated by the LLMs ten times each, resulting in 320 evaluations, with trainee teachers providing comparable ratings. For LD students, the LLMs exhibited lower frustration and higher sympathy than trainee teachers, while for non-LD students, LLMs similarly showed lower frustration, with ChatGPT3.5 aligning closely with Chinese teachers and ChatGPT4 demonstrating more sympathy than both teacher groups. Notably, LLMs expressed lower expectations of future academic failure for both LD and non-LD students compared to trainee teachers. Regarding feedback, the findings reflect ratings of the qualitative nature of feedback LLMs and teachers would provide, rather than actual feedback text. The LLMs, particularly ChatGPT3.5 and Gemini, were rated as providing more negative feedback than trainee teachers, while ChatGPT4 provided more positive ratings for both LD and non-LD students, aligning with Chinese teachers in some cases. These findings suggest that LLMs may promote a positive and inclusive outlook for LD students by exhibiting lower judgmental tendencies and higher optimism. However, their tendency to rate feedback more negatively than trainee teachers highlights the need to recalibrate AI tools to better align with cultural and emotional nuances.}
}
@article{VANTAM2025113526,
title = {How generative AI reshapes construction and built environment: The good, the bad, and the ugly},
journal = {Building and Environment},
volume = {284},
pages = {113526},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2025.113526},
url = {https://www.sciencedirect.com/science/article/pii/S0360132325009990},
author = {Nguyen {Van Tam}},
keywords = {Generative AI, Construction, Built environment, The good, The bad, The ugly},
abstract = {Generative Artificial Intelligence (GenAI) is poised to fundamentally transform the Construction and Built Environment (CBE) industry by offering innovative solutions to longstanding challenges. However, like any disruptive innovation, the adoption of GenAI presents a dual-edged sword, offering immense benefits while simultaneously posing considerable difficulties and potential pitfalls. This paper aims to illuminate this complex interplay by systematically uncovering the “Good,” the “Bad,” and the “Ugly” aspects of GenAI adoption within the CBE sector. This discussion provides a valuable foundation for understanding GenAI’s multifaceted impact, thereby offering valuable insights for future research and practical implementation.}
}
@article{GONG2026103733,
title = {Beyond embedding-mapping: Social network alignment via generative information fusion and LLM-guided iterative mechanism},
journal = {Information Fusion},
volume = {127},
pages = {103733},
year = {2026},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103733},
url = {https://www.sciencedirect.com/science/article/pii/S156625352500795X},
author = {Jibing Gong and Jiquan Peng and Wei Wang and Wei Zhou and Chaozhuo Li and Philip S. Yu},
keywords = {Social network alignment, Large language model, Information fusion, Graph learning, Generative artificial intelligence},
abstract = {Social Network Alignment (SNA) aims to identify and match user accounts belonging to the same real-world individual across multiple social platforms, which has garnered growing research interest. Existing methods typically encode textual and structural information into a latent space and learn a mapping function from annotated user alignments to accomplish SNA. However, the inherent sparsity and noise in social data limit these models’ ability to fully capture user characteristics. Moreover, direct alignment based on latent space often overlooks critical details from the original information, reducing both alignment quality and interpretability. To address these limitations, we propose LLM-SNA, a novel framework that integrates generative information fusion and LLM-guided iterative mechanism. The generative information fusion leverages LLMs to transform sparse user attributes, microblogs, and neighbor descriptions into enriched, comprehensive user profiles. We further perform intra-network and inter-network graph learning on enriched user profiles to incorporate structural information. To balance accuracy and efficiency, the LLM-guided iterative mechanism first applies a coarse filter based on embedding similarities to collect potential alignment candidates. The LLM then evaluates these candidates by reasoning over their original textual information. If the LLM deems the candidates misaligned, the candidate set is expanded until confident matches emerge. Comprehensive experiments on three widely used datasets demonstrate the advantages of LLM-SNA over state-of-the-art baseline methods and highlight the potential of LLMs for SNA tasks.}
}
@article{CHANGALIMA2024101063,
title = {Social influence and information quality on Generative AI use among business students},
journal = {The International Journal of Management Education},
volume = {22},
number = {3},
pages = {101063},
year = {2024},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101063},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001344},
author = {Ismail Abdi Changalima and David Amani and Ismail Juma Ismail},
keywords = {Artificial intelligence, ChatGPT, Social influence, Information quality, Behavioural intention, Multigroup analysis},
abstract = {Despite the increasing utilisation of generative artificial intelligence (AI) in educational settings, its influence on shaping students’ behaviour remains relatively under-researched. This study employs PLS-SEM to explore the relationships between social influence, information quality, and behavioural intentions regarding ChatGPT usage among business students. Drawing from data collected from 477 business students, the study unveils that both social influence and information quality significantly impact behavioural intentions. Moreover, information quality strengthens the influence of social influence on behavioural intentions. The multigroup analysis reveals that the effects of social influence and information quality on behavioural intentions differ between females and males. However, the moderating effect of information quality does not differ significantly between them. Furthermore, the effects observed in all hypothesised relationships do not differ significantly between first-year and second-year students. By empirically validating the proposed model for behavioural intentions regarding ChatGPT usage and identifying statistical differences among males and females, as well as between first-year and second-year students, this study contributes to filling existing knowledge gaps. Furthermore, the study offers potential avenues for future research and serves as a valuable resource for academics, professionals, and policymakers interested in understanding students' engagement with generative AI.}
}
@article{GUO2023329,
title = {AIGC challenges and opportunities related to public safety: A case study of ChatGPT},
journal = {Journal of Safety Science and Resilience},
volume = {4},
number = {4},
pages = {329-339},
year = {2023},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666449623000397},
author = {Danhuai Guo and Huixuan Chen and Ruoling Wu and Yangang Wang},
keywords = {Generative artificial intelligence， Artificial intelligence generated content, ChatGPT, Public safety, Strong artificial intelligence},
abstract = {Artificial intelligence generated content (AIGC) is a production method based on artificial intelligence (AI) technology that finds rules through data and automatically generates content. In contrast to computational intelligence, generative AI, as exemplified by ChatGPT, exhibits characteristics that increasingly resemble human-level comprehension and creation processes. This paper provides a detailed technical framework and history of ChatGPT, followed by an examination of the challenges posed to political security, military security, economic security, cultural security, social security, ethical security, legal security, machine escape problems, and information leakage. Finally, this paper discusses the potential opportunities that AIGC presents in the realms of politics, military, cybersecurity, society, and public safety education.}
}
@article{HARRER2023104512,
title = {Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine},
journal = {eBioMedicine},
volume = {90},
pages = {104512},
year = {2023},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2023.104512},
url = {https://www.sciencedirect.com/science/article/pii/S2352396423000774},
author = {Stefan Harrer},
keywords = {Generative artificial intelligence, Large language models, Foundation models, AI ethics, Augmented human intelligence, Information management, AI trustworthiness},
abstract = {Summary
Large Language Models (LLMs) are a key component of generative artificial intelligence (AI) applications for creating new content including text, imagery, audio, code, and videos in response to textual instructions. Without human oversight, guidance and responsible design and operation, such generative AI applications will remain a party trick with substantial potential for creating and spreading misinformation or harmful and inaccurate content at unprecedented scale. However, if positioned and developed responsibly as companions to humans augmenting but not replacing their role in decision making, knowledge retrieval and other cognitive processes, they could evolve into highly efficient, trustworthy, assistive tools for information management. This perspective describes how such tools could transform data management workflows in healthcare and medicine, explains how the underlying technology works, provides an assessment of risks and limitations, and proposes an ethical, technical, and cultural framework for responsible design, development, and deployment. It seeks to incentivise users, developers, providers, and regulators of generative AI that utilises LLMs to collectively prepare for the transformational role this technology could play in evidence-based sectors.}
}
@article{CASTELLANO2025111,
title = {Clinical trial screening in gynecologic oncology: Defining the need and identifying best practices},
journal = {Gynecologic Oncology},
volume = {192},
pages = {111-119},
year = {2025},
issn = {0090-8258},
doi = {https://doi.org/10.1016/j.ygyno.2024.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0090825824012150},
author = {T. Castellano and O.D. Lara and C. McCormick and D. Chase and V. BaeJump and A.L. Jackson and J.T. Peppin and S. Ghamande and K.N. Moore and B. Pothuri and T.J. Herzog and T. Myers},
keywords = {Clinical trial screening, Artificial intelligence in clinical trial screening, Best practices for clinical trial screening, Clinical trial equity, Clinical trial screening survey in gynecologic oncology},
abstract = {Background
Evidence is limited in gynecologic cancers on best practices for clinical trial screening, but the risk of ineffective screening processes and subsequent under-enrollment introduces significant cost to patient, healthcare systems, and scientific advancement. Absence of a defined screening process makes determination of who and when to screen potential patients inconsistent allowing inefficiency and potential introduction of biases. This is especially germane as generative artificial intelligence (AI), and electronic health record (EHR) integration is applied to trial screening. Though often a requirement of cooperative groups such as the Cancer therapy Evaluation Program (CTEP), and/or the Commission on Cancer (CoC), there are no standard practice guidelines on best practices regarding screening and how best to track screening data.
Development of manuscript
The authors provided a review of current clinical trial screening practices and the effect on enrollment and trial activation across a variety of disease and practice sites. Established clinical trial screening practices and evidence supporting emerging strategies were reviewed and reported. Due to lack of published literature in gynecologic oncology, authors sought to survey the members of current rostered GOG sites to provide perspectives on clinical trial screening practices. Survey results showed a variety of screening practices. Most respondents participate in some type of manual screening process, where approximately 13 % also report incorporating AI or EHR integration. Over half (60 %) of sites track screening data to use for feasibility when opening new trials. The rapid increase in generative AI, EHR integration, and site agnostic screening initiatives could provide a significant opportunity to improve screening efficiency, translating to improved enrollment, but limitations and barriers remain.}
}
@article{LEE2024100283,
title = {Teachers' and students' perceptions of AI-generated concept explanations: Implications for integrating generative AI in computer science education},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100283},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100283},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000869},
author = {Soohwan Lee and Ki-Sang Song},
keywords = {Generative artificial intelligence(GAI), Elementary education, Concept explanations, Computer science education, Perceptual differences},
abstract = {The educational application of Generative AI (GAI) has garnered significant interest, sparking discussions about the pedagogical value of GAI-generated content. This study investigates the perceived effectiveness of concept explanations produced by GAI compared to those created by human teachers, focusing on programming concepts of sequence, selection, and iteration. The research also explores teachers' and students' ability to discern the source of these explanations. Participants included 11 teachers and 70 sixth-grade students who were presented with concept explanations created or generated by teachers and ChatGPT. They were asked to evaluate the helpfulness of the explanations and identify their source. Results indicated that teachers found GAI-generated explanations more helpful for sequence and selection concepts, while preferring teacher-created explanations for iteration (χ2(2, N = 11) = 10.062, p = .007, ω = .595). In contrast, students showed varying abilities to distinguish between AI-generated and teacher-created explanations across concepts, with significant differences observed (χ2(2, N = 70) = 22.127, p < .001, ω = .399). Notably, students demonstrated difficulty in identifying the source of explanations for the iteration concept (χ2(1, N = 70) = 8.45, p = .004, φ = .348). Qualitative analysis of open-ended responses revealed that teachers and students employed similar criteria for evaluating explanations but differed in their ability to discern the source. Teachers focused on pedagogical effectiveness, while students prioritized relatability and clarity. The findings highlight the importance of considering both teachers' and students' perspectives when integrating GAI into computer science education. The study proposes strategies for designing GAI-based explanations that cater to learners' needs and emphasizes the necessity of explicit AI literacy instruction. Limitations and future research directions are discussed, underlining the need for larger-scale studies and experimental designs that assess the impact of GAI on actual learning outcomes.}
}
@article{ATKINSON2025103129,
title = {AI-pocalypse now: Automating the systematic literature review with SPARK (Systematic processing and automated review Kit) – gathering, organising, filtering, and scaffolding.},
journal = {MethodsX},
volume = {14},
pages = {103129},
year = {2025},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2024.103129},
url = {https://www.sciencedirect.com/science/article/pii/S2215016124005806},
author = {Cameron Frederick Atkinson},
keywords = {Systematic literature review, Automation, Scopus, Web of science, Google, LDA Topic Modelling, Python},
abstract = {Researchers today face significant challenges reshaping the landscape of academic, government, and industry research due to the exponential growth of global research outputs and the advent of Generative Artificial Intelligence (GenAI). The annual increase in published works has made it difficult for traditional literature review and data analysis methods to keep pace, often rendering reviews outdated by the time of publication. In response, this methods article introduces a suite of new tools designed to automate a number of stages for systematic literature reviews. Designated SPARK (Systematic Processing and Automated Review Kit), the new computational-based approaches presented in this article automate the collection, organisation, and filtering of journal articles, alongside a data extraction scaffolding technique, for use in a systematic literature review on trauma-informed policing. As global research outputs rise, so does the need for automated methods. This paper highlights how these methods can enhance research efficiency and impact.•Hard-coded tools can be utilised to automate research.•Hard-coded tools do not carry the dangers of ‘hallucinations’ that GenAI infused tools may.•Hard-coded automation tools allow researchers to keep up to date with contemporary research outputs while maintaining a high level of control in the research process.}
}
@article{GONZALEZ2024941,
title = {ChatGPT: What Every Pediatric Surgeon Should Know About Its Potential Uses and Pitfalls},
journal = {Journal of Pediatric Surgery},
volume = {59},
number = {5},
pages = {941-947},
year = {2024},
issn = {0022-3468},
doi = {https://doi.org/10.1016/j.jpedsurg.2024.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022346824000101},
author = {Raquel González and Dan Poenaru and Russell Woo and A Francois Trappey and Stewart Carter and David Darcy and Ellen Encisco and Brian Gulack and Doug Miniati and Edzhem Tombash and Eunice Y. Huang},
keywords = {ChatGPT, Artificial intelligence, Natural language processing, Large language models, Pediatric surgery},
abstract = {ChatGPT - currently the most popular generative artificial intelligence system - has been revolutionizing the world and healthcare since its release in November 2022. ChatGPT is a conversational chatbot that uses machine learning algorithms to enhance its replies based on user interactions and is a part of a broader effort to develop natural language processing that can assist people in their daily lives by understanding and responding to human language in a useful and engaging way. Thus far, many potential applications within healthcare have been described, despite its relatively recent release. This manuscript offers the pediatric surgical community a primer on this new technology and discusses some initial observations about its potential uses and pitfalls. Moreover, it introduces the perspectives of medical journals and surgical societies regarding the use of this artificial intelligence chatbot. As ChatGPT and other large language models continue to evolve, it is the responsibility of the pediatric surgery community to stay abreast of these changes and play an active role in safely incorporating them into our field for the benefit of our patients.
Level of Evidence
V.}
}
@article{HUWER2025100303,
title = {Competencies for teaching with and about artificial intelligence in the natural sciences — DiKoLAN AI},
journal = {Computers and Education Open},
volume = {9},
pages = {100303},
year = {2025},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2025.100303},
url = {https://www.sciencedirect.com/science/article/pii/S266655732500062X},
author = {Johannes Huwer and Christoph Thyssen and Sebastian Becker-Genschow and Lena {von Kotzebue} and Alexander Finger and Erik Kremser and Sandra Berber and Mathea Brückner and Nikolai Maurer and Till Bruckermann and Monique Meier and Lars-Jochen Thoms},
keywords = {Artificial intelligence, STEM education, Digital competencies, AI literacy, TPACK, Preservice teachers},
abstract = {The rapid advancement and widespread adoption of digital technologies have transformed the education sector. Among these developments, the emergence of generative artificial intelligence (AI) tools such as ChatGPT has had a considerable impact on teaching and learning practices. While the integration of AI into educational settings is becoming increasingly common, subject-specific analyses, especially in STEM education, are still lacking. This paper examines the specific challenges and potential of AI in the context of STEM education. It does so by exploring how AI has transformed scientific disciplines and how these changes impact teaching and learning. It highlights the necessity for educators to acquire specific competencies to effectively incorporate AI into their instructional practices. Building on existing frameworks such as DigCompEdu and the subject-specific DiKoLAN, the paper proposes an AI-focused framework: DiKoLAN AI. This framework aligns AI-related teacher competencies with instructional practice in science education. It also provides a structure for categorizing existing teacher training programs. The paper outlines the development of the DiKoLAN AI framework and its content consensus validation by a total of 64 experts through three iterative cycles. Its practical application is demonstrated through 20 case studies from different authors, which offer a practical approach for supporting teacher training and curriculum design in AI-integrated STEM education. The paper concludes with a discussion of opportunities, challenges and future research needs for teacher professionalization.}
}
@article{DECARDINELSON2024108723,
title = {Generative AI and process systems engineering: The next frontier},
journal = {Computers & Chemical Engineering},
volume = {187},
pages = {108723},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108723},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424001418},
author = {Benjamin Decardi-Nelson and Abdulelah S. Alshehri and Akshay Ajagekar and Fengqi You},
keywords = {Generative AI, Process systems engineering, Large language models, Multiscale},
abstract = {This review article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could potentially advance PSE methodologies, providing insights and prospects for each area. Furthermore, the article identifies and discusses potential challenges in fully leveraging GenAI within PSE, including multiscale modeling, data requirements, evaluation metrics and benchmarks, and trust and safety, thereby deepening the discourse on effective GenAI integration into systems analysis, design, optimization, operations, monitoring, and control. This paper provides a guide for future research focused on the applications of emerging GenAI in PSE.}
}
@article{WEBB2025105835,
title = {Advancing qualitative analysis in professional disaster and risk communication: A comparative study of an OpenAI ChatGPT 3.5 model-enabled method for processing complex public discourse},
journal = {International Journal of Disaster Risk Reduction},
volume = {130},
pages = {105835},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105835},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925006594},
author = {Margaret Webb and Harman Singh and Rachel Inman and Sweta Baniya and Andrew Katz},
keywords = {Social media discourse analysis, Natural language processing, GPT-enabled qualitative analysis, Crisis communication, COVID-19, Climate change, Compounding crisis},
abstract = {Crisis managers and risk communicators face increasing challenges analyzing social media discourse during interconnected crises. This paper introduces an end-to-end method for qualitative codebook generation using the ChatGPT-3.5 Generative Artificial Intelligence (GenAI) Large Language Model (LLM) and Generative Pre-trained Transformer (GPT) from OpenAI, comparing a human-in-the-loop GenAI-enabled approach with traditional qualitative coding through a case study of Twitter discourse on COVID-19 and climate change in Virginia (2020–2022). Methods build on prior work establishing qualitative analysis enhanced by Natural Language Processing (NLP) by stacking multiple iterative coding processes with GenAI and humans-in-the-loop to generate qualitative codebooks. A comparative analysis establishes recommendations for assessing the validity of the GPT-enabled generated codebooks. Human validation confirmed substantial concordance (91.7 % agreement) with the GPT-enabled process's coding, revealing structural similarities and distinct patterns between the two approaches' representation of analysis. This research's contribution is methodological, establishing an approach for conducting and assessing GPT-enabled qualitative analysis while mapping relationships between computationally enhanced and traditional qualitative coding approaches. Findings advance risk communication methods by offering empirical guidance on integrating AI-assisted techniques within traditional qualitative research to maintain analytical rigor. Compared to prior NLP-enabled methods for disaster-related social media discourse analysis, this iterative approach mimics aspects of traditional qualitative codebook development, yielding multi-tiered codebook structures that can capture aspects of complex disaster discourse. While findings demonstrate that GenAI can enable analytical efficiency of traditional (human-only) codebook generation through self-assessment and iterative improvement with the help of humans-in-the-loop, they also illuminate areas of coding process where human expertise remains essential.}
}
@article{HUANG2025101336,
title = {Harnessing large multimodal models in pulmonary CT: the generative AI edge in lung cancer diagnostics},
journal = {The Lancet Regional Health - Western Pacific},
volume = {55},
pages = {101336},
year = {2025},
issn = {2666-6065},
doi = {https://doi.org/10.1016/j.lanwpc.2024.101336},
url = {https://www.sciencedirect.com/science/article/pii/S2666606524003304},
author = {Lihaoyun Huang and Junyi Shen and Anqi Lin and Jian Zhang and Peng Luo and Ting Wei},
abstract = {Background
Generative Artificial Intelligence (Gen-AI) has rapidly advanced in multimodal information processing, particularly in medical applications such as the refinement of instruments and interpretation of medical images. However, limited evidence exists on the diagnostic performance of Gen-AI models in tumor recognition, particularly using computed tomography (CT) images. This study aimed to evaluate the diagnostic capabilities of several prevelant Gen-AI models (GPT-4-turbo, Gemini-pro-vision, Claude-3-opus) in the context of lung CT image analysis.
Methods
This retrospective study analyzed chest CT scans from 404 patients with lung conditions with lung neoplasms (n=184) and non-malignancy (n=210). After standardizing CT images, the diagnostic performance and reliability of three Gen-AI (GPT-4-turbo, Gemini-pro-vision, and Claude-3-opus) were assessed using chi-square tests and Receiver Operating Characteristic (ROC) curves across various clinical scenarios. Likert scale scoring and response rate analysis were employed to evaluate internal diagnostic tendencies, while regression analyses were conducted for model optimization.
Findings
In a cueing environment limited to a single CT image, Gemini demonstrated the highest diagnostic accuracy (92.21%), followed by Claude (91.49%), while GPT exhibited the lowest performance (65.22%). As the complexity of the cueing environment increased, all models experienced a decline in diagnostic accuracy. Claude showed a marginal decrease, whereas Gemini's accuracy fluctuated significantly. Under simplified cueing conditions, the performance of all models improved notably (Gemini AUC = 0.76, Claude AUC = 0.69, GPT AUC = 0.73). Feature identification analysis revealed that Claude and GPT excelled in recognizing key features, particularly prioritizing “Morphology/Margins” when diagnosing primary malignancies, with “spiculated” and “irregular” serving as critical indicators. However, in cases of misdiagnosis or missed diagnoses, Gen-AI exhibited significant deviations across multiple feature dimensions—some even completely contradicted the actual findings. Following optimization through Lasso and stepwise regression, the diagnostic performance of the models was significantly enhanced (AUC = 0.896 and AUC = 0.894, respectively).
Interpretation
Gen-AI shows promising potential in pulmonary CT imaging, particularly in simplified diagnostic settings. However, their limitations in processing complex multi-modal information highlight significant challenges for clinical integration. Ongoing efforts to improve the robustness and reliability of these models are crucial for their successful adoption in healthcare.}
}
@article{KIRK2025114984,
title = {The AI-authorship effect: Understanding authenticity, moral disgust, and consumer responses to AI-generated marketing communications},
journal = {Journal of Business Research},
volume = {186},
pages = {114984},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.114984},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324004880},
author = {Colleen P. Kirk and Julian Givi},
keywords = {Artificial intelligence, Generative AI, Moral disgust, Authenticity, ChatGPT, Large language models},
abstract = {Seven preregistered experiments demonstrate that when consumers believe emotional marketing communications are written by an AI (vs. a human), positive word of mouth and customer loyalty are reduced. Drawing from authenticity theory, we show that this “AI-authorship effect” is attenuated for factual (vs. emotional) messages (Study 2); when an AI only edits the communication (Study 3); when a communication is signed directly by an AI (Study 4); and when consumers believe that most marketing communications are written by AI (Study WA1). Importantly, when consumers believe a communication is reused (i.e., not originally written by the sender), the effect is reversed (Study 6). This “AI-authorship effect” is serially mediated by perceived authenticity (Studies 5 and 6) and moral disgust (Studies 1–6 and WA1). These findings are evidenced using both personalized and mass communications, different emotions, businesses and organizational employees, and both hypothetical and behavioral measures.}
}
@article{ZAFAR2025100802,
title = {Reimagining human creativity and learning in the age of generative AI: A multi-method meta-thematic synthesis},
journal = {Next Research},
volume = {2},
number = {4},
pages = {100802},
year = {2025},
issn = {3050-4759},
doi = {https://doi.org/10.1016/j.nexres.2025.100802},
url = {https://www.sciencedirect.com/science/article/pii/S3050475925006694},
author = {Muhammad Bilal Zafar and Hassnian Ali and Talha Yasin},
keywords = {Generative ai, Human creativity, Structural topic modeling, Co-creation, Prompt literacy, Distributed agency, Cognitive augmentation, Creative labor, Ai ethics, Information systems},
abstract = {The rise of Generative Artificial Intelligence (GenAI) has catalyzed a profound epistemic shift in how creativity is conceptualized, practiced, and structured within socio-technical systems. As GenAI systems increasingly act not merely as facilitators but as co-creators, they blur traditional boundaries between human and machine agency, raising critical questions about originality, authorship, and creative labor. This paper systematically reviews and synthesizes 137 peer-reviewed articles (2023–2025) to map the evolving intersection of GenAI and human creativity, with a specific focus on implications for adaptive and personalized learning environments. Integrating Structural Topic Modeling (STM) with a meta-thematic synthesis, we identify twelve latent topics and five higher-order meta-themes: co-creation in design and aesthetics; GenAI-supported education and future skills; cognitive augmentation and creative reasoning; organizational knowledge work and market narratives; and ethical tensions surrounding copyright, bias, and representation. Our findings reveal that GenAI not only augments but transforms creativity, redistributing agency, reshaping evaluation criteria, and redefining the loci of value creation in socio-technical systems. These transformations carry significant implications for learner engagement, instructional design, and curriculum personalization. We advance five cross-cutting constructs including prompt literacy, distributed agency, ethical tensions, methodological gaps, and governance futures that collectively reframe creativity and learning in the GenAI era. The paper culminates in a conceptual framework and a future-oriented research agenda, offering theoretically generative and practically actionable insights for educators, designers, researchers, and policymakers seeking to harness GenAI for adaptive and personalized education.}
}
@article{RAJAGOPAL2025151673,
title = {Using Predictive Models and AI for AKI Research},
journal = {Seminars in Nephrology},
pages = {151673},
year = {2025},
issn = {0270-9295},
doi = {https://doi.org/10.1016/j.semnephrol.2025.151673},
url = {https://www.sciencedirect.com/science/article/pii/S0270929525001512},
author = {Madhumitha Rajagopal and Lili Chan and Girish N. Nadkarni},
keywords = {Acute kidney injury, machine learning, risk-stratification, generative AI, natural language processing},
abstract = {Summary
Acute kidney injury (AKI), a drop in kidney function with multiple etiologies, is a common complication in hospitalized patients and is associated with poorer patient outcomes. With the advent of electronic health records, machine learning algorithms have been developed that can predict the incidence and severity of AKI, AKI persistence, as well as patient outcomes like mortality and the need for kidney replacement therapies. Furthermore, it can risk-stratify patients based on early presentations to aid with clinical management. Newer technologies like natural language processing and generative artificial intelligence (AI) (e.g., ChatGPT) also show promise in the realm of AKI prediction and management. This review provides an overview of the role of AI in adults with AKI, as well as explores some limitations and ethical considerations that need to be addressed as we move forward. Semin Nephrol 36:x-xx © 20XX Elsevier Inc. All rights reserved.}
}
@article{CELIK2026105485,
title = {Co-constructing adaptive lesson plans with GenAI: Pre-service teachers' Intelligent-TPACK and prompt engineering strategies},
journal = {Computers & Education},
volume = {241},
pages = {105485},
year = {2026},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105485},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525002532},
author = {Ismail Celik and Sini Kontkanen and Jari Laru and Alanur Ahsen Dalyanci},
keywords = {Adaptive learning, Teacher education, Intelligent-TPACK, Prompt engineering},
abstract = {Generative Artificial Intelligence (GenAI) technologies present new opportunities for teachers to design adaptive and student-centered instruction. However, the educational value of GenAI depends not only on technical usage but also on teachers' ability to formulate pedagogically meaningful prompts. Prompting strategies are not isolated from teachers′ prior knowledge and skills. Less is known about how pre-service teachers′ AI-related knowledge influences prompt engineering strategies, in turn leading to meaningful adaptive lesson plans. Considering this gap, we design an instructional task for pre-service teachers to generate adaptive lesson plans with the help of GenAI. Prior to the task, we collected data about their AI-related skills, namely AI literacy and Intelligent-TPACK. The prompts were qualitatively analyzed based on the phases of the Knowledge Construction (KC) Framework. Then, we explored the pedagogical value of adaptive lesson plans through a rubric in terms of three indicators: student agency, adaptive strategies, and flexible tools. PLS-SEM analysis revealed that as long as pre-service teachers have AI-specific technological and pedagogical knowledge, they formulate higher phases of prompts based on the KC framework. Our analysis showed that prompts from higher phases generated more adaptive lesson plans in terms of student agency, adaptive strategies, and flexible tools. We also found an indirect effect of Intelligent-TPK on adaptive lesson plans. This study highlights that effective prompt engineering is a pedagogical act shaped by teachers’ knowledge, not merely a technical command. It also underscores the importance of embedding AI-specific pedagogical training in teacher education. By conceptualizing prompts as epistemic moves, we offer new insights into how teachers and GenAI can collaborate to produce responsive and inclusive learning experiences.}
}
@article{GARG2025100380,
title = {Enhancing data analysis and programming skills through structured prompt training: The impact of generative AI in engineering education},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100380},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100380},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000207},
author = {Ashish Garg and K. {Nisumba Soodhani} and Ramkumar Rajendran},
keywords = {Applications in subject areas, Post-secondary education, Teaching/learning strategies},
abstract = {The advent of Generative Artificial Intelligence (GenAI) and large language models like LLama, Palm2, GPT, Gemini, and Claude has revolutionized education by generating human-like text and contextually relevant responses. Our research investigates the impact of structured prompt training on students' learning in data analysis and programming. We experimented with 157 first-year engineering students divided into three groups: a control group (internet access, no GenAI), an experimental group 1 (internet and GenAI without prompt training), and an experimental group 2 (internet and GenAI with prompt training). The prompt training session included techniques like few-shot prompting, chain prompting, and the CLEAR framework. We assessed participants' performance in data analysis tasks using Python, with pre-tests and post-tests measuring their skills in programming across three Bloom's taxonomy levels (understanding, application, and analysis). ANOVA on post-test scores showed significant differences among the groups, with G3 (with prompt training) outperforming G2 (without prompt training) and the control group across all three levels, evidenced by higher mean scores (G3: 6.60, G2: 4.94, Control: 4.28), similar pattern observed in task completion also. These results underscore the effectiveness of structured prompt training in enhancing students' data analysis and programming skills. Our study highlights the potential of GenAI and structured prompt training to transform educational practices and suggests future research directions, including integrating prompt engineering within human-AI collaboration.}
}
@article{CHEN2023100184,
title = {Integrating generative AI in knowledge building},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100184},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100184},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000632},
author = {Bodong Chen and Xinran Zhu and Fernando {Díaz del Castillo H.}},
keywords = {Generative AI, ChatGPT, Knowledge building, Human-AI partnership},
abstract = {Generative artificial intelligence (GenAI) is penetrating in various social sectors, motivating a strong need for teaching AI literacy in younger generations. While substantial efforts have been made to teach AI literacy and to use AI to facilitate learning, few studies have provided empirical accounts of students' nuanced processes of using GenAI for learning. In this study, we engaged a group of high school students in leveraging ChatGPT to support their knowledge building efforts. Following the teacher's pedagogical design, students used ChatGPT for a range of distinct purposes. Student interviews showed detailed processes of using ChatGPT for knowledge building and students' emerging AI literacy in multiple dimensions. This study offers practical implications for the integration of GenAI in K-12 education and urges educators to create spaces and scaffolds for students to mindfully engage with GenAI in the classroom.}
}
@article{LIAN2025108645,
title = {Using a two-stage method to understand the critical factors influencing customers’ intention to switch from traditional to artificial intelligence based banking services: A perspective based on the push–pull–mooring model},
journal = {Computers in Human Behavior},
volume = {168},
pages = {108645},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108645},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225000925},
author = {Jiunn-Woei Lian and Cai-Wei Li},
keywords = {Generative artificial intelligence (GenAI), Customer service, Push–pull–mooring (PPM) model, Switching intention, Banking},
abstract = {Purpose
Generative artificial intelligence (GenAI) enables banks to enhance customer service experiences. However, limited research has been conducted on factors influencing customers’ intention to switch from traditional to artificial intelligence (AI) based banking services. Therefore, the present study explored the key antecedents of the aforementioned intention.
Methods
This two-stage study was based on the push–pull–mooring model. In Stage 1, in-depth semistructured interviews were conducted with stakeholders related to AI-based banking services to identify critical factors influencing customers’ intention to switch from traditional to AI-based customer services. In Stage 2, the results obtained in Stage 1 were combined with results from the literature to create a second-order model and direct-effect model (Models 1 and 2, respectively). Quantitative survey data were then collected to validate these models.
Results
Model 1 indicated that among pull, push, and mooring factors, pull factors had the strongest effect on the aforementioned intention, followed by mooring factors and then push factors. The explanatory power (R2) of this model was 70 %. Furthermore, Model 2 indicated that the attractiveness of the alternative (a pull factor) and the need for interpersonal interaction and inertia (mooring factors) were the key factors influencing switching intention. The explanatory power (R2) of this model was 76 %.
Conclusion
In summary, this study identified and validated critical factors affecting customers’ intention to switch to AI-based banking services. The findings enrich the understanding the social interaction and user behavior of AI, offering valuable insights for promoting AI-driven services and applications.}
}
@article{BALTASALVADOR2026101958,
title = {Evaluating AI-assisted creative ideation: A crossover study in higher education},
journal = {Thinking Skills and Creativity},
volume = {59},
pages = {101958},
year = {2026},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2025.101958},
url = {https://www.sciencedirect.com/science/article/pii/S187118712500207X},
author = {Rosó Baltà-Salvador and Enric Brasó-Vives and Marta Peña},
keywords = {Co-creativity, Engineering education, Generative artificial intelligence, Higher Education, Human-AI Interaction},
abstract = {As artificial intelligence (AI) becomes increasingly integrated into educational contexts, its impact on students’ creative thinking remains unclear. Given the critical role of creativity in engineering and design education, understanding how AI tools shape students’ ideation processes is essential for developing effective pedagogical practices. This study examines the impact of generative AI, specifically ChatGPT, on creative ideation among undergraduate design engineering students. The research was conducted through a randomised crossover experiment, with students alternating between AI-assisted and unaided ideation tasks. A mixed-methods approach combined quantitative analyses of the 728 ideas generated with a qualitative evaluation of students’ interactions with AI. Results show that the use of AI did not reduce fluency, flexibility, or originality, nor did it lead to thematic homogenisation. However, semantic divergence was significantly lower in the AI-assisted condition, suggesting convergence in the way ideas were formulated. Additionally, AI-assisted ideas more frequently resembled existing products and exhibited reduced textual elaboration. A mixed between-within subjects ANOVA revealed that students who began with AI support produced more original and diverse ideas across both tasks, pointing to a lasting effect of early AI use. Qualitative analysis of student-AI interactions revealed important patterns, including predominantly passive and directive use, with limited exploratory or collaborative engagement. These findings provide new insights into human-AI co-creation and highlight the importance of promoting intentional, critical, and pedagogically guided use of generative AI tools in education.}
}
@article{ROSATI2025106207,
title = {The future of the movie industry in the wake of generative AI: A perspective under EU and UK copyright law},
journal = {Computer Law & Security Review},
volume = {59},
pages = {106207},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106207},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000793},
author = {Eleonora Rosati},
keywords = {Copyright, Artificial intelligence, Movie industry, LLM training, Authorship, Originality, Liability, Styles, Text and data mining, Exceptions},
abstract = {Executive summary
Like all sectors, the movie industry has been both affected by and exploring potential uses of generative Artificial Intelligence ('AI'). On the one hand, movie studios have detected and begun to add warnings against unlicensed third-party uses of their content, including for AI training,11W Cho, ‘Universal Pictures to Big Tech: We’ll sue if you steal our movies for AI’ (6 August 2025) The Hollywood Reporter, available at https://www.hollywoodreporter.com/business/business-news/universal-pictures-big-tech-well-sue-if-you-steal-movies-ai-1236337712/. and have taken enforcement initiatives through court action. On the other hand, the use of AI within and by the industry itself has been growing. Regarding the latter, some have emphasised the opportunities presented by the implementation of AI, including by advancing claims that AI tools can offer a `purer' form of expression. Others have instead warned against the potential displacement of industry workers, including workers employed in technical roles and younger and emerging actors. Against the background illustrated above, this study maps and critically evaluates relevant issues facing the development, deployment, and use of AI models from a movie industry perspective. The legal analysis is conducted having regard to EU and UK copyright law and is divided into three parts:•Input/AI training: By considering relevant legal restrictions applicable to the training of AI models on protected audiovisual content, the border between lawful unlicensed uses and restricted uses is drawn;•Protectability of AI-generated outputs: Turning to the output generation phase, the protectability of such outputs is considered next, by focusing in particular on the requirements of authorship and originality under EU and UK copyright law;•Legal risks and potential liability stemming from the use of third-party AI models for output generation: Still having regard to the output generation phase, relevant legal issues that might arise having regard to the use of AI models that `regurgitate' third-party training data at output generation are considered, alongside the question of style protection under copyright. The main conclusions are as follows:•Input/AI training: Insofar as model training on third-party protected content is concerned, there are no exceptions under EU/UK law that fully cover the entirety of these processes. As a result, lacking legislative reform, the establishment of a licensing framework appears unavoidable for such activities to be deemed lawful;•Protectability of AI-generated outputs: The deployment of AI across various phases of the creative process does not render the resulting content unprotectable, provided that human involvement and control remain significant throughout, with the result that AI is relied upon as a tool that aids – rather than replaces – the creativity of industry workers.•Legal risks and potential liability stemming from the use of third-party AI models for output generation: The use of AI models that generate infringing outputs, such as by regurgitating input data or merely imitating style, may trigger the application of exclusive rights under copyright and related rights. The resulting liability may vest with the user of such models, as well as the model developer/provider. The latter aspect means that terms that exclude any such liability may ultimately be found to be unenforceable against users and ineffective against rightholders.}
}
@article{ALHASHIMI2025100509,
title = {Exploring the role of generative AI in enhancing cybersecurity in software development life cycle},
journal = {Array},
volume = {28},
pages = {100509},
year = {2025},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2025.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2590005625001365},
author = {Hussein A. Al-Hashimi and Rafiq Ahmad Khan and Hathal S. Alwageed and Asaad M. Algarni and Sarra Ayouni and Alaa Omran Almagrabi},
keywords = {Generative artificial intelligence, Cybersecurity, Software development lifecycle, Risks and practices, Systematic literature review, Survey},
abstract = {Context
The rapid integration of Generative AI (GenAI) technologies in various sectors has introduced new opportunities and challenges. One of the areas where GenAI is gaining prominence is cybersecurity, particularly within the Software Development Life Cycle (SDLC). As cyber threats evolve, there is a growing need to explore innovative solutions to mitigate vulnerabilities during software development.
Objectives
This study investigates the role of GenAI in enhancing cybersecurity in the SDLC. It examines current security practices, recent advancements in AI-driven security solutions, and the potential of GenAI to strengthen threat detection, vulnerability management, and risk mitigation. Additionally, the research identifies key opportunities and challenges associated with integrating GenAI into SDLC processes, highlighting its implications for secure software development and future industry practices.
Methods
This research employs a mixed-methods approach to investigate the role of GenAI in cybersecurity. Specifically, it combines a Systematic Literature Review (SLR) with questionnaire-based data collection targeting software development and cyber defense experts. The SLR aims to identify prevailing themes and gaps, while the questionnaire gathers insights from IT professionals about their experiences and perspectives on GenAI systems.
Results
Our research shows that GenAI technology enhances SDLC security by supporting development through vulnerability detection, threat modeling, secure coding practices, and incident response. However, our review shows that AI adoption introduces ethical risks alongside reliability issues with AI-created results and challenges to integrate it into standard development methods.
Conclusion
The integration of GenAI into the SDLC offers significant potential for enhancing cybersecurity. While challenges such as algorithm transparency and the need for skilled professionals remain, the benefits of AI in proactive threat detection and response make it a promising tool for future cybersecurity strategies in software development.}
}