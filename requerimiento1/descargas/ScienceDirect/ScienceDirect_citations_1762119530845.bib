@article{THAKKAR2024110452,
title = {Advances in materials and machine learning techniques for energy storage devices: A comprehensive review},
journal = {Journal of Energy Storage},
volume = {81},
pages = {110452},
year = {2024},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2024.110452},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X24000379},
author = {Prit Thakkar and Sachi Khatri and Drashti Dobariya and Darpan Patel and Bishwajit Dey and Alok Kumar Singh},
keywords = {Material science, Machine learning, Battery, Supercapacitor, Energy technology},
abstract = {The increasing global need for energy supply in modern society has created a pressing need to explore new materials for renewable energy technologies. However, conventional trial and error methods in materials science are often tedious as well as costly, making it challenging to meet the growing demands. In recent years, machine learning (ML) become a prominent research strategy transfigure the discovery of materials. This review offers a concise summary of the elementary ML procedures and widely used algorithms in the field of materials science. It particularly emphasizes the latest advancements in utilizing ML for predicting material properties and developing materials for energy-related fields like Li-Ion batteries, Super-Capacitors, and Hybrid Systems. Furthermore, the review discusses the contributions of ML to experimental research. This review intents to serve as a guiding resource for future developments of ML in materials science.}
}
@article{NAWN2024128998,
title = {The distal-proximal relationships among the human moonlighting proteins: Evolutionary hotspots and Darwinian checkpoints},
journal = {International Journal of Biological Macromolecules},
volume = {259},
pages = {128998},
year = {2024},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2023.128998},
url = {https://www.sciencedirect.com/science/article/pii/S014181302305897X},
author = {Debaleena Nawn and Sk. Sarif Hassan and Moumita Sil and Ankita Ghosh and Arunava Goswami and Pallab Basu and Guy W. Dayhoff and Kenneth Lundstrom and Vladimir N. Uversky},
keywords = {Moonlighting proteins, Phylogeny, Distal-proximal, Promiscuous proteins, Signature-features},
abstract = {Moonlighting proteins, known for their ability to perform multiple, often unrelated functions within a single polypeptide chain, challenge the traditional “one gene, one protein, one function” paradigm. As organisms evolved, their genomes remained relatively stable in size, but the introduction of post-translational modifications and sub-strategies like protein promiscuity and intrinsic disorder enabled multifunctionality. Enzymes, in particular, exemplify this phenomenon, engaging in unrelated processes alongside their primary catalytic roles. This study employs a systematic, quantitative informatics approach to shed light on human moonlighting protein sequences. Phylogenetic analyses of human moonlighting proteins are presented, elucidating the distal-proximal relationships among these proteins based on sequence-derived quantitative features. The findings unveil the captivating world of human moonlighting proteins, urging further investigations in the emerging field of moonlighting proteomics, with the potential for significant contributions to our understanding of multifunctional proteins and their roles in diverse cellular processes and diseases.}
}
@article{VERAAMARO2025107821,
title = {Towards accessible website design through artificial intelligence: A systematic literature review},
journal = {Information and Software Technology},
volume = {186},
pages = {107821},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107821},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925001600},
author = {Guillermo Vera-Amaro and José Rafael Rojano-Cáceres},
keywords = {Web accessibility, Systematic literature review, Artificial intelligence, Wcag, Machine learning, Large language models},
abstract = {Context:
Web accessibility ensures that individuals with disabilities can access, navigate, and interact with online content effectively. Despite the availability of the Web Content Accessibility Guidelines (WCAG), significant barriers persist, largely due to the complexity of their implementation. Artificial intelligence (AI), particularly machine learning models, has emerged as a promising avenue to address these challenges, offering solutions for evaluation, correction, and content generation.
Objective:
This study aims to systematically review the intersection of web accessibility and AI by evaluating how AI-based methods enhance compliance with accessibility standards over the period 2019–2025, assessing their efficacy and alignment with WCAG principles.
Methods:
A systematic literature review (SLR) was conducted in three phases: planning, execution, and reporting. Research questions were formulated guiding the selection of search terms and strategies. A systematic search process was implemented, complemented by a snowballing technique to ensure comprehensive coverage of relevant studies. The quality of selected studies was rigorously assessed using predefined criteria, and data extraction was carried out following established best practices. The analysis combined narrative synthesis for qualitative insights and bibliometric techniques for quantitative evaluation.
Results:
From 277 studies, 31 were identified as relevant, highlighting AI’s primary contributions to generating alternative text for images, automating compliance assessments, providing correction suggestions, and designing alternative interfaces to enhance accessibility. Advances in large language models (LLMs) further demonstrate their potential for facilitating the creation of accessible content.
Conclusions:
AI presents significant potential to improve web accessibility by streamlining evaluation processes and supporting the creation of accessible content. However, further research is needed to address limitations such as inconsistent compliance and the lack of focus on non-visual disabilities. These findings underline the importance of leveraging AI to facilitate inclusive web design practices while ensuring adherence to accessibility standards.}
}
@article{RUNGRUANGJIT2024100523,
title = {The power of human-like virtual-influencer-generated content: Impact on consumers’ willingness to follow and purchase intentions},
journal = {Computers in Human Behavior Reports},
volume = {16},
pages = {100523},
year = {2024},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100523},
url = {https://www.sciencedirect.com/science/article/pii/S2451958824001568},
author = {Warinrampai Rungruangjit and Kulachet Mongkol and Intaka Piriyakul and Kitti Charoenpornpanichkul},
keywords = {Artificial intelligence, Emotional content, Generated content, Uses and gratifications theory, Virtual influencer, Willingness to follow},
abstract = {The swift progress in machine learning algorithms, artificial intelligence, and interactive immersive media technologies has led to the introduction of computer-generated imagery on Instagram. This feature, so-called “human-like virtual influencers (VIs)", has revolutionized the way people interact with technology. Using a combination of cutting-edge AI technologies, in a novel application of computer vision algorithms, and large language models to extract the content posted by two popular human-like VIs on Instagram, the present study is the first to categorize and classify types of human-like virtual-influencer-generated content. Quantitative methods, such as partial least squares structural equation modeling (PLS-SEM), were used to examine the impact of human-like virtual-influencer-generated content on consumers' willingness to follow as well as purchase intentions. The information was gathered from 650 Thai customers. The findings showed that consumers' willingness to follow and purchase intentions were significantly influenced by the positive effects of emotional appeal content, which includes relational, entertaining, positive emotion, and negative emotion content. These effects outweighed those of rational appeal content, such as informative and remunerative content, as well as authenticity appeal content. Meanwhile, disclosing sponsored content had no effect on consumers' willingness to follow. The theoretical underpinnings of uses and gratifications (U&G) theory, parasocial relationships and Richins' hierarchical model of emotions are confirmed and expanded upon in this work, and the suggested inclusive approach also significantly advances the expanding corpus of research on VIs. Our research also provides a contribution to the recent literature on human-like VI marketing.}
}
@article{ZARETSKY2024636,
title = {The Psychedelic Future of Post-Traumatic Stress Disorder Treatment},
journal = {Current Neuropharmacology},
volume = {22},
number = {4},
pages = {636-735},
year = {2024},
issn = {1570-159X},
doi = {https://doi.org/10.2174/1570159X22666231027111147},
url = {https://www.sciencedirect.com/science/article/pii/S1570159X24000301},
author = {Tamar Glatman Zaretsky and Kathleen M. Jagodnik and Robert Barsic and Josimar Hernandez Antonio and Philip A. Bonanno and Carolyn MacLeod and Charlotte Pierce and Hunter Carney and Morgan T. Morrison and Charles Saylor and George Danias and Lauren Lepow and Rachel Yehuda},
keywords = {3,4-methylenedioxymethamphetamine (MDMA), Ayahuasca, Ketamine, Lysergic Acid Diethylamide (LSD), Post-Traumatic Stress Disorder (PTSD), psilocybin, psychedelics, trauma},
abstract = {Post-traumatic stress disorder (PTSD) is a mental health condition that can occur following exposure to a traumatic experience. An estimated 12 million U.S. adults are presently affected by this disorder. Current treatments include psychological therapies (e.g., exposure-based interventions) and pharmacological treatments (e.g., selective serotonin reuptake inhibitors (SSRIs)). However, a significant proportion of patients receiving standard-of-care therapies for PTSD remain symptomatic, and new approaches for this and other trauma-related mental health conditions are greatly needed. Psychedelic compounds that alter cognition, perception, and mood are currently being examined for their efficacy in treating PTSD despite their current status as Drug Enforcement Administration (DEA)-scheduled substances. Initial clinical trials have demonstrated the potential value of psychedelic-assisted therapy to treat PTSD and other psychiatric disorders. In this comprehensive review, we summarize the state of the science of PTSD clinical care, including current treatments and their shortcomings. We review clinical studies of psychedelic interventions to treat PTSD, trauma-related disorders, and common comorbidities. The classic psychedelics psilocybin, lysergic acid diethylamide (LSD), and N,N-dimethyltryptamine (DMT) and DMT-containing ayahuasca, as well as the entactogen 3,4-methylenedioxymethamphetamine (MDMA) and the dissociative anesthetic ketamine, are reviewed. For each drug, we present the history of use, psychological and somatic effects, pharmacology, and safety profile. The rationale and proposed mechanisms for use in treating PTSD and trauma-related disorders are discussed. This review concludes with an in-depth consideration of future directions for the psychiatric applications of psychedelics to maximize therapeutic benefit and minimize risk in individuals and communities impacted by trauma-related conditions.}
}
@article{GIAMATTEI2025107599,
title = {Causal reasoning in Software Quality Assurance: A systematic review},
journal = {Information and Software Technology},
volume = {178},
pages = {107599},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107599},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002040},
author = {Luca Giamattei and Antonio Guerriero and Roberto Pietrantuono and Stefano Russo},
keywords = {Causal reasoning, Causal discovery, Causal inference, Software quality},
abstract = {Context:
Software Quality Assurance (SQA) is a fundamental part of software engineering to ensure stakeholders that software products work as expected after release in operation. Machine Learning (ML) has proven to be able to boost SQA activities and contribute to the development of quality software systems. In this context, Causal Reasoning is gaining increasing interest as a methodology to go beyond a purely data-driven approach by exploiting the use of causality for more effective SQA strategies.
Objective:
Provide a broad and detailed overview of the use of causal reasoning for SQA activities, in order to support researchers to access this research field, identifying room for application, main challenges and research opportunities.
Methods:
A systematic review of the scientific literature on causal reasoning for SQA. The study has found, classified, and analyzed 86 articles, according to established guidelines for software engineering secondary studies.
Results:
Results highlight the primary areas within SQA where causal reasoning has been applied, the predominant methodologies used, and the level of maturity of the proposed solutions. Fault localization is the activity where causal reasoning is more exploited, especially in the web services/microservices domain, but other tasks like testing are rapidly gaining popularity. Both causal inference and causal discovery are exploited, with the Pearl’s graphical formulation of causality being preferred, likely due to its intuitiveness. Tools to favor their application are appearing at a fast pace — most of them after 2021.
Conclusions:
The findings show that causal reasoning is a valuable means for SQA tasks with respect to multiple quality attributes, especially during V&V, evolution and maintenance to ensure reliability, while it is not yet fully exploited for phases like requirements engineering and design. We give a picture of the current landscape, pointing out exciting possibilities for future research.}
}
@article{MUHAMMAD2025216602,
title = {Hemicyanine-based fluorescent probes: Advancements in biomedical sensing and activity-based detection},
journal = {Coordination Chemistry Reviews},
volume = {534},
pages = {216602},
year = {2025},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2025.216602},
url = {https://www.sciencedirect.com/science/article/pii/S0010854525001729},
author = {Sibtain Muhammad and Haroon Ahmad and Yuqian Yan and Xin Chen and Saz Muhammad and Madappa C. Maridevaru and Shubham Roy and Zun Wang and Yinghe Zhang and Bing Guo},
keywords = {Hemicyanine dyes, NIR fluorescence probes, Biomarker detection, Non-intrusive, Imaging, Polymethine dyes},
abstract = {Near-infrared (NIR) fluorescent probes have revolutionized in vivo imaging by enabling the real-time detection of biomarkers for disease diagnosis and drug screening. Among these, donor-π-acceptor (D-π-A) architecture of hemicyanine dyes have gained attention for their high fluorescence quantum yield, ease in structural versatility, and good capability to construct activatable probes. Hemicyanine-based NIR activatable probes (HNAPs) leverage intramolecular charge transfer (ICT) mechanisms for precise, biomarker-responsive fluorescence activation, making them invaluable tools in preclinical and clinical research. This review highlights the principles of molecular design and applications of HNAPs for detecting key biomarkers associated with several diseases, including as skin conditions, digestive problems, cancer, inflammation, and acute organ failure. These probes enable real-time imaging with high specificity, addressing critical clinical challenges in early diagnosis and monitoring disease progression. Moreover, the translational potential of HNAPs lies in their capacity for non-invasive imaging and targeted biomarker detection, paving the way for innovations in imaging-guided diagnosis and treatment strategies. The versatility of hemicyanine scaffolds and their ability to be tailored for diverse applications underscore their unique value in bioimaging. This progress emphasizes the transformative potential of HNAPs in advancing precision diagnostics and improving clinical outcomes. We hope this review would stimulate interest among wide researchers and expedite the clinic translation for HNAPs.}
}
@article{ROSSEEL2025109992,
title = {A state-of-the-art review on acoustic preservation of historical worship spaces through auralization},
journal = {Signal Processing},
volume = {234},
pages = {109992},
year = {2025},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2025.109992},
url = {https://www.sciencedirect.com/science/article/pii/S0165168425001069},
author = {Hannes Rosseel and Toon {van Waterschoot}},
keywords = {Acoustic preservation, Historical worship spaces, Room acoustics, Auralization},
abstract = {Historical Worship Spaces (HWS) are significant architectural landmarks which hold both cultural and spiritual value. The acoustic properties of these spaces play a crucial role in historical and contemporary religious liturgies, rituals, and ceremonies, as well as in the performance of sacred music. However, the original acoustic characteristics of these spaces are often at risk due to repurposing, renovations, natural disasters, or deterioration over time. This paper presents a comprehensive review of the current state of research on the acquisition, analysis, and synthesis of acoustics, with a focus on HWS. An example case study of the Nassau chapel in Brussels, Belgium, is presented to demonstrate the application of these techniques for the preservation and auralization of historical worship space acoustics. The paper concludes with a discussion of the challenges and opportunities in the field, and outlines future research directions.}
}
@article{CASPRINI2024100621,
title = {Untangling the yarn: A contextualization of human resource management to the family firm setting},
journal = {Journal of Family Business Strategy},
volume = {15},
number = {3},
pages = {100621},
year = {2024},
issn = {1877-8585},
doi = {https://doi.org/10.1016/j.jfbs.2024.100621},
url = {https://www.sciencedirect.com/science/article/pii/S1877858524000160},
author = {Elena Casprini and Rocco Palumbo and Alfredo {De Massis}},
keywords = {Family firm, Human resource management, Family resource management, Nonfamily resource management, People management},
abstract = {Despite the efforts to contextualize human resource management to family firms, scientific literature addressing this study domain suffers from limited systematization. The article arranges an integrative framework to make sense of the challenges faced by family firms in designing and implementing human resource management practices. Bibliographic coupling was run on an intellectual core of 69 papers to illuminate dominant research streams. Besides, co-citation was executed to determine the conceptual roots nurturing recent scholarly advancements. A dance between formality and informality of human resource management practices characterizes extant research, calling for developments to understand how family firms can deal with it.}
}
@article{2024I,
title = {Abstracts for SAR/RCMI PolyU International Research Conference},
journal = {Journal of Integrative Medicine},
volume = {22},
number = {3},
pages = {I-LXXVI},
year = {2024},
issn = {2095-4964},
doi = {https://doi.org/10.1016/S2095-4964(24)00328-5},
url = {https://www.sciencedirect.com/science/article/pii/S2095496424003285}
}
@article{2024100362,
title = {Pathology Visions 2023 Overview},
journal = {Journal of Pathology Informatics},
volume = {15},
pages = {100362},
year = {2024},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2024.100362},
url = {https://www.sciencedirect.com/science/article/pii/S2153353924000014}
}
@article{20251,
title = {ASGCT 28th Annual Meeting Abstracts},
journal = {Molecular Therapy},
volume = {33},
number = {4, Supplement 1},
pages = {1-1667},
year = {2025},
note = {28th Annual Meeting of the American Society of Gene & Cell Therapy},
issn = {1525-0016},
doi = {https://doi.org/10.1016/j.ymthe.2025.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S1525001625003028}
}
@article{PITAKASO2025102011,
title = {AI-driven cultural urbanism: A data-integrated model for learning city development in emerging heritage contexts},
journal = {Social Sciences & Humanities Open},
volume = {12},
pages = {102011},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.102011},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125007399},
author = {Rapeepan Pitakaso and Surajet Khonjan and Thanatkij Srichok and Natthapong Nanthasamroeng and Paweena Khampukka and Arunrat Sawettham and Sairoong Dinkoksung and Chawapot Supasarn and Kanya Jungvimutipan and Yong Boonarree and Ganokgarn Jirasirilerd and Pornpimol Mongkhonngam},
keywords = {Learning cities, Generative AI, Cultural heritage Design, Persona-based planning, Participatory urbanism},
abstract = {This study introduces an AI-enhanced framework for designing culturally rooted Learning Cities, demonstrated through Warinchamrap, Thailand—a heritage-rich secondary city overlooked by conventional urban development. Addressing gaps in smart city models that neglect symbolic landscapes, community narratives, and intergenerational learning, this research merges AI modeling with participatory cultural mapping and spatial analytics to create eight location-specific learning nodes.The methodology comprised five phases: (1) integrating multi-source data (archival, oral history, geospatial, social media); (2) applying AI clustering and natural language processing to identify user personas and spatial patterns; (3) employing generative AI (GPT-4, diffusion models) for culturally appropriate design concepts; (4) evaluating designs via a Design Suitability Score (DSS) framework combining AI metrics and stakeholder validation; and (5) refining architectural designs based on community feedback.Initial AI concepts achieved DSS scores averaging 0.866. After prompt refinement, version 2 designs improved to 0.908. Final architectural designs, informed by AI outputs, maintained strong community alignment with a DSS of 0.893. Projects like Songsarn x Rails of Memory and Warin Light Avenue successfully integrated spatial storytelling, cultural heritage, and adaptive learning design. Findings demonstrate that culturally embedded AI can serve as an effective co-designer for inclusive, memory-driven urban learning environments. This research provides a replicable framework for Learning Cities discourse, synthesizing generative AI, local heritage, and spatial intelligence. It offers valuable insights for developing culturally sustainable smart cities, adaptive tourism, and community-driven urban design, establishing AI as a responsive tool when anchored in cultural semantics and participatory engagement.}
}
@article{WANG2025116100,
title = {Progress in beamforming acoustic imaging based on phased microphone arrays: Algorithms and applications},
journal = {Measurement},
volume = {242},
pages = {116100},
year = {2025},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2024.116100},
url = {https://www.sciencedirect.com/science/article/pii/S0263224124019857},
author = {Yong Wang and Zhi Deng and Jiaxi Zhao and Victor Feliksovich Kopiev and Donglai Gao and Wen-Li Chen},
keywords = {Phased microphone arrays, Acoustic imaging, Beamforming, Source location, Time-delay estimation},
abstract = {Beamforming acoustic imaging technology, utilizing phased microphone arrays, enables precise sound source localization and finds widespread application in aerodynamic wind tunnel testing, acoustic signal recognition, and mechanical fault diagnosis. This paper presents a comprehensive review of beamforming evolution, detailing its mathematical foundations and diverse applications in acoustic imaging. Various beamforming methodologies are critically analyzed using wind tunnel test data, and an overview of correction methods for external interferences and array optimization approaches is provided. Through this examination, the strengths and limitations of each method are highlighted, offering insights for future research. Additionally, potential future enhancements, including paradigm-shift approaches to advance beamforming capabilities, are explored, suggesting directions for further innovation. This review aims to establish a foundation for newcomers to the field, stimulate academic discussion, and drive ongoing research in acoustic imaging. By elucidating beamforming complexities, correction methods, and optimization techniques, this study seeks to enhance collective knowledge and support continued advancements in this technology.}
}
@incollection{GAUR202625,
title = {Chapter 2 - Understanding training data and mitigating biases in training data},
editor = {Loveleen Gaur and Ajith Abraham},
booktitle = {Generative Artificial Intelligence and Ethics for Healthcare},
publisher = {Academic Press},
pages = {25-38},
year = {2026},
isbn = {978-0-443-33124-4},
doi = {https://doi.org/10.1016/B978-0-443-33124-4.00008-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443331244000084},
author = {Loveleen Gaur and Ajith Abraham},
keywords = {Bias mitigation, Data augmentation, Dataset diversity, Data preprocessing, Ethical ai development, Fairness in AI, Training data quality},
abstract = {Quality of data is becoming vital in the era of artificial intelligence (AI) and especially crucial in healthcare domain, where AI models facilitate jobs such as diagnosis, treatment planning, and patient monitoring. It displays numerous ethical catches, particularly the concern of “bias in training data.” This chapter investigates the intricacies of training data, the effects of biases in healthcare, and provides a listed inspection of peculiar ethical dilemmas appearing from these biases.}
}
@incollection{SAFARI2025,
title = {Blockchain innovations for transparent energy transactions},
booktitle = {Reference Module in Materials Science and Materials Engineering},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-12-803581-8},
doi = {https://doi.org/10.1016/B978-0-443-29210-1.00038-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292101000388},
author = {Ashkan Safari and Arman Oshnoei and Ahsan Nadeem and Frede Blaabjerg},
keywords = {Artificial intelligence, Blockchain technology, Smart grids, Transactive energy systems},
abstract = {Blockchain is among the key technologies in transforming trade by creating a secure, transparent, and decentralized platform for transactions. It eliminates intermediaries, reducing costs and increasing efficiency in supply chains, cross-border payments, and contract execution in transactive energy system (TES). By using distributed ledger systems, blockchain facilitates peer-to-peer (P2P) energy transactions, allowing prosumers to buy/sell excess energy, such as from renewable energy sources (RES), without intermediaries. To this end, this article presents a detailed explanation and different frameworks for blockchain-based TES markets in a complete perspective. Firstly, the concept of TES, its integration with distributed energy resource (DER), and different considered objective functions are presented. Then, the fundamentals of blockchain, its different types, algorithms, and each type's integration in TES are taken into account. Following this concept, the smart contracts structure, algorithms, as well as different hash function, their mathematical forms, and overall processes are exemplified. Therefore, different transaction logs of direct producer-consumer and marketplace are considered to provide the context for potential applications of artificial intelligence (AI). An extensive literature review of AI is conducted, and three example AI integrations in blockchain-based TES are manifested, as a selection for security-focused (federated learning [FL], dynamic-dependence [Deep Reinforcement Learning (DRL)], and data-dependence [long-short term memory (LSTM)]) models. Therefore, a discussion is provided upon the long-term sustainability goals and also TES alliance with sustainability standards. Finally, challenges and future works on security and user privacy aspects of blockchain-based TES are presented. The blockchain and AI integration in TES enhance grid resilience, promotes adoption, and empowers consumers, making TES markets more accessible and scalable for a sustainable energy future.}
}
@article{SHI2025126943,
title = {Ultra-short-term photovoltaic power prediction based on ground-based cloud images: A review},
journal = {Applied Energy},
volume = {402},
pages = {126943},
year = {2025},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2025.126943},
url = {https://www.sciencedirect.com/science/article/pii/S0306261925016733},
author = {Chaojun Shi and Xiaoyun Zhang and Ke Zhang and Xiongbin Xie and Qiaochu Lu and Ningxuan Zhang and Zibo Su},
keywords = {Ground-based cloud images, Cloud identification, Trajectory prediction, Ultra-short-term, Photovoltaic power prediction},
abstract = {Solar power is a key option for sustainable development and clean energy transition due to its advantages of renewability, almost zero carbon emissions, and flexibility in scale. However, the intermittent variation of solar radiation makes it a huge impact on power quality and power dispatch after large-scale access to the power grid. Photovoltaic (PV) power Prediction is the best way to solve this problem. In practical application, the ultra-short-term PV power prediction can not only improve the stability of PV power generation system, but also has a great reference value for the spot trading in the power market. Since cloud cover significantly and intermittently affects the amount of incident solar radiation, the high-resolution cloud images are helpful for detecting fine-grained cloud variations. Therefore, studying ultra-short-term PV power prediction based on ground-based cloud images is significant. This paper firstly introduces the ultra-short-term PV power prediction systems based on ground-based cloud images. Subsequently, it reviews four parts: ground-based cloud images acquisition, preprocessing, cloud identification and trajectory prediction, and PV power prediction. It also summarizes the ground-based cloud images acquisition device and existing available datasets, and conducts a comprehensive comparison and analysis of the methods involved in ground-based cloud image preprocessing, cloud cluster identification and trajectory prediction, and photovoltaic power prediction. Finally, it puts forward the outlook of the future technological research with respect to the current research status.}
}
@incollection{2026iv,
title = {Copyright},
editor = {Loveleen Gaur and Ajith Abraham},
booktitle = {Generative Artificial Intelligence and Ethics for Healthcare},
publisher = {Academic Press},
pages = {iv},
year = {2026},
isbn = {978-0-443-33124-4},
doi = {https://doi.org/10.1016/B978-0-443-33124-4.00303-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443331244003039}
}
@article{HERBOSCH2024105941,
title = {Fraud by generative AI chatbots: On the thin line between deception and negligence},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105941},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.105941},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000086},
author = {Maarten Herbosch},
keywords = {Artificial intelligence, Contract law, Fraud, Law of obligations, Vice of consent},
abstract = {The use of generative AI systems is on the rise. As a result, we are increasingly often conversing with AI chatbots rather than with fellow humans. This increasing use of AI systems leads to legal challenges as well, particularly when the chatbot provides incorrect information. In this article, we study whether someone who decides to contract on the basis of incorrect information provided by a generative AI chatbot might invoke the fraud regime to annul the resulting contract in various legal systems. During this analysis, it becomes clear that some of the requirements that are currently being put forward from a public law perspective, such as in the European AI Act, may also naturally arise from existing private law figures. In the same vein, this analysis highlights the interesting intradisciplinary feedback between instruments of public law and other legal domains.}
}
@article{HAN2024101007,
title = {Recent progress of efficient low-boom design and optimization methods},
journal = {Progress in Aerospace Sciences},
volume = {146},
pages = {101007},
year = {2024},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2024.101007},
url = {https://www.sciencedirect.com/science/article/pii/S0376042124000332},
author = {Zhonghua Han and Jianling Qiao and Liwen Zhang and Qing Chen and Han Yang and Yulin Ding and Keshi Zhang and Wenping Song and Bifeng Song},
keywords = {Sonic boom, Supersonic transport, Low-boom design, Surrogate-based optimization, Adjoint method},
abstract = {Reducing the sonic boom to a community-acceptable level is a fundamental challenge in the configuration design of the next-generation supersonic transport aircraft. This paper conducts a survey of recent progress in developing efficient low-boom design and optimization methods, and provides a perspective on the state-of-the-art and future directions. First, the low- and high-fidelity sonic boom prediction methods used in metric of low-boom design are briefly introduced. Second, efficient low-boom inverse design methods are reviewed, such as the classic Jones–Seebass–George–Darden (JSGD) method (and its variants), the high-fidelity near-field-overpressure-based method, and the mixed-fidelity method. Third, direct numerical optimization methods for low-boom designs, including the gradient-, surrogate-, and deep-learning-based optimization methods, are reviewed. Fourth, the applications of low-boom design and optimization methods to representative low-boom configurations are discussed, and the challenging demands for commercially viable supersonic transports are presented. In addition to providing a comprehensive summary of the existing research, the practicality and effectiveness of the developed methods are assessed. Finally, key challenges are identified, and further research directions such as full-carpet-low-boom-driven multidisciplinary design optimization considering mission requirements are recommended.}
}
@article{CHAKRABORTY2024100677,
title = {Role of human physiology and facial biomechanics towards building robust deepfake detectors: A comprehensive survey and analysis},
journal = {Computer Science Review},
volume = {54},
pages = {100677},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100677},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000613},
author = {Rajat Chakraborty and Ruchira Naskar},
keywords = {Deepfake, Facial biomechanics, Generative AI, Physiological signal, rPPG, Social engineering},
abstract = {AI based multimedia content generation, already having achieved hyper-realism, deeply influences human perception and trust. Since emerging around late 2017, deepfake technology has rapidly gained popularity due to its diverse applications, raising significant concerns regarding its malicious and unethical use. Although many deepfake detectors have been developed by forensic researchers in recent years, there is an urgent need for robust detectors that can overcome demographic, social, and cultural barriers in identifying deepfakes. To identify a human as a human, to distinguish a person from a synthetic entity, the literature faces compelling necessity to introduce deepfake detectors that can withstand all forms of demographic and social biases. (Multiple researches have been conducted in recent times to prove the existence of social and demographic biases in synthetic media detectors.) In this article, we examine human physiological signals as the foundation for robust deepfake detectors, and present a survey of recent developments in deepfake detection research that relies on human physiological signals and facial biomechanics. We perform in-depth analysis of the techniques to understand the contribution of human physiology in deepfake detection. Hence, we comprehend how human physiology based deepfake detectors fare by exploiting the inherent robustness of physiological signals, in contrast to other existing detectors.}
}
@article{2025e1,
title = {2025 Abstracts},
journal = {Journal of Endodontics},
volume = {51},
number = {3},
pages = {e1-e50},
year = {2025},
issn = {0099-2399},
doi = {https://doi.org/10.1016/S0099-2399(25)00143-8},
url = {https://www.sciencedirect.com/science/article/pii/S0099239925001438}
}
@article{CALISKAN20234895,
title = {Metadata integrity in bioinformatics: Bridging the gap between data and knowledge},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {4895-4913},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023003616},
author = {Aylin Caliskan and Seema Dangwal and Thomas Dandekar},
keywords = {Meta-data, Error, Annotation, Error-transfer, Wrong labelling, Patient data, Control group, Tools overview},
abstract = {In the fast-evolving landscape of biomedical research, the emergence of big data has presented researchers with extraordinary opportunities to explore biological complexities. In biomedical research, big data imply also a big responsibility. This is not only due to genomics data being sensitive information but also due to genomics data being shared and re-analysed among the scientific community. This saves valuable resources and can even help to find new insights in silico. To fully use these opportunities, detailed and correct metadata are imperative. This includes not only the availability of metadata but also their correctness. Metadata integrity serves as a fundamental determinant of research credibility, supporting the reliability and reproducibility of data-driven findings. Ensuring metadata availability, curation, and accuracy are therefore essential for bioinformatic research. Not only must metadata be readily available, but they must also be meticulously curated and ideally error-free. Motivated by an accidental discovery of a critical metadata error in patient data published in two high-impact journals, we aim to raise awareness for the need of correct, complete, and curated metadata. We describe how the metadata error was found, addressed, and present examples for metadata-related challenges in omics research, along with supporting measures, including tools for checking metadata and software to facilitate various steps from data analysis to published research.}
}
@article{FRISTON2024105500,
title = {Federated inference and belief sharing},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {156},
pages = {105500},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105500},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423004694},
author = {Karl J. Friston and Thomas Parr and Conor Heins and Axel Constant and Daniel Friedman and Takuya Isomura and Chris Fields and Tim Verbelen and Maxwell Ramstead and John Clippinger and Christopher D. Frith},
keywords = {Active inference, Distributed cognition, Federated learning, Structure learning, Message passing},
abstract = {This paper concerns the distributed intelligence or federated inference that emerges under belief-sharing among agents who share a common world—and world model. Imagine, for example, several animals keeping a lookout for predators. Their collective surveillance rests upon being able to communicate their beliefs—about what they see—among themselves. But, how is this possible? Here, we show how all the necessary components arise from minimising free energy. We use numerical studies to simulate the generation, acquisition and emergence of language in synthetic agents. Specifically, we consider inference, learning and selection as minimising the variational free energy of posterior (i.e., Bayesian) beliefs about the states, parameters and structure of generative models, respectively. The common theme—that attends these optimisation processes—is the selection of actions that minimise expected free energy, leading to active inference, learning and model selection (a.k.a., structure learning). We first illustrate the role of communication in resolving uncertainty about the latent states of a partially observed world, on which agents have complementary perspectives. We then consider the acquisition of the requisite language—entailed by a likelihood mapping from an agent's beliefs to their overt expression (e.g., speech)—showing that language can be transmitted across generations by active learning. Finally, we show that language is an emergent property of free energy minimisation, when agents operate within the same econiche. We conclude with a discussion of various perspectives on these phenomena; ranging from cultural niche construction, through federated learning, to the emergence of complexity in ensembles of self-organising systems.}
}
@article{MANTELERO2024106020,
title = {The Fundamental Rights Impact Assessment (FRIA) in the AI Act: Roots, legal obligations and key elements for a model template},
journal = {Computer Law & Security Review},
volume = {54},
pages = {106020},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106020},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000864},
author = {Alessandro Mantelero},
keywords = {AI Act, Fundamental rights impact assessment, FRIA, Fundamental Rights, AI},
abstract = {What is the context which gave rise to the obligation to carry out a Fundamental Rights Impact Assessment (FRIA) in the AI Act? How has assessment of the impact on fundamental rights been framed by the EU legislator in the AI Act? What methodological criteria should be followed in developing the FRIA? These are the three main research questions that this article aims to address, through both legal analysis of the relevant provisions of the AI Act and discussion of various possible models for assessment of the impact of AI on fundamental rights. The overall objective of this article is to fill existing gaps in the theoretical and methodological elaboration of the FRIA, as outlined in the AI Act. In order to facilitate the future work of EU and national bodies and AI operators in placing this key tool for human-centric and trustworthy AI at the heart of the EU approach to AI design and development, this article outlines the main building blocks of a model template for the FRIA. While this proposal is consistent with the rationale and scope of the AI Act, it is also applicable beyond the cases listed in Article 27 and can serve as a blueprint for other national and international regulatory initiatives to ensure that AI is fully consistent with human rights.}
}
@article{2024102745,
title = {Full Issue PDF},
journal = {JACC: Case Reports},
volume = {29},
number = {21},
pages = {102745},
year = {2024},
issn = {2666-0849},
doi = {https://doi.org/10.1016/S2666-0849(24)00778-2},
url = {https://www.sciencedirect.com/science/article/pii/S2666084924007782}
}
@incollection{BOHR202025,
title = {Chapter 2 - The rise of artificial intelligence in healthcare applications},
editor = {Adam Bohr and Kaveh Memarzadeh},
booktitle = {Artificial Intelligence in Healthcare},
publisher = {Academic Press},
pages = {25-60},
year = {2020},
isbn = {978-0-12-818438-7},
doi = {https://doi.org/10.1016/B978-0-12-818438-7.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128184387000022},
author = {Adam Bohr and Kaveh Memarzadeh},
keywords = {Artificial intelligence, healthcare applications, machine learning, precision medicine, ambient assisted living, natural language programming, machine vision},
abstract = {Big data and machine learning are having an impact on most aspects of modern life, from entertainment, commerce, and healthcare. Netflix knows which films and series people prefer to watch, Amazon knows which items people like to buy when and where, and Google knows which symptoms and conditions people are searching for. All this data can be used for very detailed personal profiling, which may be of great value for behavioral understanding and targeting but also has potential for predicting healthcare trends. There is great optimism that the application of artificial intelligence (AI) can provide substantial improvements in all areas of healthcare from diagnostics to treatment. It is generally believed that AI tools will facilitate and enhance human work and not replace the work of physicians and other healthcare staff as such. AI is ready to support healthcare personnel with a variety of tasks from administrative workflow to clinical documentation and patient outreach as well as specialized support such as in image analysis, medical device automation, and patient monitoring. In this chapter, some of the major applications of AI in healthcare will be discussed covering both the applications that are directly associated with healthcare and those in the healthcare value chain such as drug development and ambient assisted living.}
}
@incollection{MOURTZIS202413,
title = {2 - Industry 4.0 and smart manufacturing},
editor = {Dimitris Mourtzis},
booktitle = {Manufacturing from Industry 4.0 to Industry 5.0},
publisher = {Elsevier},
pages = {13-61},
year = {2024},
isbn = {978-0-443-13924-6},
doi = {https://doi.org/10.1016/B978-0-443-13924-6.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443139246000028},
author = {Dimitris Mourtzis},
keywords = {Computer science, operations management, information systems, computing, human-centered computing, systems engineering, software engineering, human–computer interaction, robotics, technology management, knowledge management, specific industry, data science, sustainability engineering, applied computing, sustainable development, computing methodology},
abstract = {Industry 4.0 and smart manufacturing are the latest technological advancements that have revolutionized the manufacturing industry. Industry 4.0 refers to the Fourth Industrial Revolution, which involves the integration of digital technologies such as the Internet of Things, Big Data, and Artificial Intelligence into the manufacturing process. Next, smart manufacturing is the application of Industry 4.0 technologies to improve the efficiency, productivity, and flexibility of manufacturing systems, processes, and services. Industry 4.0 and smart manufacturing have several benefits, such as improved production efficiency, reduced costs, enhanced product quality, and increased agility in responding to market changes. However, implementing these technologies requires significant investment in infrastructure, skilled personnel, and technology upgrades. To address these challenges and fully leverage the potential of Industry 4.0 and smart manufacturing, manufacturers need to develop a clear strategy, which includes identifying the areas of the business that can benefit the most from the technologies. In addition, manufacturers need to train their workforce to use and maintain the new technologies, while also addressing concerns about job displacement. In summary, Industry 4.0 and smart manufacturing offer significant potential for manufacturers to improve their operations, reduce costs, and stay competitive. However, it requires careful planning and investment to fully realize the benefits.}
}
@article{XU2024123923,
title = {Collaborative optimization of multi-energy multi-microgrid system: A hierarchical trust-region multi-agent reinforcement learning approach},
journal = {Applied Energy},
volume = {375},
pages = {123923},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.123923},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924013060},
author = {Xuesong Xu and Kai Xu and Ziyang Zeng and Jiale Tang and Yuanxing He and Guangze Shi and Tao Zhang},
keywords = {Multi-microgrid system, Integrated multi-energy network, Collaboration optimization, Flexible retraining mechanism, Hierarchical multi-agent reinforcement learning},
abstract = {In the context of the expanding diversity of energy demands, an increasing number of heterogeneous Multi-energy Microgrids (MEMGs) are engaging in the collaborative framework of the Multi-energy Multi-microgrid System (MEMMG). However, following this trend, the existing centralized Integrated Energy Management System (IEMS) control strategy is unreliable for future energy systems, characterized by more complex optimization control and a flexible system structure. This paper introduces a hierarchical Multi-agent Deep Reinforcement Learning (HMADRL) approach for distributed IEMS in MEMMG. Firstly, by employing a hierarchical approach, this method simplifies control complexity by segmenting the overarching control challenge into tasks of collaborative planning and action control, which are distributed across varied multi-agent scenes. Considering both macro and microeconomic factors, alongside carbon emissions, the optimal operation of MEMMG is realized through a bottom-up edge multi-agent control approach, in contrast to traditional top-down centralized methods. Secondly, in the phase of the inter-MEMG collaborative strategy, the Centralized Training Decentralized Execution (CTDE) framework is adopted to overcome the problems of unstable training environments and large-scale agent training, and each heterogeneous microgrid can develop local strategies independently with the assurance that their internal data will not be overly exposed. Thirdly, within each MEMG, the Trust-Region (TR) model is introduced for multi-agent action control, adeptly addressing the effects of mutual exclusion in decision-making time series. Simultaneously, an initialized Hot Experience Pool (HEP) is implemented, efficiently reducing exploration in complex, high-dimensional spaces. Finally, the off-time agent model is integrated into the HMADRL environment and undergoes secondary training based on real interactions, thereby deriving the optimal energy management policy. The proposed method markedly reduces reliance on exact physical modeling systems. Comparative simulations validate the proposed control scheme’s efficacy.}
}
@article{SMIMOU2024351,
title = {Commodities and Policy Uncertainty Channel(s)},
journal = {International Review of Economics & Finance},
volume = {92},
pages = {351-379},
year = {2024},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2024.01.065},
url = {https://www.sciencedirect.com/science/article/pii/S1059056024000650},
author = {K. Smimou and D. Bosch and G. Filbeck},
keywords = {Hedgers, Speculators, Commodity stock markets, Commodity futures, Policy uncertainty, Financialization of commodities},
abstract = {Based on a proposed linked theoretical model, this study dissects the contributory role of policy uncertainty on commodity futures contracts and pertinent commodity equity sectors. Within an economically connected framework, we elucidate the dynamic relationship between these groups of assets while allowing for policy uncertainty shock. Findings show that commodity hedgers altered trading positions in metals in response to a high policy uncertainty shock before 2004. In contrast, speculators account for that shock after that date purely via crude oil. Both monetary policy and regulatory uncertainties influence the pricing dynamics of metals and energy commodities. Given the inextricable commodity–stock relationship, we offer evidence to support the triple effect of economic policy uncertainty on the intensiveness of financialization of commodities and commodity stock returns through (1) change of institutional holdings, (2) managers’ alteration of CAPEX investment of commodity firms, and (3) the interactive causality channel between commodity futures and commodity stocks.}
}
@article{YAN2025126743,
title = {Representing similarities of map projections: An approach to approximate integrations and dimensionality reductions},
journal = {Expert Systems with Applications},
volume = {274},
pages = {126743},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126743},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425003653},
author = {Jin Yan and Jun Zhang and Tiansheng Xu and Jing Gao and Ni Li and Guanghong Gong},
keywords = {Map projection, Distance matrix, Distortion, Integrals calculation, Approximation, Dimensionality reduction},
abstract = {This study addresses key limitations in structured or hierarchical classification and pairwise comparison methods for analyzing map projections, which often fail to capture the intricate relationships among them. It introduces an innovative approach that automates and simplifies the approximation of complex results from an improved integration metric for measuring (dis)similarities between projections. This enhances the understanding of over 340 map projections comprehensively. The study combines formula-based and image-based methodologies within a hybrid sampling scheme, effectively handling complex integrations, particularly for projections with intricate formulas. Using NASA’s G.Projector mapping software, an 87 GB image dataset is generated for 175 map projections, while an additional 170 projections are processed through a formula-based approach. The approximate integral calculations closely align with theoretical values, with an acceptable error margin of a few thousandths. To visually represent map projections in two-dimensional space, multiple dimensionality reduction techniques are employed, incorporating features such as distortions. The resulting quantitative metrics demonstrate that global, local, and category-based features are reasonably preserved. A clear and intuitive visual representation simplifies the complexity of map projection relationships, offering valuable insights. Additionally, an interactive web application prototype is developed to showcase the relationships among map projections. To the best of our knowledge, this research is the first to comprehensively evaluate such a large number of map projections using automated calculations combined with dimensionality reduction and visualization techniques. This methodology represents a significant advancement in cartography, providing a robust framework for comparing and analyzing map projections in practice.}
}
@article{RIQUE2025107586,
title = {Constructing the graphical structure of expert-based Bayesian networks in the context of software engineering: A systematic mapping study},
journal = {Information and Software Technology},
volume = {177},
pages = {107586},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107586},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001915},
author = {Thiago Rique and Mirko Perkusich and Kyller Gorgônio and Hyggo Almeida and Angelo Perkusich},
keywords = {Bayesian networks, Bayesian network structure, Expert knowledge, Software engineering, Systematic mapping},
abstract = {Context:
In scenarios where data availability issues hinder the applications of statistical causal modeling in software engineering (SE), Bayesian networks (BNs) have been widely used due to their flexibility in incorporating expert knowledge. However, the general understanding of how the graphical structure, i.e., the directed acyclic graph (DAG), of these models is built from domain experts is still insufficient.
Objective:
This study aims to characterize the SE landscape of constructing the graphical structure of BNs, including their potential for causal modeling.
Method:
We conducted a systematic mapping study employing a hybrid search strategy that combines a database search with parallel backward and forward snowballing.
Results:
Our mapping included a total of 106 studies. Different methods are commonly combined to construct expert-based BN structures. These methods span across data gathering & analysis (e.g., interviews, focus groups, literature research, grounded theory, and statistical analysis) and reasoning mechanisms (e.g., using idioms combined with the adoption of lifecycle models, risk-centric modeling, and other frameworks to guide BN construction). We found a lack of consensus regarding validation procedures, particularly critical when modeling cause–effect relationships from knowledge. Additionally, expert-based BNs are mainly applied at the tactical level to address problems related to software engineering management and software quality. Challenges in creating expert-based structures include validation procedures, experts’ availability, expertise level, and structure complexity handling. Key recommendations involve empirical validation, participatory involvement, and balance between adaptation to organizational constraints and model construction requirements.
Conclusion:
The construction of expert-based BN structures in SE varies in rigor, with some methods being systematic while others appear ad hoc. To enhance BN application, reducing expert knowledge subjectivity, enhancing methodological rigor, and clearly articulating the construction rationale is essential. Addressing these challenges is crucial for improving the reliability of causal inferences drawn from these models, ultimately leading to better-informed decisions in SE practices.}
}
@article{REHMAN2025100174,
title = {3D-Printed solid-state electrolytes for next-generation batteries: Advances in design, challenges, and future opportunities},
journal = {EnergyChem},
volume = {7},
number = {6},
pages = {100174},
year = {2025},
issn = {2589-7780},
doi = {https://doi.org/10.1016/j.enchem.2025.100174},
url = {https://www.sciencedirect.com/science/article/pii/S2589778025000314},
author = {Fazal Ur Rehman and Trang Thi Vu and Jihwan Kim and Yujin Kim and Hyesoo Choi and Nayan Ranjan Singha and Mincheol Chang},
keywords = {3D-printing, Energy storage devices, Solid-state electrolytes, Polymers, LLZO},
abstract = {Battery technology is undergoing a transformative shift with the integration of 3D printing into solid-state electrolyte (SSE) design, enabling safer, more efficient, and sustainable next-generation energy storage solutions. This review examines recent advancements in 3D-printed SSEs, addressing critical failure mechanisms, performance challenges, and fundamental design principles. Traditional manufacturing methods often struggle to produce complex architectures; however, 3D printing offers exceptional precision, facilitating the fabrication of intricate structures that enhance interfacial compatibility with electrodes, improve thermal stability, and, most importantly, optimize ionic conductivity. This study explores monovalent cations (Na, K, and Li) and multicharged cations (Al3+, Ca2+, Zn2+, and Mg2+), highlighting the broad potential of next-generation batteries. By leveraging 3D-printed designs that optimize geometric, chemical, and mechanical properties, key challenges in SSEs are addressed, including poor ionic conductivity and interfacial resistance in inorganic electrolytes, as well as low cation transference numbers and oxidative instability in polymer-based components. Future prospects involve the integration of 3D-printed metals with advanced cathodic chemistries, such as Ni-rich and Li-rich additives, while also exploring renewable organic alternatives—including sulfur, oxygen, and even carbon dioxide—as sustainable components in battery technologies. This review underscores the transformative role of 3D printing in advancing SSEs as frontrunners in clean, efficient, and high-performance energy storage systems.}
}
@article{BAIRD20251945,
title = {A call to action: the second Lancet Commission on adolescent health and wellbeing},
journal = {The Lancet},
volume = {405},
number = {10493},
pages = {1945-2022},
year = {2025},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(25)00503-3},
url = {https://www.sciencedirect.com/science/article/pii/S0140673625005033},
author = {Sarah Baird and Shakira Choonara and Peter S Azzopardi and Prerna Banati and Judith Bessant and Olivia Biermann and Anthony Capon and Mariam Claeson and Pamela Y Collins and Nicole {De Wet-Billings} and Surabhi Dogra and Yanhui Dong and Kate L Francis and Luwam T Gebrekristos and Allison K Groves and Simon I Hay and David Imbago-Jácome and Aaron P Jenkins and Caroline W Kabiru and Elissa C Kennedy and Luo Li and Chunling Lu and Jun Ma and Terry McGovern and Augustina Mensa-Kwao and Sanyu A Mojola and Jason M Nagata and Adesola O Olumide and Olayinka Omigbodun and Molly O'Sullivan and Audrey Prost and Jennifer H Requejo and Yusra R Shawar and Jeremy Shiffman and Avi Silverman and Yi Song and Sharlene Swartz and Rita Tamambang and Henrik Urdal and Joseph L Ward and George C Patton and Susan M Sawyer and Alex Ezeh and Russell M Viner}
}
@incollection{2026v,
title = {Contents},
editor = {Loveleen Gaur and Ajith Abraham},
booktitle = {Generative Artificial Intelligence and Ethics for Healthcare},
publisher = {Academic Press},
pages = {v-xvi},
year = {2026},
isbn = {978-0-443-33124-4},
doi = {https://doi.org/10.1016/B978-0-443-33124-4.00304-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443331244003040}
}
@article{QUINTAIS2025106107,
title = {Generative AI, copyright and the AI Act},
journal = {Computer Law & Security Review},
volume = {56},
pages = {106107},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106107},
url = {https://www.sciencedirect.com/science/article/pii/S0267364925000020},
author = {João Pedro Quintais},
keywords = {Generative AI, AI Act, AI, Copyright, Text and data mining, Transparency, Remuneration},
abstract = {This paper provides a critical analysis of the Artificial Intelligence (AI) Act's implications for the European Union (EU) copyright acquis, aiming to clarify the complex relationship between AI regulation and copyright law while identifying areas of legal ambiguity and gaps that may influence future policymaking. The discussion begins with an overview of fundamental copyright concerns related to generative AI, focusing on issues that arise during the input, model, and output stages, and how these concerns intersect with the text and data mining (TDM) exceptions under the Copyright in the Digital Single Market Directive (CDSMD). The paper then explores the AI Act's structure and key definitions relevant to copyright law. The core analysis addresses the AI Act's impact on copyright, including the role of TDM in AI model training, the copyright obligations imposed by the Act, requirements for respecting copyright law—particularly TDM opt-outs—and the extraterritorial implications of these provisions. It also examines transparency obligations, compliance mechanisms, and the enforcement framework. The paper further critiques the current regime's inadequacies, particularly concerning the fair remuneration of creators, and evaluates potential improvements such as collective licensing and bargaining. It also assesses legislative reform proposals, such as statutory licensing and AI output levies, and concludes with reflections on future directions for integrating AI governance with copyright protection.}
}
@article{ABUALIGAH2025100646,
title = {A control-driven transition strategy for enhanced multi-level threshold image segmentation optimization},
journal = {Egyptian Informatics Journal},
volume = {30},
pages = {100646},
year = {2025},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2025.100646},
url = {https://www.sciencedirect.com/science/article/pii/S1110866525000398},
author = {Laith Abualigah and Mohammad H. Almomani and Saleh Ali Alomari and Raed Abu Zitar and Vaclav Snasel and Kashif Saleem and Aseel Smerat and Absalom E. Ezugwu},
keywords = {Flood Algorithm, Non-Monopolize search, Multi-level threshold, Image segmentation, Transition mechanism},
abstract = {This work proposes an image segmentation approach based on a multi-threshold segmentation method and the enhanced Flood Algorithm combined with the Non-Monopolize search (named Improved IFLANO). The introduced approach, depending on IFLANO, offers much better segmentation quality for various images. Based on the existing structure, two different types of optimization techniques are added within IFLANO to enhance the update dynamics during optimization. The random strategy used in the Aquila optimization procedure enhances the performance of FLA, and a self-transition adaptation enhances the exploration ability of the image analysis. IFLANO framework is implemented for multi-level threshold image segmentation wherein the evaluation metric is Kapur’s entropy-based between-class variance. Benchmarking studies against standard test images show that IFLANO works not only faster but also yields better, more stable outcomes in image segmentations within similar time frames. IFLANO is shown to put any solution a step forward in its search for more accurate alternatives than any of the considered techniques by getting 96% improvement. We also find that the proposed method got better results in solving large medical clustering applications.}
}
@article{WU2025127823,
title = {Recent development on online public opinion communication and early warning technologies: Survey},
journal = {Expert Systems with Applications},
volume = {284},
pages = {127823},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127823},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425014459},
author = {Wei Wu and Yawen Yang and Tianlu Qiao and Haipeng Peng},
keywords = {Online public opinion, Public opinion communication, Public opinion analyzing, Public opinion warning, Artificial intelligence},
abstract = {The rapid development of the Internet and communication technologies has led to the emergence of online public opinion, which plays a crucial role in disseminating information and guiding the public. However, due to its large volume of data and fast-paced changes, automated monitoring of online public opinion is challenging. Currently, advancements in artificial intelligence (AI) technology provides new solutions for detecting and early warning of online public opinion. This paper reviews the literature in the field of online public opinion research over the years. Firstly, it systematically elucidates the formation mechanism and dissemination model of online public opinion. Secondly, it focuses on the analysis of public opinion monitoring technology in the era of artificial intelligence, clarifying the research branches of online public opinion early warning technology. Finally, it identifies the shortcomings in current research on online public opinion and proposes directions for future research.}
}
@article{MUSTAPHA2025103066,
title = {A survey of emerging applications of large language models for problems in mechanics, product design, and manufacturing},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103066},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103066},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007171},
author = {K.B. Mustapha},
keywords = {Pre-trained language models, Large language models, Generative AI, Generative pre-trained transformer, Mechanical engineering, Engineering design, Manufacturing, Mechanics, Intelligent digital twins, Intelligent maintenance, Creativity},
abstract = {In the span of three years, the application of large language models (LLMs) has accelerated across a multitude of professional sectors. Amid this development, a new collection of studies has manifested around leveraging LLMs for segments of the mechanical engineering (ME) field. Concurrently, it has become clear that general-purpose LLMs faced hurdles when deployed in this domain, partly due to their training on discipline-agnostic data. Accordingly, there is a recent uptick of derivative ME-specific LLMs being reported. As the research community shifts towards these new LLM-centric solutions for ME-related problems, the shift compels a deeper look at the diffusion of LLMs in this emerging landscape. Consequently, this review consolidates the diversity of ME-tailored LLMs use cases and identifies the supportive technical stacks associated with these implementations. Broadly, the review demonstrates how various categories of LLMs are re-shaping concrete aspects of engineering design, manufacturing and applied mechanics. At a more specific level, it uncovered emerging LLMs’ role in boosting the intelligence of digital twins, enriching bidirectional communication within the human-cyber-physical infrastructure, advancing the development of intelligent process planning in manufacturing and facilitating inverse mechanics. It further spotlights the coupling of LLMs with other generative models for promoting efficient computer-aided conceptual design, prototyping, knowledge discovery and creativity. Finally, it revealed training modalities/infrastructures necessary for developing ME-specific language models, discussed LLMs' features that are incongruent with typical engineering workflows, and concluded with prescriptive approaches to mitigate impediments to the progressive adoption of LLMs as part of advanced intelligent solutions.}
}
@article{RADU2023100008,
title = {Charting opportunities and guidelines for augmented reality in makerspaces through prototyping and co-design research},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100008},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100008},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000028},
author = {Iulian Radu and Josia Yuan and Xiaomeng Huang and Bertrand Schneider},
keywords = {Augmented reality, Makerspaces, Co-design, STEM, Classroom integration},
abstract = {Makerspace environments are becoming popular project-based learning spaces where students interact with physical objects and peer collaboration, while developing 21st century skills and engaging with science, technology, engineering, and math (STEM) topics. At the same time, augmented reality (AR) technology, which combines physical objects with digital visualizations, is becoming increasingly applicable for makerspace activities and has potential to address challenges for student learning in makerspaces. However, there is a lack of understanding of how to use and integrate AR in real makerspace environments. In this research we use a co-design methodology to address the following questions: (1) How can AR be useful for education in makerspaces? (2) How are students impacted by the process of co-designing AR technology? and (3) What are practical considerations for integrating AR in makerspaces? We engaged in a co-design process in a semester-long makerspace course attended by 18 students in a graduate school of education. Through this process, we generated six prototypes with seven student co-designers, exploring AR use in design, fabrication, programming, electronics, and training. We also identified areas where AR technology can benefit makerspaces, such as teaching STEM skills, facilitating construction activities, enhancing contextualization of learning, and debugging. We observed that students participating in co-design demonstrated improved understanding of technology design, enthusiasm for engaging with makerspaces and AR, and increased critical thinking about AR technology. These results suggest considerations and guidelines for integrating AR technology into makerspace environments.}
}
@article{2024i,
title = {Table of Contents},
journal = {The Journal of Pharmacology and Experimental Therapeutics},
volume = {389},
pages = {i-xxvi},
year = {2024},
issn = {0022-3565},
doi = {https://doi.org/10.1016/S0022-3565(24)17260-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022356524172606}
}
@article{FOROUZANFAR2025177690,
title = {Significance of NMDA receptor-targeting compounds in neuropsychological disorders: An in-depth review},
journal = {European Journal of Pharmacology},
volume = {999},
pages = {177690},
year = {2025},
issn = {0014-2999},
doi = {https://doi.org/10.1016/j.ejphar.2025.177690},
url = {https://www.sciencedirect.com/science/article/pii/S0014299925004443},
author = {Fatemeh Forouzanfar and Amir Mahmoud Ahmadzadeh and Ali Mohammad Pourbagher-Shahri and Ali Gorji},
keywords = {Excitotoxicity, Glutamate receptors, Neurological diseases, Brain, Synaptic plasticity},
abstract = {N-methyl-D-aspartate receptors (NMDARs), a subclass of glutamate-gated ion channels, play an integral role in the maintenance of synaptic plasticity and excitation-inhibition balance within the central nervous system (CNS). Any irregularities in NMDAR functions, whether hypo-activation or over-activation, can destabilize neural networks and impair CNS function. Several decades of experimental and clinical investigations have demonstrated that NMDAR dysfunction is implicated in the pathophysiology of various neurological disorders. Despite designing a long list of compounds that differentially modulate NMDARs, success in developing drugs that can selectively and effectively regulate various NMDAR subtypes while showing encouraging efficacy in clinical settings remains limited. A better understanding of the basic mechanism of NMDAR function, particularly its selective regulation in pathological conditions, could aid in designing effective drugs for the treatment of neurological conditions. Here, we reviewed the experimental and clinical investigations that studied the effects of available NMDAR modulators in various neurological disorders and weighed up the pros and cons of the use of these substances on the improvement of functional outcomes of these disorders. Despite numerous efforts to develop NMDAR modulatory drugs that did not produce the desired outcomes, NMDARs remain a significant target for advancing novel drugs to treat neurological disorders. This article reviews the complexity of NMDAR signaling dysfunction in different neurological diseases, the efforts taken to examine designed compounds targeting specific subtypes of NMDARs, including challenges associated with using these substances, and the potential enhancements in drug discovery for NMDAR modulatory compounds by innovative technologies.}
}
@article{WU2026100817,
title = {New paradigm of distributed artificial intelligence for LLM implementation and its key technologies},
journal = {Computer Science Review},
volume = {59},
pages = {100817},
year = {2026},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100817},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000930},
author = {Yijin Wu and Zirun Li and Bingrui Guo and Shanshan He and Bijing Liu and Xiaojie Liu and Shan He and Donghui Guo},
keywords = {Distributed artificial intelligence, Cloud computing, Caching, Load-balancing, Reasoning, LLM},
abstract = {With the Internet’s development and information technology advancement, current network applications and services, such as e-commerce, industrial automation, and vehicular automation, have experienced substantial expansion. Foundation models, represented by large language models (LLMs), have emerged in response to growing demands. Their broad range of applications has brought significant advancements to various industries. While such developments have improved people’s economic lives and social activities, the challenges posed by the rapid growth of data volume and network traffic cannot be overlooked. Intelligent systems aimed at enhancing knowledge computation and learning capabilities are gradually gaining attention. Nevertheless, efficient and flexible intelligent systems are still in their early stages, leaving ample space for further optimization. This study provides an overview of Distributed Artificial Intelligence (DAI) with its related paradigm, briefly introduces the evolution of LLMs, and proposes a novel optimization framework named PCD Tri-Tuning for DAI workflows: leveraging caching-related technologies to enhance perceptual capabilities, adopting load-balancing techniques for computational optimization, and developing reasoning methodologies and cooperation techniques to improve decision-making. Subsequently, the study examines the pivotal role of the proposed optimization framework in practical domains such as e-commerce, smart manufacturing, and vehicular automation while also discussing the challenges and outlining strategies for further development.}
}
@article{DEMARTINO2025107678,
title = {Classification and challenges of non-functional requirements in ML-enabled systems: A systematic literature review},
journal = {Information and Software Technology},
volume = {181},
pages = {107678},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107678},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000175},
author = {Vincenzo {De Martino} and Fabio Palomba},
keywords = {Software engineering for artificial intelligence, Non-functional requirements, Systematic literature reviews},
abstract = {Context:
Machine learning (ML) is nowadays so pervasive and diffused that virtually no application can avoid its use. Nonetheless, its enormous potential is often tempered by the need to manage non-functional requirements (NFRs) and navigate pressing, contrasting trade-offs.
Objective:
In this respect, we notice a lack of systematic synthesis of challenges explicitly tied to achieving and managing NFRs in ML-enabled systems. Such a synthesis may not only provide a comprehensive summary of the state of the art but also drive further research on the analysis, management, and optimization of NFRS of ML-enabled systems.
Method:
In this paper, we propose a systematic literature review targeting two key aspects such as (1) the classification of the NFRs investigated so far, and (2) the challenges associated with achieving and managing NFRs in ML-enabled systems during model development Through the combination of well-established guidelines for conducting systematic literature reviews and additional search criteria, we survey a total amount of 130 research articles.
Results:
Our findings report that current research identified 31 different NFRs, which can be grouped into six main classes. We also compiled a catalog of 26 software engineering challenges, emphasizing the need for further research to systematically address, prioritize, and balance NFRs in ML-enabled systems.
Conclusion:
We conclude our work by distilling implications and a future outlook on the topic.}
}
@incollection{GAUR2026131,
title = {Chapter 8 - Trust, accountability, and informed consent: Cornerstones of ethical practice in clinical medicine},
editor = {Loveleen Gaur and Ajith Abraham},
booktitle = {Generative Artificial Intelligence and Ethics for Healthcare},
publisher = {Academic Press},
pages = {131-154},
year = {2026},
isbn = {978-0-443-33124-4},
doi = {https://doi.org/10.1016/B978-0-443-33124-4.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443331244000059},
author = {Loveleen Gaur and Ajith Abraham},
keywords = {Accountability, Clinical ethics, Healthcare communication, Informed consent, Medical transparency, Patient autonomy, Trust},
abstract = {Trust, accountability, and informed consent are the foundations of ethical clinical practice, necessary for maintaining effective patient-care relationships. This chapter begins by examining the ethical foundations of clinical medicine, emphasizing how these three principles foster trust and ensure patient autonomy. Trust is investigated in the context of healthcare relationships, highlighting its role in patient outcomes and how factors like communication and empathy build confidence. Accountability is addressed as a key aspect of ethical responsibility, with mechanisms such as peer review and legal frameworks enforcing it. Informed consent, legally and ethically grounded, empowers patients, promoting shared decision-making. Challenges, such as patient literacy and time constraints, complicate achieving true informed consent. The chapter also covers strategies to improve transparency, patient-provider communication, and shared decision-making models. Through case studies, it highlights real-world examples of trust, accountability, and informed consent in clinical settings, focusing on critical moments like end-of-life care and high-risk procedures. Lastly, the future of ethical practice is discussed, emphasizing evolving standards and the role of ethics education in creating more patient-centered healthcare.}
}
@article{SOME2024103486,
title = {Heterogeneity in the competition-cost of equity relation},
journal = {International Review of Economics & Finance},
volume = {95},
pages = {103486},
year = {2024},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2024.103486},
url = {https://www.sciencedirect.com/science/article/pii/S1059056024004787},
author = {Hyacinthe Y. Somé and Pascale Valéry},
keywords = {Competition, Agency costs, Implied cost of equity, Firm size, Free cash flow},
abstract = {We explore the effect of firm heterogeneity on the implied cost of equity. To this end, we exploit two opposing effects of competition on the cost of equity: its negative effect on a firm's exposure to systematic risk and its positive effect on a firm's agency costs when it acts as a managerial disciplining device. Using a U.S. sample comprising 4764 firms from 1986 to 2017, we find that, on average, competition reduces equity costs by diminishing managerial expropriation, but increases these costs for small and distressed firms as they are more exposed to systematic risk. This agency costs link holds in developed countries only.}
}
@article{RAZAQUE2025100789,
title = {A comprehensive review of cybersecurity vulnerabilities, threats, and solutions for the Internet of Things at the network-cum-application layer},
journal = {Computer Science Review},
volume = {58},
pages = {100789},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100789},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000656},
author = {Abdul Razaque and Salim Hariri and Abrar M. Alajlan and Joon Yoo},
keywords = {Internet of Things, Cyber security, Real-time, Controlling system, Data mining, Scientific decision-making system, Vulnerabilities, Security attacks},
abstract = {The proliferation of smart homes, smart logistics, and other technologies has expedited the expansion of Internet-of-Things (IoT) devices. This expansion has heightened the complexity of associated security challenges. Despite extensive research on IoT security, several studies fail to provide a comprehensive examination of both the network and application layers. This is particularly applicable to real-time and mission-critical settings. This review addresses that deficiency by offering a systematic review of IoT across five tiers. It concentrates on the application layer, categorizing it into three domains: real-time control systems, scientific decision-making systems, and query/scan search systems. The study examines vulnerabilities, attack vectors, and security measures in real-time control and query/scan systems. It examines how emerging technologies such as artificial intelligence (AI), Software Defined Networking (SDN), and fog/edge computing can enhance security via improved context awareness and access management. The study ultimately presents recommendations and suggests enhancements to foster trust, scalability, and enhanced security in contemporary IoT systems.}
}
@article{PESQUEIRA2024110655,
title = {Exploring the impact of EU tendering operations on future AI governance and standards in pharmaceuticals},
journal = {Computers & Industrial Engineering},
volume = {198},
pages = {110655},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110655},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224007770},
author = {Antonio Pesqueira and Andreia {de Bem Machado} and Sama Bolog and Rúben Pereira and Maria José Sousa},
keywords = {Artificial Intelligence (AI), Pharmaceutical Industry, Tender Management (TM), Governance, Ethics, Operational Efficiency},
abstract = {This research examines the incorporation of artificial intelligence (AI) into the domain of tender management (TM) within the pharmaceutical industry, with a particular emphasis on operational efficiency, governance, and compliance with European regulatory standards. A comparative analysis of four companies—two that have adopted AI and two that have not—reveals significant discrepancies in the management of TM processes between AI-driven and traditional companies. The study employs the Delphi method to ascertain expert consensus on eight critical areas of AI governance, including data privacy, transparency, and ethical AI use. The findings indicate that companies integrating AI demonstrate enhanced decision-making capabilities, accelerated processing times, and enhanced stakeholder engagement. However, they also encounter challenges pertaining to ethical governance and regulatory compliance. The research highlights the necessity of aligning the adoption of AI with the latest European directives, such as the AI Act and General Data Protection Regulation (GDPR), to ensure both operational efficiency and adherence to ethical standards. The broader implications of the study underscore the necessity for pharmaceutical companies to develop robust governance frameworks, prioritize ethical considerations, and maintain regulatory compliance to fully leverage the potential of AI. Additionally, the study contributes to the ongoing scholarly discourse by providing empirical evidence on the interplay between AI, ethics, and governance, thereby encouraging further interdisciplinary research. This work emphasizes the critical role of strategic AI adoption in maintaining competitive advantage while safeguarding societal trust and adhering to legal requirements.}
}
@article{RAZA2025108144,
title = {A comprehensive survey of Network Digital Twin architecture, capabilities, challenges, and requirements for Edge–Cloud Continuum},
journal = {Computer Communications},
volume = {236},
pages = {108144},
year = {2025},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2025.108144},
url = {https://www.sciencedirect.com/science/article/pii/S014036642500101X},
author = {Syed Mohsan Raza and Roberto Minerva and Noel Crespi and Maira Alvi and Manoj Herath and Hrishikesh Dutta},
keywords = {Edge-Cloud Continuum, Network Digital Twin, Componentization, Containerization, Segment, Microservices, Data modeling, Learning models, Simulation},
abstract = {Network Digital Twin (NDT) collects data from physical, virtual, and software components and supports real-time network performance analysis, emulation, and intelligent physical network control. This paper surveys the current state of NDT specifications and explores NDT benefits for Network Operators (NOs) and its possible roles in future network management. It discusses the NDT key components, architecture, and integration of Machine Learning and Artificial Intelligence models in the NDT. Further, it covers virtualization technology management, suitability of Software-Defined Networking capabilities, and simulation tools to empower NDT. Two perspectives make the position of this survey different from existing studies; first, it highlights NDT limitations regarding Edge–Cloud Continuum (ECC) contextualization. ECC is a purposeful trending integration of Edge and Cloud Computing, involving multiple stakeholders like Service Providers, Customers, and Platform or Infrastructure Providers. However, current NDT specifications have not mentioned the ways to benefit stakeholders other than NOs. We also discuss notable computing and communication technologies transformations necessary to consider during NDT modeling, the existing data models, and reusable vocabularies that can be extended to achieve a detailed ECC representation for all stakeholders, essentially for Service Providers and Customers. Secondly, a data model is proposed that covers descriptive and prescriptive features and aims to provide a granular representation of ECC components to meet stakeholders’ requirements and render particular user information views. Different explored NDT perspectives, and proposed data model reduces the impact of existing NDT limitations in ECC representation.}
}
@article{IGNACZ2025123256,
title = {Machine learning for the advancement of membrane science and technology: A critical review},
journal = {Journal of Membrane Science},
volume = {713},
pages = {123256},
year = {2025},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2024.123256},
url = {https://www.sciencedirect.com/science/article/pii/S0376738824008500},
author = {Gergo Ignacz and Lana Bader and Aron K. Beke and Yasir Ghunaim and Tejus Shastry and Hakkim Vovusha and Matthew R. Carbone and Bernard Ghanem and Gyorgy Szekely},
keywords = {Deep learning, Predictive models, Generative models, Molecular modeling, Cheminformatics},
abstract = {Machine learning (ML) has been rapidly transforming the landscape of natural sciences and has the potential to revolutionize the process of data analysis and hypothesis formulation as well as expand scientific knowledge. ML has been particularly instrumental in the advancement of cheminformatics and materials science, including membrane technology. In this review, we analyze the current state-of-the-art membrane-related ML applications from ML and membrane perspectives. We first discuss the ML foundations of different algorithms and design choices. Then, traditional and deep learning methods, including application examples from the membrane literature, are reported. We also discuss the importance of learning data and both molecular and membrane-system featurization. Moreover, we follow up on the discussion with examples of ML applications in membrane science and technology. We detail the literature using data-driven methods from property prediction to membrane fabrication. Various fields are also discussed, such as reverse osmosis, gas separation, and nanofiltration. We also differentiate between downstream predictive tasks and generative membrane design. Additionally, we formulate best practices and the minimum requirements for reporting reproducible ML studies in the field of membranes. This is the first systematic and comprehensive review of ML in membrane science.}
}
@article{OYELADE2025127455,
title = {SMAR + NIE IdeaGen: A knowledge graph based node importance estimation with analogical reasoning on large language model for idea generation},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127455},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127455},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010772},
author = {Olaide N. Oyelade and Hui Wang and Karen Rafferty},
keywords = {Knowledge graphs (KGs), Large language model (LLMs), Idea generation, Novelty, Analogical reasoning, Node importance estimation, Natural language processing (NLP), Isomorphic subgraphs},
abstract = {Idea generation describes a creative process involving reasoning over some knowledge to derive new information. Traditional approaches such as mind-map and brainstorming are limited and often fail due to lack of quality ideas and ineffective methods. The reasoning capability of large language models (LLMs) have been investigated for ideation tasks and have reported interesting performance. However, these models suffer from limited logical reasoning capability which hinders the use of structural and factual real-world knowledge in discovery of latent insight and predict possible outcome when applied to ideation. In addition, the possibility of LLMs regurgitating knowledge learnt from datasets might adversely impact the degree of novel ideas the models can generate. In this paper, a two-stage logical reasoning approach is applied to initiate the search for candidate idea pathways based on the knowledge graphs (KGs) to address the problem of reasoning, domain-specificity and novelty. The divergence stage this reasoning explores utilizes a new node importance estimation (NIE) technique over KGs to discover latent connections supporting idea generation. In the convergence stage of this reasoning, subgraph matching using analogical reasoning (SMAR) is applied to find matching patterns to describe a new idea. The use of SMAR + NIE and KGs helps to achieve an improvement in reasoning over KGs before transferring such reasoning to LLMs for translation of idea into natural language. To evaluate the degree of novelty of ideas generated, a relevance-to-novelty scoring metrics is proposed based on multiple premise entailment (MPE). We combined this metric with other popular metrics to evaluate the performance of SMAR + NIE on benchmark datasets, and as well on the quality of ideas generated. Findings from the study showed that this approach demonstrates competitive performance with mainstream LLMs in idea generation tasks.}
}
@article{DEWITTE2024106019,
title = {Better alone than in bad company: Addressing the risks of companion chatbots through data protection by design},
journal = {Computer Law & Security Review},
volume = {54},
pages = {106019},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106019},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000852},
author = {Pierre Dewitte},
keywords = {Privacy, Data protection, GDPR, AI Act, Data protection by design, Data protection impact assessments, Companion chatbots, Enforcement},
abstract = {Recent years have seen a surge in the development and use of companion chatbots, conversational agents specifically designed to act as virtual friends, romantic partners, life coaches or even therapists. Yet, these tools raise many concerns, especially when their target audience is comprised of vulnerable individuals. While the recently adopted AI Act is expected to address some of these concerns, both compliance and enforcement are bound to take time. Since the development of companion chatbots involves the processing of personal data at nearly every step of the process, from training to fine-tuning to deployment, this paper argues that the General Data Protection Regulation (“GDPR”), and data protection by design more specifically, already provides a solid ground for regulators and courts to force controllers to mitigate these risks. In doing so, it sheds light on the broad material scope of Articles 24(1) and 25(1) GDPR, highlights the role of these provisions as proxies to Fundamental Rights Impact Assessments (“FRIAs”), and peels off the many layers of personal data processing involved in the companion chatbots supply chain. That reasoning served as the basis for a complaint lodged with the Belgian data protection authority, the full text and supporting evidence of which are provided as supplementary materials.}
}
@incollection{2024II1,
title = {Index},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {II1-II29},
year = {2024},
isbn = {978-0-443-13223-0},
doi = {https://doi.org/10.1016/B978-0-443-13223-0.20002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132230200027}
}
@incollection{TODESCHINI2020599,
title = {4.25 - Chemometrics for QSAR Modeling☆},
editor = {Steven Brown and Romà Tauler and Beata Walczak},
booktitle = {Comprehensive Chemometrics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {599-634},
year = {2020},
isbn = {978-0-444-64166-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.14703-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472147031},
author = {Roberto Todeschini and Viviana Consonni and Davide Ballabio and Francesca Grisoni},
keywords = {Applicability domain, Consensus modelling, Molecular descriptors, QSAR, QSPR, Ranking, Validation, Variable selection},
abstract = {Chemometrics plays a fundamental role in quantitative structure-activity relationships (QSARs) and quantitative structure-property relationships (QSPRs) methods, which aim at empirically linking the molecular structure of chemicals to experimentally-measurable properties. In fact, chemometrics statistics and chemoinformatics are the basic tools for finding meaningful mathematical relationships between the molecular structure and biological, physico-chemical, toxicological and environmental properties of chemicals. The key elements of QSAR/QSPR are molecular descriptors, which are numerical indices encoding information related to the structure of chemicals and are used as independent variables in the subsequent modeling, thus connecting the QSAR approaches to the multivariate and chemometric world. In this article, historical and novel QSAR modeling approaches are presented. After describing the historical background of QSAR and the classical approaches, molecular descriptors are illustrated, with state-of-the-art as well as novel description methodologies. Additionally, the key principles of QSAR are introduced, along with specific elements of the QSAR modeling workflow, e.g., variable reduction and selection, similarity-based approaches, validation, the definition of applicability domain and consensus modeling.}
}
@article{JEGATHEESAN2024137648,
title = {Modeling the properties of terminal blend crumb rubber modified bitumen with crosslinking additives},
journal = {Construction and Building Materials},
volume = {444},
pages = {137648},
year = {2024},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2024.137648},
url = {https://www.sciencedirect.com/science/article/pii/S0950061824027909},
author = {N. Jegatheesan and Mohd Rasdan Ibrahim and Ali Najah Ahmed and Suhana Koting and Ahmed El-Shafie and Herda Yati Binti Katman},
keywords = {Machine-learning algorithms, Prediction models, Terminal blend-crumb rubber modified bitumen, Crosslinking additive, Composite modification, High interaction parameters},
abstract = {This study aimed to develop models assessing 26 machine-learning algorithms in regression analysis to predict the properties of terminal blend crumb rubber-modified bitumen (TB-CRMB) made with crosslinking additives. During the data collection, the properties of the modified binders prepared at 6, 10 and 14% of crumb rubber (CR), considering three types of modifications and eighteen blending scenarios with different interaction factors, were assessed in terms of penetration, softening point, rotational viscosity, storage stability, rheological parameters, and rutting and fatigue factors. Results showed that the Matern 5/2 Gaussian Process Regression (GPR) model demonstrated efficient performance in predicting physical, viscoelastic, rutting, and fatigue properties whereas wide artificial neural networks exhibited enhanced accuracy in predicting storage stability and rotational viscosity. The results also suggest that it is feasible to implement a single type of model developed using the Matern 5/2 GPR algorithm for accurately predicting all the TB-CRMB properties considered. The best models demonstrated that crosslinking additives significantly influenced TB-CRMB production and performance. In TB-CRMB production, sulfur as a crosslinking additive showed better compatibility than trans-polyoctenamer-rubber and significantly reduced interaction temperatures at lower CR content, leading to energy savings compared to the traditional TB production.}
}
@article{BAASSIRI2025125204,
title = {CFD modelling and simulations of atomization-based processes for production of drug particles: A review},
journal = {International Journal of Pharmaceutics},
volume = {670},
pages = {125204},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125204},
url = {https://www.sciencedirect.com/science/article/pii/S0378517325000407},
author = {Mohamad Baassiri and Vivek Ranade and Luis Padrela},
keywords = {CFD, Drug nanoparticles, Atomization, Computational modelling},
abstract = {Atomization-based techniques are widely used in pharmaceutical industry for production of fine drug particles due to their versatility and adaptability. Key performance measure of such techniques is their ability to provide control over critical quality attributes (CQAs) of produced drug particles. CQAs of drug particles produced via atomization critically depend on fluid dynamics of sprays; resulting mixing, heat and mass transfer; distribution of supersaturation and subsequent nucleation and growth of particles. It is essential to develop and use computational fluid dynamics (CFD) models for adequate understanding of multi-scale transport processes ranging from molecular scale mixing and particle scale processes, and from atomizer nozzle to overall spray chamber scale establishing relationships between CQAs and design and operating parameters of spray nozzle and chamber. In this work, we critically review past and current research efforts on CFD modelling of pharmaceutical atomization-based processes with an objective to provide clear assessment of the state of the art and to provide recommendations. An overview of the key atomization-based methods for producing drug particles with desired CQAs is presented. Key underlying physical processes and relevant concepts are then outlined. This discussion is related to the demands on CFD models; and state of the art is then discussed with respect to the process needs. Recommendations are provided towards higher fidelity and more efficient models of atomized multiphase flow dynamics and turbulence, drying modelling for the produced particles, and validation approaches. We conclude by highlighting a perceived need for numerical atomization studies with a pharmaceutical context; then, we deliver an outlook on current promising active control and machine learning strategies to augment the shift towards quality-by-design approaches in pharmaceutical manufacturing.}
}
@article{CZECH20231867,
title = {The Lancet Commission on medicine, Nazism, and the Holocaust: historical evidence, implications for today, teaching for tomorrow},
journal = {The Lancet},
volume = {402},
number = {10415},
pages = {1867-1940},
year = {2023},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(23)01845-7},
url = {https://www.sciencedirect.com/science/article/pii/S0140673623018457},
author = {Herwig Czech and Sabine Hildebrandt and Shmuel P Reis and Tessa Chelouche and Matthew Fox and Esteban González-López and Etienne Lepicard and Astrid Ley and Miriam Offer and Avi Ohry and Maike Rotzoll and Carola Sachse and Sari J Siegel and Michal Šimůnek and Amir Teicher and Kamila Uzarczyk and Anna {von Villiez} and Hedy S Wald and Matthew K Wynia and Volker Roelcke}
}
@article{2024101309,
title = {Full Issue PDF},
journal = {JACC: Advances},
volume = {3},
number = {9, Part 2},
pages = {101309},
year = {2024},
note = {AI in Cardiology: Improving Outcomes for All Focus Issue},
issn = {2772-963X},
doi = {https://doi.org/10.1016/j.jacadv.2024.101309},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X24005878}
}
@article{BAYS2023100065,
title = {Artificial intelligence and obesity management: An Obesity Medicine Association (OMA) Clinical Practice Statement (CPS) 2023},
journal = {Obesity Pillars},
volume = {6},
pages = {100065},
year = {2023},
issn = {2667-3681},
doi = {https://doi.org/10.1016/j.obpill.2023.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2667368123000116},
author = {Harold Edward Bays and Angela Fitch and Suzanne Cuda and Sylvia Gonsahn-Bollie and Elario Rickey and Joan Hablutzel and Rachel Coy and Marisa Censani},
keywords = {Adiposopathy, Artificial intelligence, Education, Obesity},
abstract = {Background
This Obesity Medicine Association (OMA) Clinical Practice Statement (CPS) provides clinicians an overview of Artificial Intelligence, focused on the management of patients with obesity.
Methods
The perspectives of the authors were augmented by scientific support from published citations and integrated with information derived from search engines (i.e., Chrome by Google, Inc) and chatbots (i.e., Chat Generative Pretrained Transformer or Chat GPT).
Results
Artificial Intelligence (AI) is the technologic acquisition of knowledge and skill by a nonhuman device, that after being initially programmed, has varying degrees of operations autonomous from direct human control, and that performs adaptive output tasks based upon data input learnings. AI has applications regarding medical research, medical practice, and applications relevant to the management of patients with obesity. Chatbots may be useful to obesity medicine clinicians as a source of clinical/scientific information, helpful in writings and publications, as well as beneficial in drafting office or institutional Policies and Procedures and Standard Operating Procedures. AI may facilitate interactive programming related to analyses of body composition imaging, behavior coaching, personal nutritional intervention & physical activity recommendations, predictive modeling to identify patients at risk for obesity-related complications, and aid clinicians in precision medicine. AI can enhance educational programming, such as personalized learning, virtual reality, and intelligent tutoring systems. AI may help augment in-person office operations and telemedicine (e.g., scheduling and remote monitoring of patients). Finally, AI may help identify patterns in datasets related to a medical practice or institution that may be used to assess population health and value-based care delivery (i.e., analytics related to electronic health records).
Conclusions
AI is contributing to both an evolution and revolution in medical care, including the management of patients with obesity. Challenges of Artificial Intelligence include ethical and legal concerns (e.g., privacy and security), accuracy and reliability, and the potential perpetuation of pervasive systemic biases.}
}
@article{TAHERKHANI2023848,
title = {On the application of in-situ monitoring systems and machine learning algorithms for developing quality assurance platforms in laser powder bed fusion: A review},
journal = {Journal of Manufacturing Processes},
volume = {99},
pages = {848-897},
year = {2023},
issn = {1526-6125},
doi = {https://doi.org/10.1016/j.jmapro.2023.05.048},
url = {https://www.sciencedirect.com/science/article/pii/S1526612523005212},
author = {Katayoon Taherkhani and Osazee Ero and Farima Liravi and Sahar Toorandaz and Ehsan Toyserkani},
keywords = {Additive manufacturing, Laser powder bed fusion, In-situ sensors, In-situ monitoring, Machine learning},
abstract = {Laser powder bed fusion (LPBF) is one class of metal additive manufacturing (AM) used to fabricate high-quality complex-shape components. This technology has significantly progressed over the last several years allowing the fabrication of high-value components for a broad range of applications, normally unmatched by other metal AM processes. However, the full adoption of LPBF to serial production is still challenging due to several barriers such as repeatability and reliability of final product quality. The main obstacle could be the high sensitivity of LPBF to environmental and process disturbances. Additionally, LPBF is governed by many process parameters. These factors profoundly affect the process, causing defects formation. To achieve high quality parts, trial and errors are conventionally carried out to obtain optimum parameters that result in good quality for a specific application. However, in recent years attention to the development of quality assurance platforms in LPBF has been the cornerstone of research and development. To this end, researchers have proceeded with three steps: 1) Gaining knowledge from the process by installing in-situ sensing equipment and collecting information from the process. 2) Understanding how the print parameters affect the process, analyzing in-situ datasets and developing defect detection algorithms, and 3) Developing real-time closed-loop control systems using the detection algorithms of Step 2 to automatically adjust the undesired phenomena in the process by changing the print parameters. Although valuable studies were published for the two first steps, the development of real-time controllers has remained challenging. Thus, this study aims to critically review the two first steps to provide insights for researchers into moving toward the development of the control system. In this study, in-situ sensing devices implemented in LPBF are categorized, explained in detail, and mapped to the literature. Then, a comprehensive review is conducted on the latest machine learning (ML) algorithms applied to the in-situ data of LPBF, such as supervised learning, unsupervised learning, and reinforcement learning. Additionally, a comprehensive discussion is provided on in-situ sensors and ML methods applied to LPBF. Lastly, this article specifies trends and future research outlook on this topic.}
}
@article{ASHIK2025115053,
title = {Can marketing reduce inequality? Evidence from marketing science},
journal = {Journal of Business Research},
volume = {188},
pages = {115053},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.115053},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324005575},
author = {Farhan Ashik and Weng {Marc Lim} and Jarrod P. Vassallo and Ranjit Voola},
keywords = {Inequality, Marketing, Systematic Literature Review, Sustainable Development Goals},
abstract = {Reducing inequality is integral to ensuring that no one is left behind. As a discipline, marketing can play a significant role in addressing inequality. To understand the current status and potential of marketing theory and practice to reduce inequality, a comprehensive and systematic literature review is needed. Addressing this gap, we conduct a systematic review of 313 marketing studies on inequalities. Our review highlights that inequality manifests in numerous ways—including consumption, culture, digital, economic, education, gender, geographical, health, income, power, racial, social, socioeconomic, structural, wealth, and general inequalities—and identifies five broad categories of antecedents and outcomes (individual, family, environmental, organization, and country). However, many studies lack specificity in defining and examining these inequalities, hindering the development of targeted interventions. To address this, we propose new definitions for each type of inequality and outline clear pathways for future research. These contributions not only highlight current progress in the field but also establish a roadmap for advancing marketing scholarship on inequality.}
}
@article{ZIVIC2025113525,
title = {Materials informatics: A review of AI and machine learning tools, platforms, data repositories, and applications to architectured porous materials},
journal = {Materials Today Communications},
volume = {48},
pages = {113525},
year = {2025},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2025.113525},
url = {https://www.sciencedirect.com/science/article/pii/S2352492825020379},
author = {Fatima Zivic and Ana Kaplarevic Malisic and Nenad Grujovic and Boban Stojanovic and Milos Ivanovic},
keywords = {Traditional computational models, Data-driven AI material models, Smart materials, Deep Tech, Structure-property-processing relationships, High-throughput screening, Electrospinning, 3D printed biomimetic porosity},
abstract = {This review presents the key aspects and development directions of materials informatics, emphasizing the role of artificial intelligence (AI) and machine learning (ML) in materials science research. The objective is to provide a comprehensive overview of materials informatics tools, workflows, and case studies, particularly aimed at experimental researchers unfamiliar with AI frameworks. Basic concepts are introduced and traditional modelling methods compared to AI/ML-assisted models. Existing material models serve as a foundation for advanced modelling and simulations aimed at reducing the time required for characterisation and discovery, with physics-based models gaining importance in the development of AI-supported surrogate models. This review also covers currently available resources, including: (i) software for solving complex mathematical equations and material modelling; (ii) web-based platforms and tools designed for both expert and non-expert users; and (iii) materials data repositories, prioritising standardisation. Case examples involving materials with architectured macro-, micro-, and nano-porosity are reviewed across three material types: metal-organic frameworks (MOFs), electrospun PVDF piezoelectrics, and 3D printed mechanical metamaterials. Traditional computational models offer interpretability and physical consistency, AI/ML excels in speed and complexity handling but may lack transparency. Hybrid models combining both approaches show excellent results in prediction, simulation, and optimisation, offering both speed and interpretability. Progress depends on modular, interoperable AI systems, standardised FAIR data, and cross-disciplinary collaboration. Addressing data quality and integration challenges will resolve issues related to metadata gaps, semantic ontologies, and data infrastructures, especially for small datasets and unlock transformative advances in fields like nanocomposites, MOFs, and adaptive materials.}
}
@article{2025e1,
title = {PDF of Full Issue},
journal = {Annals of Emergency Medicine},
volume = {86},
number = {3, Supplement 1},
pages = {e1-e277},
year = {2025},
note = {ACEP Research Forum 2025},
issn = {0196-0644},
doi = {https://doi.org/10.1016/S0196-0644(25)01074-1},
url = {https://www.sciencedirect.com/science/article/pii/S0196064425010741}
}
@article{YU2026103113,
title = {Transformation of industrial robotics with natural language models: Recent progress and future prospects},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {97},
pages = {103113},
year = {2026},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.103113},
url = {https://www.sciencedirect.com/science/article/pii/S073658452500167X},
author = {Zhao Yu and Peize Zhang and Jing Shi},
keywords = {Natural language models, Industrial robots, Industry 4.0/5.0, Human-robot interactions, Regulatory considerations},
abstract = {Integration of Natural Language Models (NLMs) into industrial robots enhances operational efficiency and intuitive human-robot interactions, and thus it represents a significant opportunity in the pursuit of Industry 4.0/5.0. This paper provides a comprehensive survey on the technological advancements and applications in this area, by emphasizing their role in improving task execution, cognitive capabilities, and communication in the industrial environments. Meanwhile, related challenges are analyzed and discussed. In particular, NLMs inherently struggle with contextual understanding, which can lead to inappropriate or impractical outputs in complex industrial environments. Also, the external noise and the need for real-time responsiveness present further complications to the effectiveness of NLMs. Concerns regarding safety, transparency, privacy, and ethical usage amplify the need for regulatory considerations. In addition, standardized approaches to interpreting vague human instructions are called for to improve the interaction between humans and robots. It is pointed out that the broader impacts of NLMs can extend beyond industrial environments into commercial and social settings, thereby enhancing service quality and customer interactions. As a result, the review is expected to provide insights on how to effectively integrate NLMs with robotic systems, stimulate research to address the remaining challenges, and enhance transparency to improve social acceptability.}
}
@article{2025I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Interventions},
volume = {18},
number = {20},
pages = {I-CXXXIV},
year = {2025},
issn = {1936-8798},
doi = {https://doi.org/10.1016/S1936-8798(25)02545-2},
url = {https://www.sciencedirect.com/science/article/pii/S1936879825025452}
}
@article{WEIMANN2025222,
title = {ESPEN guideline on clinical nutrition in surgery – Update 2025},
journal = {Clinical Nutrition},
volume = {53},
pages = {222-261},
year = {2025},
issn = {0261-5614},
doi = {https://doi.org/10.1016/j.clnu.2025.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0261561425002432},
author = {Arved Weimann and Mihailo Bezmarevic and Marco Braga and M. Isabel T.D. Correia and Pamela Funk-Debleds and Luca Gianotti and Chelsia Gillis and Martin Hübner and Jesus Fernando B. Inciong and Mohammad Shukri Jahit and Stanislaw Klek and Takayuki Kori and Alessandro Laviano and Olle Ljungqvist and Dileep N. Lobo and Carmelo Loinaz Segurola and Isacco Montroni and B. Ravinder Reddy and Nicole M. Saur and Anna Schweinlin and Han-Ping Shi and Hiroya Takeuchi and Dan L. Waitzberg and Ola Wallengren and Paul E. Wischmeyer and Dirk Ysebaert and Stephan C. Bischoff},
keywords = {Enhanced recovery after surgery, Enteral nutrition, Parenteral nutrition, Perioperative nutrition, Prehabilitation, Surgery},
abstract = {Summary
Early oral feeding is the preferred mode of nutrition for surgical patients. Avoidance of any nutritional therapy bears the risk of underfeeding during the postoperative course after major surgery. Considering that malnutrition and underfeeding are risk factors for postoperative complications, nutritional therapy is mandatory for any surgical patient at nutritional risk, especially for those undergoing upper gastrointestinal surgery. The focus of this guideline is to cover nutritional aspects of the Enhanced Recovery After Surgery (ERAS) concept and the special nutritional needs of patients undergoing major surgery, e.g. for cancer, and of those developing severe complications despite best perioperative care. From a metabolic and nutritional point of view, the key aspects of perioperative care include: a) Integration of nutrition into the overall management of the patient, b) avoidance of long periods of preoperative fasting c) re-establishment of oral feeding as early as possible after surgery d) start of nutritional therapy early, as soon as a nutritional risk becomes apparent e) metabolic control e.g. of blood glucose, f) reduction of factors which exacerbate stress-related catabolism or impair gastrointestinal function, g) minimized time on paralytic agents in the postoperative period, and h) early mobilization to facilitate protein synthesis and muscle function. The guideline presents 44 recommendations for clinical practice in patients undergoing elective and non-elective surgery, including new recommendations for frailty assessment, sarcopenia diagnosis, and prehabilitation. As in the former ESPEN practical guideline, the recommendations were additonally presented in decision-making flowcharts.}
}
@incollection{GAUR2026155,
title = {Chapter 9 - Personalized medicine and data privacy: Where to draw the boundary?},
editor = {Loveleen Gaur and Ajith Abraham},
booktitle = {Generative Artificial Intelligence and Ethics for Healthcare},
publisher = {Academic Press},
pages = {155-174},
year = {2026},
isbn = {978-0-443-33124-4},
doi = {https://doi.org/10.1016/B978-0-443-33124-4.00010-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443331244000102},
author = {Loveleen Gaur and Ajith Abraham},
keywords = {Data breaches, Data privacy, Data security, Healthcare ethics, Informed consent, Patient autonomy, Personalized medicine},
abstract = {Personalized medicine promises to revolutionize healthcare by tailoring treatments to individual patients. However, this innovation raises significant ethical concerns, particularly in terms of data privacy and informed consent. Large-scale patient data are essential for personalized treatments, yet its aggregation poses risks of misuse and exploitation. Informed consent is crucial, but ensuring true conception in complex data-driven healthcare can be challenging. This chapter explores strategies to enhance patient privacy, improve informed consent processes, and secure patient data through methods like encryption and anonymization. Case studies of data breaches in personalized medicine demonstrate the heightened risks and underscore the need for robust data security measures. Also, the role of regulation and policy in addressing privacy concerns is vital to balancing innovation with the ethical responsibility to protect patient information and ensure transparency.}
}
@article{BENNEHALLI2025101172,
title = {A review on the formation, recovery, and properties of coal fly ash (CFA)-derived microspheres for sustainable technologies and biomedical applications},
journal = {Next Materials},
volume = {9},
pages = {101172},
year = {2025},
issn = {2949-8228},
doi = {https://doi.org/10.1016/j.nxmate.2025.101172},
url = {https://www.sciencedirect.com/science/article/pii/S2949822825006902},
author = {Basavaraju Bennehalli and Suresh Subramanyam Poyil and Budigi Lokesh and Santhosh Nagaraja and Sunil Basavaraju and  Rispandi and Muhammad Imam Ammarullah},
keywords = {Coal fly ash (CFA), Cenospheres (CS), Plerospheres (PS), Ferrospheres (FS), Microspheres},
abstract = {Coal fly ash (CFA), a by-product of coal combustion in thermal power plant (TPP), is an environmental concern due to its massive production and improper disposal. Among its components, microspheres like cenospheres (CS), plerospheres (PS), and ferrospheres (FS) hold significant industrial value. CS are lightweight, hollow particles with unique properties such as low density, high mechanical strength, and thermal stability, making them suitable for composites, ceramics, and insulation. PS, with their porous structures, are useful in construction and ceramics, while FS, rich in iron, are applied in catalysis and magnetic materials. Additionally, CFA-derived microspheres, such as CS and FS, exhibit promising potential in biomedical applications due to their unique structural and chemical features. Their suitability for drug delivery, tissue engineering, and diagnostic tools highlights their emerging role in sustainable healthcare solutions. This review focuses on the formation, recovery, and properties of these microspheres, highlighting their sustainable applications in lightweight composites, environmental clean-up, and advanced materials. Various recovery methods, including wet and dry techniques, are discussed to optimize extraction processes. The study emphasizes the potential of these microspheres in reducing CFA waste while supporting innovative and eco-friendly technologies. This work contributes to developing sustainable solutions for managing CFA, with the goal of reducing environmental impacts and enhancing industrial utility, particularly in sustainable and biomedical applications.}
}
@incollection{CHOU20241,
title = {Chapter I - A new alternative concept for cost-effective R&D: The MAL-dynamics/algorithms/digital informatics⊛⊛This book has a companion website hosting complementary materials. Visit this URL to access it: https://www.elsevier.com/books-and-journals/book-companion/9780443288746.},
editor = {Ting-Chao Chou},
booktitle = {Mass-Action Law Dynamics Theory and Algorithm for Translational and Precision Medicine Informatics},
publisher = {Academic Press},
pages = {1-37},
year = {2024},
isbn = {978-0-443-28874-6},
doi = {https://doi.org/10.1016/B978-0-443-28874-6.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288746000019},
author = {Ting-Chao Chou},
keywords = {Mass-action law dynamics, Bioinformatics algorithms, Biodynamics simulation, Pharmacodynamics, Median-effect equation, Median-effect plot and simulation, Doctrine of the median, Combination index equation, Combination index computer simulation, CompuSyn software, CalcuSyn software, Dose-reduction index equation, Dose-reduction index computer simulation, Definition of synergism, Definition of pharmacodynamics},
abstract = {This chapter provides a comprehensive overview, illustrations, and updates of the mass-action law (MAL) based on unified general dose-effect pharmacodynamics, biodynamics, bioinformatics, and the combination index theorem for synergy definition and quantification (MAL-PD/BD/BI/CI). The general system analysis was developed with pattern analysis, combinatory analysis, and mathematical induction and deduction on the MAL principle, which resulted in (i) The general median-effect equation (MEE) for each entity with two basic dynamic parameters of potency (Dm) and dynamics-order (m), for the potency and the shape of dose-effect curves; (ii) the combination index equation (CIE) theorem/algorithm, determines the drugs or entities interaction, where CI < 1, =1, and >1 indicate synergism, additive-effect, and antagonism, respectively; (iii) the dose-reduction index equation (DRIE) that digitally determines the outcomes of how many folds of dose-reduction for each drug in synergistic combinations. Since all terms of MAL-general-equations (MEE, CIE, and DRIE) are dimensionless-relativity ratios, they are equally applicable in vitro, in animals, and in clinical trials. They are also valid regardless of drug entities, units, mechanisms, or physical states. To date, this MAL-PD/BD/BI/CI theory-based “Top-Down” approach and quantitative method has received multidisciplinary research popularity globally, with citations in over 1500 journals and 1268 citing patents, encompassing nearly all disciplines of biomedical sciences and beyond, the subjects including material, agricultural, marine, environmental and food sciences. Original applications were mainly in vitro; however, in vivo PD applications are on the rise. The MAL-BD/PD unified theory/method has the features of simplicity, efficiency, and cost-effectiveness, allows automated computer simulation with only a few dose-data points, and shares the same basic MAL principle. In vitro, in vivo, or other physical states in animal studies and clinical trial protocol design, automated computerized data analysis and simulation to single drug and drug combinations. With the same MAL-PD principle, thus enabling comparisons and rankings. This book illustrates the MAL-dynamics theory to update the increasing applications in recent years and to provide specific real sample analysis, including data entries and automated computer report print-outs in Appendixes and Supplementary Materials in PDF slides illustrations. The MAL-theory-based Top Down approach (traditional biomedical R&D) is an observation and statistics-based “Bottom-Up” approach with specific aims, proposals, and methods to reach feasible hypotheses or conclusions. This open approach is usually accomplished with multiple experimental evidence and results, using unbiased statistics or other methods to reach a hypothesis, mechanism, or interpretive conclusion. However, the best curve fitting for dose-effect relationship data is frequently empirical and requires many dose-data points. The primary purpose of this book is to indicate that the MAL theory/algorithm-based “Top-Down” digital approach is the opposite and yet a complementary alternative to the observation/statistics-based “Bottom-Up” traditional approach in R&D.}
}
@article{2024e1,
title = {PDF of the Full Issue},
journal = {Annals of Emergency Medicine},
volume = {84},
number = {4, Supplement 1},
pages = {e1-e251},
year = {2024},
note = {ACEP Research Forum 2024},
issn = {0196-0644},
doi = {https://doi.org/10.1016/S0196-0644(24)01011-4},
url = {https://www.sciencedirect.com/science/article/pii/S0196064424010114}
}
@article{GOLDSCHMIDT2025104510,
title = {Network intrusion datasets: A survey, limitations, and recommendations},
journal = {Computers & Security},
volume = {156},
pages = {104510},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104510},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825001993},
author = {Patrik Goldschmidt and Daniela Chudá},
keywords = {Network intrusion detection, NIDS, Machine learning for intrusion detection, Cybersecurity datasets, NIDS best practices, Comparative dataset analysis},
abstract = {Data-driven cyberthreat detection has become a crucial defense technique in modern cybersecurity. Network defense, supported by Network Intrusion Detection Systems (NIDSs), has also increasingly adopted data-driven approaches, leading to greater reliance on data. Despite the importance of data, its scarcity has long been recognized as a major obstacle in NIDS research. In response, the community has published many new datasets recently. However, many of them remain largely unknown and unanalyzed, leaving researchers uncertain about their suitability for specific use cases. In this paper, we aim to address this knowledge gap by performing a systematic literature review (SLR) of 89 public datasets for NIDS research. Each dataset is comparatively analyzed across 13 key properties, and its potential applications are outlined. Beyond the review, we also discuss domain-specific challenges and common data limitations to facilitate a critical view on data quality. To aid in data selection, we conduct a dataset popularity analysis in contemporary state-of-the-art NIDS research. Furthermore, the paper presents best practices for dataset selection, generation, and usage. By providing a comprehensive overview of the domain and its data, this work aims to guide future research toward improving data quality and the robustness of NIDS solutions.}
}
@article{2025S729,
title = {Subject Index},
journal = {Gastrointestinal Endoscopy},
volume = {101},
number = {5, Supplement },
pages = {S729-S756},
year = {2025},
note = {ASGE Abstracts - DDW 2025},
issn = {0016-5107},
doi = {https://doi.org/10.1016/j.gie.2025.03.150},
url = {https://www.sciencedirect.com/science/article/pii/S0016510725003190}
}
@article{DWIVEDI2024102750,
title = {“Real impact”: Challenges and opportunities in bridging the gap between research and practice – Making a difference in industry, policy, and society},
journal = {International Journal of Information Management},
volume = {78},
pages = {102750},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102750},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223001317},
author = {Yogesh K. Dwivedi and Anand Jeyaraj and Laurie Hughes and Gareth H. Davies and Manju Ahuja and Mousa Ahmed Albashrawi and Adil S. Al-Busaidi and Salah Al-Sharhan and Khalid Ibrahim Al-Sulaiti and Levent Altinay and Shem Amalaya and Sunil Archak and María Teresa Ballestar and Shonil A. Bhagwat and Anandhi Bharadwaj and Amit Bhushan and Indranil Bose and Pawan Budhwar and Deborah Bunker and Alexandru Capatina and Lemuria Carter and Ioanna Constantiou and Crispin Coombs and Tom Crick and Csaba Csáki and Yves Darnige and Rahul Dé and Rick Delbridge and Rameshwar Dubey and Robin Gauld and Ravi Kumar Gutti and Marié Hattingh and Arve Haug and Leeya Hendricks and Airo Hino and Cathy H.C. Hsu and Netta Iivari and Marijn Janssen and Ikram Jebabli and Paul Jones and Iris Junglas and Abhishek Kaushik and Deepak Khazanchi and Mitsuru Kodama and Sascha Kraus and Vikram Kumar and Christian Maier and Tegwen Malik and Machdel Matthee and Ian P. McCarthy and Marco Meier and Bhimaraya Metri and Adrian Micu and Angela-Eliza Micu and Santosh K. Misra and Anubhav Mishra and Tonja Molin-Juustila and Leif Oppermann and Nicholas O’Regan and Abhipsa Pal and Neeraj Pandey and Ilias O. Pappas and Andrew Parker and Kavita Pathak and Daniel Pienta and Ariana Polyviou and Ramakrishnan Raman and Samuel Ribeiro-Navarrete and Paavo Ritala and Michael Rosemann and Suprateek Sarker and Pallavi Saxena and Daniel Schlagwein and Hergen Schultze and Chitra Sharma and Sujeet Kumar Sharma and Antonis Simintiras and Vinay Kumar Singh and Hanlie Smuts and John Soldatos and Manoj Kumar Tiwari and Jason Bennett Thatcher and Cristina Vanberghen and Ákos Varga and Polyxeni Vassilakopoulou and Viswanath Venkatesh and Giampaolo Viglia and Tim Vorley and Michael Wade and Paul Walton},
keywords = {Academic impact, Implications for practice, Relevance, Research benefits, Research contribution, Research impact},
abstract = {Achieving impact from academic research is a challenging, complex, multifaceted, and interconnected topic with a number of competing priorities and key performance indicators driving the extent and reach of meaningful and measurable benefits from research. Academic researchers are incentivised to publish their research in high-ranking journals and academic conferences but also to demonstrate the impact of their outputs through metrics such as citation counts, altmetrics, policy and practice impacts, and demonstrable institutional decision-making influence. However, academic research has been criticized for: its theoretical emphasis, high degree of complexity, jargon-heavy language, disconnect from industry and societal needs, overly complex and lengthy publishing timeframe, and misalignment between academic and industry objectives. Initiatives such as collaborative research projects and technology transfer offices have attempted to deliver meaningful impact, but significant barriers remain in the identification and evaluation of tangible impact from academic research. This editorial focusses on these aspects to deliver a multi-expert perspective on impact by developing an agenda to deliver more meaningful and demonstrable change to how “impact” can be conceptualized and measured to better align with the aims of academia, industry, and wider society. We present the 4D model - Design, Deliver, Disseminate, and Demonstrate - to provide a structured approach for academia to better align research endeavors with practice and deliver meaningful, tangible benefits to stakeholders.}
}
@article{DOCANTO2024127320,
title = {Analyzing and predicting the response of the signal grass seed crop to plant nitrogen status},
journal = {European Journal of Agronomy},
volume = {160},
pages = {127320},
year = {2024},
issn = {1161-0301},
doi = {https://doi.org/10.1016/j.eja.2024.127320},
url = {https://www.sciencedirect.com/science/article/pii/S1161030124002417},
author = {Marcos Weber {do Canto} and Taise Robinson Kunrath and Antonio Carlos {Saraiva da Costa} and Marco {dos Santos Martinez} and Gleice Menezes {de Almeida} and Hugo Zeni Neto and João Luiz Pratt Daniel},
keywords = {Nitrogen status indicators, Nitrogen, Nitrogen remobilization, Seed growth},
abstract = {Nitrogen (N) deficiency has detrimental effects on productivity and the profit of producers in areas where signal grass [Urochloa decumbens (Stapf) R.D. Webster (syn. Brachiaria decumbens Stapf.)] cv. Basilisk is grown for seed production. The objective of this paper was to clarify the effects of indicators of signal grass plant N status on seed yield (SY), SY components, yield formation, seed quality, panicle growth parameters, and remobilization of vegetative N on seed growth. Germinable pure SY, harvest index (HI), and N harvest index (NHI) were also measured. Different rates of N fertilizer application (0, 50, 100, and 150 kg ha−1) were applied after the cleaning cut to both the first crop (October - January) and the second crop (February - May) in 2010–2011 and 2011–2012, on a sandy loam soil representative of soils used for seed production in Brazil. Although the N nutrition index (NNI) increased at key developmental stages, the highest values were near to 0.85. This suggests that all crops were maintained under N-limiting conditions. In N-limited crops, a strong relationship was detected between NNI and accumulated N deficit throughout the study period with relative SY. A low NNI after the cleaning cut was found to restrict fertile tiller number (FTN), spikelets per panicle, and spikelet density m−2 measured at anthesis. In all crops, at harvest, NNI at anthesis increased germinable pure SY, FTN, number of seeds per panicle, HI, NHI, and amount of remobilized N to seeds, but not thousand seed weight (TSW), seed germination, panicle dry matter (DM) accumulation rate, and individual seed growth rate. Regression analyses suggested that the NNI, accumulated N deficit, aboveground plant biomass (AGPB), and N content were better associated with relative SY than with plant N concentration (PNC). The study shows that the NNI quantifies the intensity and duration of N deficiency in signal grass and should be considered in research studies and for application in seed production fields to improve N fertilization recommendations.}
}
@article{2024100875,
title = {Full Issue PDF},
journal = {JACC: Advances},
volume = {3},
number = {2},
pages = {100875},
year = {2024},
issn = {2772-963X},
doi = {https://doi.org/10.1016/S2772-963X(24)00053-X},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X2400053X}
}
@article{2025I,
title = {Full Issue PDF},
journal = {JACC: Asia},
volume = {5},
number = {1, Part 1},
pages = {I-CXXVIII},
year = {2025},
issn = {2772-3747},
doi = {https://doi.org/10.1016/S2772-3747(24)00531-3},
url = {https://www.sciencedirect.com/science/article/pii/S2772374724005313}
}
@article{GIORDANO2025104186,
title = {Decomposing maintenance actions into sub-tasks using natural language processing: A case study in an Italian automotive company},
journal = {Computers in Industry},
volume = {164},
pages = {104186},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104186},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524001143},
author = {Vito Giordano and Gualtiero Fantoni},
keywords = {Natural language processing, Text mining, Maintenance work order, Industrial applications, Association rule mining, Large language model},
abstract = {Industry 4.0 has led to a huge increase in data coming from machine maintenance. At the same time, advances in Natural Language Processing (NLP) and Large Language Models provide new ways to analyse this data. In our research, we use NLP to analyse maintenance work orders, and specifically the descriptions of failures and the corresponding repair actions. Many NLP studies have focused on failure descriptions for categorising them, extracting specific information about failure, or supporting failure analysis methodologies (such as FMEA). Whereas, the analysis of repair actions and its relationship with failure remains underexplored. Addressing this gap, our study makes three significant contributions. Firstly, we focused on the Italian language, which presents additional challenges due to the dominance of NLP systems that are mainly designed for English. Secondly, it proposes a method for automatically subdividing a repair action into a set of sub-tasks. Lastly, it introduces an approach that employs association rule mining to recommend sub-tasks to maintainers when addressing failures. We tested our approach with a case study from an automotive company in Italy. The case study provides insights into the current barriers faced by NLP applications in maintenance, offering a glimpse into the future opportunities for smart maintenance systems.}
}
@article{FERRAG20251,
title = {Generative AI in cybersecurity: A comprehensive review of LLM applications and vulnerabilities},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {5},
pages = {1-46},
year = {2025},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2025.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667345225000082},
author = {Mohamed Amine Ferrag and Fatima Alwahedi and Ammar Battah and Bilel Cherif and Abdechakour Mechri and Norbert Tihanyi and Tamas Bisztray and Merouane Debbah},
keywords = {Generative AI, LLM, Transformer security, Cybersecurity},
abstract = {This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions. We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques. Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses. We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research. In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response. Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats.}
}
@article{TREANTA2024100288,
title = {Efficiency criteria and dual techniques for some nonconvex multiple cost minimization models},
journal = {IFAC Journal of Systems and Control},
volume = {30},
pages = {100288},
year = {2024},
issn = {2468-6018},
doi = {https://doi.org/10.1016/j.ifacsc.2024.100288},
url = {https://www.sciencedirect.com/science/article/pii/S246860182400049X},
author = {Savin Treanţă and Ramona-Manuela Calianu},
keywords = {Efficiency criteria, Dual techniques, Nonconvex multiple cost minimization models},
abstract = {In this study, we investigate a class of multi-objective variational control problems governed by nonconvex simple integral functionals. Concretely, we establish and prove (necessary and sufficient) efficiency criteria and dual techniques for some nonconvex multiple cost minimization models. To this aim, we extend and use the concept of (Ψ,ω)-invexity to the case of multi-objective variational control problems. Thereafter, by assuming (Ψ,ω)-invexity, (strictly) (Ψ,ω)-pseudoinvexity and/or (Ψ,ω)-quasiinvexity of the involved functionals, we state the sufficient efficiency criteria and associate a dual problem for the considered model.}
}
@article{ALKFAIRY2024100459,
title = {Factors impacting users’ willingness to adopt and utilize the metaverse in education: A systematic review},
journal = {Computers in Human Behavior Reports},
volume = {15},
pages = {100459},
year = {2024},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100459},
url = {https://www.sciencedirect.com/science/article/pii/S2451958824000927},
author = {Mousa Al-kfairy and Soha Ahmed and Ashraf Khalil},
keywords = {Metaverse education, Intention to use, Acceptance, Systematic review, Information systems theories},
abstract = {Purpose
This study explores the factors influencing the adoption and acceptance of Metaverse technologies in educational settings. Despite the growing interest in immersive educational environments provided by the Metaverse, there is a lack of comprehensive understanding regarding the elements that affect user engagement and acceptance. This paper aims to bridge this gap through a systematic review of empirical studies that apply Information Systems theories such as TAM, UTAUT, TPB, and their extensions.
Methods
A total of 35 empirical studies were analyzed using a methodical review approach. The research methodologies employed in these studies include surveys, structural equation modeling, and interviews, providing a broad spectrum of data on how different factors influence educational outcomes in the Metaverse.
Results
The findings reveal that user adoption of the Metaverse in educational contexts is influenced by multiple factors at individual, technological, and environmental levels. Key factors identified include effort expectancy, behavioral intention, self-efficacy, enjoyment, and immersion. These factors are subject to moderating effects, suggesting that the dynamics of Metaverse adoption are highly context-dependent.
Conclusion
The insights gained from this review provide valuable guidelines for educators, policymakers, and technology developers aiming to effectively integrate Metaverse technologies into educational frameworks. The study also outlines limitations and suggests directions for future research, highlighting the need for further investigations into the longitudinal impacts and cultural adaptability of Metaverse applications in education.}
}
@incollection{GALITSKY2025283,
title = {Chapter 8 - Identifying large language model hallucinations in health communication},
editor = {Boris Galitsky},
booktitle = {Healthcare Applications of Neuro-Symbolic Artificial Intelligence},
publisher = {Academic Press},
pages = {283-329},
year = {2025},
isbn = {978-0-443-30046-2},
doi = {https://doi.org/10.1016/B978-0-443-30046-2.00012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300462000120},
author = {Boris Galitsky},
keywords = {Large language model hallucination, fact-checking, handling inconsistent verification sources, collaborative iterative mode, syntax-semantic alignment},
abstract = {Large language models (LLMs) sometimes generate texts plagued with inaccuracies and fabricated information. We present a fact-checking system known as “Truth-O-Meter” which detects erroneous facts by cross-referencing generated content with information from the web and reputable sources and then offers appropriate corrections. We employ text mining and web mining techniques to pinpoint accurate corresponding sentences and to employ a syntactic and semantic generalization process to enhance content quality. To effectively handle the challenges posed by inconsistent information sources during fact-checking, we employ an argumentation-analysis framework based on defeasible logic programming. In a comparative evaluation with competitive approaches that rely on reinforcement learning integrated with LLM or token-based hallucination detection, our fact-checking engine demonstrates significant enhancements in the factual accuracy and meaningfulness of LLM-generated content.11https://github.com/bgalitsky/Truth-O-Meter-Making-ChatGPT-Truthful.}
}
@incollection{GREEN20241,
title = {Chapter One - Narrative transportation: How stories shape how we see ourselves and the world},
editor = {Bertram Gawronski},
series = {Advances in Experimental Social Psychology},
publisher = {Academic Press},
volume = {70},
pages = {1-82},
year = {2024},
issn = {0065-2601},
doi = {https://doi.org/10.1016/bs.aesp.2024.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065260124000145},
author = {Melanie C. Green and Markus Appel},
keywords = {Narrative, Story, Transportation, Character identification, Attitudes, Persuasion, Emotion, Belongingness, Immersion, Theory of mind},
abstract = {Scientific interest in the processing and effects of narrative information has substantially increased in recent years. The focus of this chapter is on narrative transportation, an experiential state of immersion in which all mental processes are concentrated on the events occurring in the narrative. We describe and integrate interdisciplinary advances in the study of narrative transportation. After an introduction of the concept and related approaches, we outline antecedents in terms of story factors, individual differences, situational variables, and related interactions. In the following sections, we introduce processes and effects that are facilitated by stories and narrative transportation. This includes research on persuasion, misinformation and its correction, self and identity, social cognitive skills, and the fulfillment of belongingness needs. We close with an outlook on the role of technology and artificial intelligence, meaning making, and climate change communication as emerging and future directions.}
}
@incollection{2024552,
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {552-598},
year = {2024},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.09001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895090015}
}
@article{NGUYEN2026112953,
title = {A review of instruction-guided image editing},
journal = {Engineering Applications of Artificial Intelligence},
volume = {163},
pages = {112953},
year = {2026},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.112953},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625029847},
author = {Thanh Tam Nguyen and Zhao Ren and Trinh Pham and Phi Le Nguyen and Quoc Viet Hung Nguyen and Hongzhi Yin},
keywords = {Instruction-guided image editing, Artificial intelligence systems, Large language models, Multimodal learning frameworks, Generative adversarial networks, Diffusion models},
abstract = {The rapid advancement of artificial intelligence (AI) systems such as large language models (LLMs) and multimodal learning frameworks has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-guided editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey reviews how AI-implemented instruction-guided image editing models – including approaches rooted in generative adversarial networks and diffusion models – empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we examine multimodal integration for fine-grained content control, compare existing literature, and highlight how these AI applications support creative visual storytelling, design workflows, and multimedia production. We also identify key challenges to stimulate further research. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instructional-editing.}
}
@article{BECK2024100524,
title = {Boundary work and high-reliability organizing in interorganizational collaborations},
journal = {Information and Organization},
volume = {34},
number = {3},
pages = {100524},
year = {2024},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2024.100524},
url = {https://www.sciencedirect.com/science/article/pii/S1471772724000241},
author = {Tammy E. Beck and Stephanie T. Solansky and Daniel J. Davis and Karen Ford-Eickhoff and Donde Plowman},
keywords = {High reliability organizing, HRO collaboration, Boundary work, Boundary object},
abstract = {Consider the massive recovery response that included over 25,000 professionals and volunteers representing more than 120 organizations tasked with locating both human remains and vehicle debris following the Columbia Space Shuttle tragedy. Despite the daunting scope of the initial search area – 2.28 million acres of land – participating members were successful in their efforts to achieve the collective's goals. We contend that the response effort was effective because relatively disparate organizations and governmental agencies came together and ultimately exemplified the hallmarks of high reliability organizing (HRO). Our study explores how the transition in boundaries made this possible. Using interview and secondary data from our case study, we explore how individuals engaged in boundary work that facilitated boundary transformation. Specifically, we document how individuals interacted with a data visualization system to temper the physical, social, temporal, and scope boundary tensions initially present following the disaster. Amidst an emergent, messy, and complex setting, the interaction with a boundary object allowed for unity in diversity of participating organizations, a common language through mapping, a form of trichordal temporal and rapid sensemaking, and a foundation for dynamic decision making. Therefore, our study yields critical insights into how organizational members engage in boundary work to aid HRO collaborations.}
}
@article{SANCHEZWELLS2025126757,
title = {Truck-multidrone same-day delivery strategies: On-road resupply vs depot return},
journal = {Expert Systems with Applications},
volume = {272},
pages = {126757},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126757},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425003793},
author = {David Sanchez-Wells and José L. Andrade-Pineda and Pedro L. Gonzalez-R},
keywords = {Truck-Multidrone Logistics, Genetic Algorithm, Makespan, Truck Mileage, Last-mile Delivery, Resupply},
abstract = {This paper explores an enhanced two-waved same-day delivery (SDD) system that leverages a mothership truck equipped with multiple drones supported by an auxiliary “resupply” truck. Under standard SDD operations, this mothership truck, also capable of performing deliveries, must return to the depot to reload, incurring extra travel time and mileage. In contrast, the proposed resupply strategy enables the second delivery wave by dispatching a secondary vehicle to meet the mothership truck on-road, reloading parcels without interrupting ongoing deliveries by the drones. A single unified routing framework, the Genetic Algorithm with Iterated Estimations for Resupply (GAIER), is presented to optimise both strategies under two selectable criteria: minimising total service time or total truck mileage. In tests with benchmark networks of different sizes (20, 50, and 75 nodes), incorporating a resupply truck reduced every selected criterion when compared to the strategy where the mothership vehicle returns to the depot. Subsequent comparative analysis points an average reduction of 17 % in service time and 21 % in truck mileage while statistical analyses support the strategy choice significancy, confirming resupply strategy’s potential for cost savings and reduced environmental impact. These findings bolster our proposition that incorporating a resupply truck into hybrid truck-multidrone systems enhances flexibility in drone delivery scheduling and improves the system’s ability to meet urban demand.}
}
@article{PANGGABEAN2025100412,
title = {How do ChatGPT's benefit–risk-coping paradoxes impact higher education in Taiwan and Indonesia?},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100412},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100412},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000529},
author = {Ellis Mardiana Panggabean and Andri Dayarana K. Silalahi},
keywords = {Artificial intelligence, Benefit-coping-risk, chatgpt in education, fsqca, Education, Technology adoption},
abstract = {The integration of ChatGPT into higher education in Taiwan and Indonesia presents both opportunities and challenges. This integration creates a paradox of benefits and risks that must be carefully managed. While previous studies have explored ChatGPT's applications, its complexities in educational contexts remain partially unaddressed. This study bridges that gap by integrating the Unified Theory of Acceptance and Use of Technology (UTAUT) with Protection Motivation Theory (PMT) to examine ChatGPT's role through a benefit–risk–coping mechanism. Data were collected from higher education users in Taiwan and Indonesia. Structural Equation Modeling (SEM) revealed distinct patterns in the two regions. In Taiwan, perceived severity reduces the intention to use ChatGPT, while self-efficacy fosters adoption. In Indonesia, users emphasize response efficacy and performance expectancy as stronger predictors of usage intention. Task efficiency and performance expectancy enhance usage intention in both settings, with Indonesia showing a stronger link between intention and actual use. Fuzzy sets Qualitative Comparative Analysis (fsQCA) further identifies diverse configurations for actual usage and disusage of ChatGPT. Task efficiency and performance expectancy emerge as key usage drivers in both contexts. Disusage in Taiwan primarily stems from task inefficiency, whereas multiple factors—including low self-efficacy—hinder adoption in Indonesia. These findings provide practical insights for higher education institutions, guiding strategies to optimize ChatGPT's benefits, minimize risks, and ensure its responsible use in educational settings across Taiwan and Indonesia.}
}
@incollection{VINTER2025,
title = {Artificial intelligence in GPCR drug discovery: A paradigm shift in computational pharmacology},
booktitle = {Reference Module in Chemistry, Molecular Sciences and Chemical Engineering},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-12-409547-2},
doi = {https://doi.org/10.1016/B978-0-443-29808-0.00047-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443298080000479},
author = {Adrijana Vinter and Ivan Grgičević},
keywords = {G protein-coupled receptors, Artificial intelligence, Drug discovery, Machine learning, Deep learning, Graph neural networks, Reinforcement learning, AlphaFold2, Biased signaling, Allosteric modulators, Multi-omics integration, Virtual screening, De novo drug design, Precision medicine, Explainable AI (XAI)},
abstract = {This Chapter explores the transformative role of artificial intelligence in GPCR-targeted drug discovery, highlighting how machine learning, deep learning, and reinforcement learning are reshaping ligand screening, structure prediction, and personalized medicine. AI models such as AlphaFold2, graph neural networks, and generative adversarial networks have significantly accelerated hit identification, improved functional selectivity, and enabled allosteric modulator discovery. Integrated with multi-omics data, AI enhances the precision and efficiency of therapeutic development while reducing cost and time. The Chapter also addresses the challenges of data scarcity, model interpretability, and experimental validation, offering potential solutions through explainable AI and hybrid workflows. These advancements position AI not just as a supportive tool but as a central driver in next-generation GPCR pharmacology and precision drug design.}
}
@incollection{GAUR2026175,
title = {Chapter 10 - Autonomous medical diagnosis: How to balance accuracy and accountability?},
editor = {Loveleen Gaur and Ajith Abraham},
booktitle = {Generative Artificial Intelligence and Ethics for Healthcare},
publisher = {Academic Press},
pages = {175-196},
year = {2026},
isbn = {978-0-443-33124-4},
doi = {https://doi.org/10.1016/B978-0-443-33124-4.00007-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443331244000072},
author = {Loveleen Gaur and Ajith Abraham},
keywords = {Accountability, Accuracy, AI diagnostics, Autonomous medical diagnosis, Diagnostic errors, Ethical healthcare, Patient trust, Transparency},
abstract = {The rise of autonomous medical diagnosis, powered by artificial intelligence (AI) and machine learning, is revolutionizing how diseases are detected and treated in healthcare. This chapter delves into the intricate balance between accuracy and accountability in AI diagnostics. While AI offers substantial potential to enhance diagnostic precision and efficiency, it also introduces ethical challenges and accountability concerns. The critical role of accuracy in medical diagnostics is explored, alongside the challenges of data quality and algorithmic limitations that impact performance. Transparency is vital for building trust among patients and healthcare providers, necessitating techniques that enhance the interpretability of AI systems. Accountability for diagnostic errors is a pressing issue, prompting discussions on who bears responsibility when AI systems fail. The chapter highlights case studies that illustrate accountability challenges and legal implications surrounding AI diagnostics. As AI continues to evolve, the need for fairness, safety protocols, and continuous monitoring becomes paramount to safeguarding patient welfare. The successful integration of AI into clinical practice relies on collaboration among stakeholders and the establishment of guidelines that uphold ethical standards. Looking ahead, the chapter addresses emerging trends in AI diagnostics and evolving standards for accuracy and accountability. Ultimately, a sustainable and ethical future in autonomous medical diagnosis hinges on a thoughtful balance between innovation and responsibility, ensuring effective healthcare solutions that prioritize patient safety and trust.}
}
@article{DENG2025105224,
title = {Does ChatGPT enhance student learning? A systematic review and meta-analysis of experimental studies},
journal = {Computers & Education},
volume = {227},
pages = {105224},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105224},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524002380},
author = {Ruiqi Deng and Maoli Jiang and Xinlu Yu and Yuyan Lu and Shasha Liu},
keywords = {Teaching/learning strategies, Improve classroom teaching, Elementary education, Secondary education, Post-secondary education},
abstract = {Chat Generative Pre-Trained Transformer (ChatGPT) has generated excitement and concern in education. While cross-sectional studies have highlighted correlations between ChatGPT use and learning performance, they fall short of establishing causality. This review examines experimental studies on ChatGPT's impact on student learning to address this gap. A comprehensive search across five databases identified 69 articles published between 2022 and 2024 for analysis. The findings reveal that ChatGPT interventions are predominantly implemented at the university level, cover various subject areas focusing on language education, are integrated into classroom environments as part of regular educational practices, and primarily involve direct student use of ChatGPT. Overall, ChatGPT improves academic performance, affective-motivational states, and higher-order thinking propensities; it reduces mental effort and has no significant effect on self-efficacy. However, methodological limitations, such as the lack of power analysis and concerns regarding post-intervention assessments, warrant cautious interpretation of results. This review presents four propositions from the findings: (1) distinguish between the quality of ChatGPT outputs and the positive effects of interventions on academic performance by shifting from well-defined problems in post-intervention assessments to more complex, project-based assessments that require skill demonstration, adopting proctored assessments, or incorporating metrics such as originality alongside quality; (2) evaluate long-term impacts to determine whether the positive effects on affective-motivational states are sustained or merely owing to novelty effect; (3) prioritise objective measures to complement subjective assessments of higher-order thinking; and (4) use power analysis to determine adequate sample sizes to avoid Type II errors and provide reliable effect size estimates. This review provides valuable insights for researchers, instructors, and policymakers evaluating the effectiveness of generative AI integration in educational practice.}
}
@article{HE2025103076,
title = {Personal data protection in China: Progress, challenges and prospects in the age of big data and AI},
journal = {Telecommunications Policy},
pages = {103076},
year = {2025},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2025.103076},
url = {https://www.sciencedirect.com/science/article/pii/S0308596125001739},
author = {Miao He and Yongfang Chen},
keywords = {Personal data, Legal protection, Privacy, Big data},
abstract = {In the big data and AI era, personal data protection is legally and academically challenging. Although China has made great progress in this regard by issuing a range of laws and sector-specific regulations, as well as bringing public interest litigation to safeguard personal data, challenges persist. However, there are few systematic studies on the progress and challenges in protecting personal data under the rule of law in China. In order to fill this gap, this article examine the evolution of personal data protection in the context of big data and AI in China, focusing on its constitutional foundations, legal improvements, and judicial applications and identify the unresolved challenges of personal data protection. Based on these, protection methods related to personal data are further proposed and discussed, such as refining the informed consent rule, bolstering personal data and risk governance via grading, contextual and privacy impact assessments mechanisms, and optimizing the Data Protection Authority system by establishing consultation and independent oversight bodies. By adapting these strategies to the unique conditions in China, this article proposes a holistic approach to better balance among personal data protection, security, and economic development. The findings of this article also hold value for developing countries seeking to align personal data governance with global standards, thereby contributing to a more robust international data protection system.}
}
@article{2025100419,
title = {Pathology Visions 2024 Overview},
journal = {Journal of Pathology Informatics},
volume = {16},
pages = {100419},
year = {2025},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2025.100419},
url = {https://www.sciencedirect.com/science/article/pii/S215335392500001X}
}
@article{LI2026103836,
title = {Cognitive assessment in neurodegenerative diseases: A review of interactive methods},
journal = {Information Fusion},
volume = {127},
pages = {103836},
year = {2026},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103836},
url = {https://www.sciencedirect.com/science/article/pii/S156625352500898X},
author = {He Li and Guanci Yang and Zhidong Su and Biao Xu and Bingqi Hu and Ling He},
keywords = {Cognitive ability, Cognitive screening instruments, Sensor data-driven cognitive assessment, Mild cognitive impairment, Multimodal fusion, Task-based diagnosis},
abstract = {The abnormal decline in cognitive abilities, a hallmark of neurodegenerative diseases such as Alzheimer’s and Parkinson’s, is often misattributed to normal aging, leading to missed opportunities for early intervention. Interactive cognitive assessment methods are therefore crucial for the timely detection of cognitive impairment, with applications spanning clinical screening, patient monitoring, community screening, and research on cognitive function changes. This article focuses on interactive cognitive assessment methods, categorizing them into two paradigms based on the assessment mechanism: brief cognitive screening instruments that rely on manual scoring, and sensor data-driven automatic cognitive assessment methods. First, we review the progress in brief cognitive screening instruments from four perspectives, based on their evaluation dimensions and applicable populations: multidimensional cognitive comprehensive assessment instruments, specialized cognitive assessment instruments, informant-collaborative cognitive assessment instruments, and cognitive assessment instruments for specific populations. Subsequently, we categorize sensor data-driven automatic cognitive assessment methods into seven types based on the data sources and collection methods, namely those based on speech, eye tracking, gait performance, calligraphy and painting tasks, smart home, gamified, and virtual reality, and separately analyze advances for each type. We then summarize critical aspects of their clinical translation, including validation challenges, primary clinical scenarios, and implementation barriers. Finally, we outline the overarching challenges and future research directions for interactive cognitive assessment methods.}
}
@article{LU2025103514,
title = {Integrating language into medical visual recognition and reasoning: A survey},
journal = {Medical Image Analysis},
volume = {102},
pages = {103514},
year = {2025},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2025.103514},
url = {https://www.sciencedirect.com/science/article/pii/S1361841525000623},
author = {Yinbin Lu and Alan Wang},
keywords = {Vision language Model, Medical imaging analysis, Multimodal learning, Visual recognition and reasoning},
abstract = {Vision-Language Models (VLMs) are regarded as efficient paradigms that build a bridge between visual perception and textual interpretation. For medical visual tasks, they can benefit from expert observation and physician knowledge extracted from textual context, thereby improving the visual understanding of models. Motivated by the fact that extensive medical reports are commonly attached to medical imaging, medical VLMs have triggered more and more interest, serving not only as self-supervised learning in the pretraining stage but also as a means to introduce auxiliary information into medical visual perception. To strengthen the understanding of such a promising direction, this survey aims to provide an in-depth exploration and review of medical VLMs for various visual recognition and reasoning tasks. Firstly, we present an introduction to medical VLMs. Then, we provide preliminaries and delve into how to exploit language in medical visual tasks from diverse perspectives. Further, we investigate publicly available VLM datasets and discuss the challenges and future perspectives. We expect that the comprehensive discussion about state-of-the-art medical VLMs will make researchers realize their significant potential.}
}
@article{CHEN2025125322,
title = {A novel air combat target threat assessment method based on three-way decision and game theory under multi-criteria decision-making environment},
journal = {Expert Systems with Applications},
volume = {259},
pages = {125322},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125322},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424021894},
author = {Qihong Chen and Qingsong Zhao and Zhigang Zou and Qunyou Qian and Junuo Zhou and Renpeng Yuan},
keywords = {Threat assessment, Air combat target, Three-way decision, Multi-criteria decision-making, Game theory},
abstract = {Threat assessment of air combat target (TAoACT) is an important issue in air combat decision-making. Current methodologies for threat assessment are insufficient in capturing the systematic and time-sensitive characteristics of air combat, lacking a comprehensive analysis for the complex threat of air combat target (ACT). To address this issue, we propose a multi-criteria three-way decision-making (MCTWD) method based on game theory for TAoACT. The proposed method comprises three key components: (1) a single-criteria three-way decision (TWD) model based on the relative utility function with a new multi-criteria attribute system; (2) a multi-criteria aggregation mechanism based on game theory to analyze and integrate diverse decision outcomes derived from various criteria; and (3) decision conditions based on joint criteria to rank and classify ACTs. The effectiveness and applicability of the proposed method have been validated through three numerical examples. Comparative analysis results indicate that the proposed method demonstrates superior performance in ranking and classification when compared to five widely-used MCTWD methods.}
}