@article{SHARMA2024102500,
title = {Intra-pulse modulation discrimination using a self-supervised attention-driven CNN-BiLSTM-VAE combination},
journal = {Physical Communication},
volume = {67},
pages = {102500},
year = {2024},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2024.102500},
url = {https://www.sciencedirect.com/science/article/pii/S1874490724002180},
author = {Purabi Sharma and Kandarpa Kumar Sarma},
keywords = {Intra-pulse modulation, CNN, Self-attention, BiLSTM, Variational autoencoder},
abstract = {Identification of intra-pulse modulation (IPM) of radar signals is a crucial part of contemporary electronic support systems and electronic intelligence reconnaissance. Artificial intelligence (AI)-based methods can be very effective in recognising the IPM of radar signals. In this direction, an automatic method is proposed for recognising a few IPMs of radar signals based on continuous wavelet transform (CWT) and a hybrid model of self-attention (SA)-aided convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM). Firstly, time–frequency attributes of different radar signals are obtained using CWT, and thereafter CNN-SA-BiLSTM is utilised for feature extraction from the 2D scalograms formed by the time–frequency components. The CNN extracts features from the scalograms, SA enhances the discriminative power of the feature map, and BiLSTM detects radar signals based on these features. Additionally, the study addresses real-world data imbalance issues by incorporating a generative AI model, namely the Variational Autoencoder (VAE). The VAE-based approach effectively mitigates challenges arising from data imbalance situations. This method is tested at varying noise levels to give a proper representation of the actual electronic warfare environment. The simulation results demonstrate that the best overall recognition accuracy of the proposed method is 98.4%, even at low signal-to-noise ratios (SNR).}
}
@article{GHOTB2024122008,
title = {Scheduling of log logistics using a metaheuristic approach},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122008},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122008},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423025101},
author = {Salar Ghotb and Taraneh Sowlati and Joel Mortyn},
keywords = {Decomposition approach, Scheduling, Synchronization, Heterogeneous trucks, Simulated annealing, Taguchi method},
abstract = {An efficient log logistics plan results in log procurement cost savings. In practice, log logistics presents complexities such as synchronization of different machines, sorting of logs, and compatibility requirements. To address these complexities, this research proposes a decomposition approach for optimization of log logistics considering synchronization of log loaders and heterogeneous trucks at both cut blocks and sort yards. In the first phase, the daily number of truckloads between cut blocks and sort yards for each trucking contractor is determined, while allocation of the truckloads to compatible trucks and detailed routing and scheduling decisions are addressed in the second phase. Furthermore, a simulated annealing algorithm is developed for the second phase to obtain the schedules for large-sized problems. Additionally, the parameters of the algorithm are tuned using the Taguchi method. The algorithm is then applied to a case of a large Canadian forest company in British Columbia, Canada with a 1-month planning horizon. The results show that the proposed solution approach can successfully satisfy synchronization requirements and generate detailed schedules in a reasonable time. Also, cost savings for contractors are possible by assigning overtime rather than utilizing more trucks to fulfill their transportation tasks.}
}
@article{HAN2024103946,
title = {Do mobile device icons help or hurt? Evidence from empirical analyses and design via interpretable machine learning},
journal = {Information & Management},
volume = {61},
number = {3},
pages = {103946},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2024.103946},
url = {https://www.sciencedirect.com/science/article/pii/S0378720624000284},
author = {Maoxin (Molson) Han},
keywords = {Mobile device icons, Review helpfulness, Perceived effort, Empirical analyses, Interpretable machine learning},
abstract = {Although the extant literature demonstrates that mobile device icons change consumers’ cognition of review helpfulness, it reports contradictory findings: displaying mobile device icons either helps or hurts review helpfulness. Drawing on the instability of peripheral routes (ELM) and perceived effort, we found that the impact of mobile device icons on review helpfulness is contingent on review writing effort, represented by review length. Econometric and experimental analyses ensured the external and internal validity of results, respectively. Moreover, we applied interpretable machine learning to designing mobile device icons varying with review writing effort, aiming at maximizing their effects on review helpfulness.}
}
@article{AZZONE2023101841,
title = {Evaluation of sight deposits and central bank digital currency},
journal = {Journal of International Financial Markets, Institutions and Money},
volume = {88},
pages = {101841},
year = {2023},
issn = {1042-4431},
doi = {https://doi.org/10.1016/j.intfin.2023.101841},
url = {https://www.sciencedirect.com/science/article/pii/S1042443123001099},
author = {Michele Azzone and Emilio Barucci},
keywords = {Central bank digital currency, Bank deposit, Interest rate, Bank run},
abstract = {We provide a market-based evaluation of sight deposits of banks when Central Bank Digital Currency (CBDC) is issued. We investigate the effects of different adoption rates (moderate, large or capped adoption), of different remuneration schemes and of the possibility of a bank-run. We perform the analysis at the aggregate level for the Euro area and the United States. We show that the effect of CBDC on deposit market value is small unless a large adoption rate is considered. The remuneration scheme plays a significant role only in the moderate/capped adoption scenario in relative terms and in a high interest rate environment. Instead, the possibility of a bank-run calibrated on the experience of the Euro area and of Greece during the debt crisis has little effect on the value of deposits even if a CBDC is introduced.}
}
@article{WU2026101287,
title = {Exploring the impact of artificial intelligence on business talent development in higher education:A systematic literature review and research agenda},
journal = {The International Journal of Management Education},
volume = {24},
number = {1},
pages = {101287},
year = {2026},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2025.101287},
url = {https://www.sciencedirect.com/science/article/pii/S1472811725001570},
author = {Qinglan Wu and Lanzhen Chen and Minwei Chen and Yangjie Huang},
keywords = {Artificial intelligence, Big data, Business education, Management education},
abstract = {Against the strategic backdrop of the digital and intelligent transformation of global higher education, the emerging cluster of technologies with artificial intelligence (AI) at its core is fundamentally reshaping the operational logic and value ecosystem of business education systems. To comprehensively understand the current research landscape and progress of AI and business talent development, this study conducts a systematic literature review, retrieving 192 research articles published between 2015 and 2024 from the Scopus and Web of Science databases. Based on descriptive statistics and CiteSpace bibliometric analysis, this study summarizes the major progress and key findings of the past decade across four domains of business talent development in higher education. The study further provides practical implications for business school administrators and faculty, highlighting insufficient attention to macro-level issues such as core AI competencies, curriculum restructuring, and institutional resource support. It also notes the lack of in-depth reflection on the mechanisms through which AI is embedded in business talent development. In addition, through further analysis of theoretical frameworks and research methods, this study suggests that future academic research should explore emerging frontier topics such as artificial general intelligence and quantum algorithms, promote interdisciplinary integration of business education with neuroscience, social sciences, and environmental science, place greater emphasis on longitudinal research, and adopt research paradigms driven by both data and mechanisms.}
}
@article{XIAO2024102679,
title = {UI semantic component group detection: Grouping UI elements with similar semantics in mobile graphical user interface},
journal = {Displays},
volume = {83},
pages = {102679},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2024.102679},
url = {https://www.sciencedirect.com/science/article/pii/S014193822400043X},
author = {Shuhong Xiao and Yunnong Chen and Yaxuan Song and Liuqing Chen and Lingyun Sun and Yankun Zhen and Yanfang Chang and Tingting Zhou},
keywords = {UI element grouping, UI object detection, UI-related software application, Transformer},
abstract = {Texts, widgets, and images on a UI page do not work separately. Instead, they are partitioned into groups to achieve certain interaction functions or visual information. Existing studies on UI elements grouping mainly focus on a specific single UI-related software engineering task, and their groups vary in appearance and function. In this case, we propose our semantic component groups that pack adjacent text and non-text elements with similar semantics. In contrast to those task-oriented grouping methods, our semantic component group can be adopted for multiple UI-related software tasks, such as retrieving UI perceptual groups, improving code structure for automatic UI-to-code generation, and generating accessibility data for screen readers. To recognize semantic component groups on a UI page, we propose a robust, deep learning-based vision detector, UISCGD, which extends the SOTA deformable-DETR by incorporating UI element color representation and a learned prior on group distribution. The model is trained on our UI screenshots dataset of 1988 mobile GUIs from more than 200 apps in both iOS and Android platforms. The evaluation shows that our UISCGD achieves 6.1% better than the best baseline algorithm and 5.4 % better than deformable-DETR in which it is based.}
}
@article{GARZA2024e31144,
title = {Photoluminescence research of the graphene quantum dots (GQD) interaction on the zinc oxide (ZnO) surface for application as H2O2 photosensor},
journal = {Heliyon},
volume = {10},
number = {11},
pages = {e31144},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e31144},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024071755},
author = {Rolando Efraín Ramírez Garza and Sara Luisa {Rodríguez de Luna} and Idalia Gómez},
keywords = {Zinc oxide, Graphene quantum dots, Photoluminescence, HO detection, Oxygen-vacancies ZnO},
abstract = {Photoluminescence (PL) spectroscopy is one of the best methods to detect molecules due to its easiness, fast time of analysis and high sensitivity. In addition, zinc oxide (ZnO) possesses good optical properties and particularly PL emission in these materials have been exploited for their potential use as photocatalyst, light harvesting and photosensor. These PL properties enhance when graphene quantum dots (GQD) are added to ZnO. For these reasons, we investigated the PL performance of ZnO-GQD nanocomposites. In one experiment we evaluated the PL emission of solid samples ZnO and ZnO-GQD. In a second experiment, these samples were also evaluated in aqueous phase to investigate the H2O2 effect during an experiment lasting 170 minutes. Both experiments displayed six peaks and they were related to the same PL emission source. The PL emission peak around 415 nm was found to be principal source where GQD are interacting. By varying the GQD amount to low, medium, and high concentration, the effect of H2O2 acted consequently, altering the PL emission during experiment in aqueous phase. An oxygen rich environment (ORE) occurred due to H2O2 which oxides the ZnO surface. Low GQD concentration resulted affected by an ORE weakening the GQD-ZnO contact, decreasing PL emission. In high GQD concentration, H2O2 induced GQD to reach the ZnO surface, increasing the PL emission. Only medium GQD concentration prevented oxidation of ZnO and maintained the PL emission intensity constant. When H2O2 concentration increased, for the medium GQD concentration, an excess of charge by peroxides inhibited the charge transfer from GQD to ZnO. This inhibition produces a quenching of the PL emission.}
}
@article{JI2026116159,
title = {A systematic review of electricity demand for large language models: evaluations, challenges, and solutions},
journal = {Renewable and Sustainable Energy Reviews},
volume = {225},
pages = {116159},
year = {2026},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.116159},
url = {https://www.sciencedirect.com/science/article/pii/S1364032125008329},
author = {Zhenya Ji and Ming Jiang},
keywords = {Electricity consumption, Artificial intelligence, Large language model, Power grid, Energy sustainability},
abstract = {Large language models (LLMs) are revolutionizing technology landscapes, transforming production workflows, and deeply embedding themselves into our daily lives. Their unparalleled capabilities, fueled by extensive training on vast datasets and intensive utilizations, underpin this transformation. However, this achievement comes at a significant cost: the immense electricity demand required for their training and inference poses an urgent and critical challenge. As the reliance on LLMs grows, ensuring a reliable and sustainable power supply becomes paramount for their uninterrupted progress and widespread adoption. This comprehensive review paper thoroughly examines the intricacies of LLMs' lifecycle, focusing on both training and inference stages. It critically assesses various parameters and approaches for accurately estimating their electricity consumption and associated carbon emissions, drawing upon representative statistical data spanning diverse LLM products and task types. By delving into the complexities, the paper uncovers the fundamental challenges in forecasting and fulfilling LLMs’ extensive electricity demand, which are intricately linked to controversies within four bottom-up tiers: soft-and-hardware, data center, power grid, and external societal factors. Crucially, this paper offers a suite of tailored solutions for each tier, aimed at not only addressing the immediate electricity demand challenge but also fostering the long-term sustainability of LLMs. The insights outlined herein endeavor to alleviate the power shortage crisis facing LLMs, paving the way for their widespread adoption while minimizing their environmental impact and contributing to a greener future.}
}
@article{SIINO2025104281,
title = {Integrating Large Language Models into network testbeds: A novel approach for automated experimentation and optimization},
journal = {Journal of Network and Computer Applications},
volume = {243},
pages = {104281},
year = {2025},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2025.104281},
url = {https://www.sciencedirect.com/science/article/pii/S108480452500178X},
author = {Marco Siino and Fabrizio Giuliano and Ilenia Tinnirello},
keywords = {Large Language Models (LLMs), Wireless networking, Networking testbed, Few-shot learning, Prompt engineering, System configuration, LoRaWAN, Anomaly detection, Network optimization, Experimentation efficiency},
abstract = {This manuscript introduces a novel approach to integrate Large Language Models (LLMs) into wireless network testbeds for automated experimentation and optimization. We propose a framework that leverages LLMs to define system configurations using a structured format called Blueprint and analyse network behaviour through user-prompted queries. Our methodology combines few-shot prompting and prompt engineering to enable automated analysis of network configurations, enhancing decision-making and experimentation efficiency. We validated our approach in both simulated and real-world wireless environments and demonstrated its efficacy in streamlining experiment processes and extracting actionable insights.}
}
@article{DIMAKOU20251310,
title = {The predictive nature of spontaneous brain activity across scales and species},
journal = {Neuron},
volume = {113},
number = {9},
pages = {1310-1332},
year = {2025},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2025.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0896627325001278},
author = {Anastasia Dimakou and Giovanni Pezzulo and Andrea Zangrossi and Maurizio Corbetta},
keywords = {spontaneous brain activity, predictive brains, behavioral priors, task-rest similarity, metabolic priors},
abstract = {Summary
Emerging research suggests the brain operates as a “prediction machine,” continuously anticipating sensory, motor, and cognitive outcomes. Central to this capability is the brain's spontaneous activity—ongoing internal processes independent of external stimuli. Neuroimaging and computational studies support that this activity is integral to maintaining and refining mental models of our environment, body, and behaviors, akin to generative models in computation. During rest, spontaneous activity expands the variability of potential representations, enhancing the accuracy and adaptability of these models. When performing tasks, internal models direct brain regions to anticipate sensory and motor states, optimizing performance. This review synthesizes evidence from various species, from C. elegans to humans, highlighting three key aspects of spontaneous brain activity’s role in prediction: the similarity between spontaneous and task-related activity, the encoding of behavioral and interoceptive priors, and the high metabolic cost of this activity, underscoring prediction as a fundamental function of brains across species.}
}
@article{YOON2025112801,
title = {A federated learning framework for arbitrary spatio-temporal graph neural networks},
journal = {Engineering Applications of Artificial Intelligence},
volume = {162},
pages = {112801},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.112801},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625028325},
author = {Heeyong Yoon and Kang-Wook Chon and Min-Soo Kim},
keywords = {Machine learning framework for arbitrary models, Spatio-temporal graph neural network, Federated learning, Edge computing, Vehicle traffic data, Weather data},
abstract = {The proliferation of mobile and Internet of Things (IoT) devices has resulted in a surge of time-series sensor data, posing significant challenges for centralized data collection and processing. This challenge has driven the adoption of edge computing, which offloads data processing to mid-level servers located at the edge of the Internet, thereby reducing computation and bandwidth demands. Federated learning has emerged as a promising method for training models in edge-computing environments. Recently, spatio-temporal graph neural networks (STGNNs) have shown impressive performance in time-series prediction, yet their application in edge computing is limited by the complexity of adapting them to distributed environments. To address this gap, we propose FedSTGNN (Federated Spatio-Temporal Graph Neural Network), a universal framework that converts existing centralized STGNN models into a federated learning version. We formulate the common STGNN training process using matrix operations, employ graph-based imputation methods to handle missing sensor values at edge servers, and facilitate the transition from centralized to federated STGNNs. Our comprehensive evaluations demonstrate that FedSTGNN not only preserves the prediction accuracy of the original STGNN models but is also significantly more network-efficient than the competing model. Furthermore, the framework proves its robustness in challenging real-world scenarios, including sparse graphs, long-term forecasting, and dynamic server participation. Our work presents a practical, robust, and universal solution for deploying STGNNs into various edge computing applications.}
}
@article{SUNGE2024e39869,
title = {The COVID-19 pandemic and economic recovery: The mediating role of governance, a global perspective},
journal = {Heliyon},
volume = {10},
number = {22},
pages = {e39869},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e39869},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024159002},
author = {Regret Sunge and Calvin Mudzingiri and Nkosingiphile Mkhize},
keywords = {COVID-19 pandemic, Governance, Economic recovery, Mediation analysis, Structural equation modelling},
abstract = {The COVID-19 pandemic, arguably the most extensive economic shock after the Great Depression, has drawn attention from policy custodians over the past three years. Governments’ response brought to the limelight the role that governance plays in mitigating the economic shrinking effects of a pandemic. This study investigated the mediating role of governance in the post-pandemic recovery process using structural equation modelling of cross-sectional data from 125 countries for the years 2020 and 2021. The results show that governance did not mediate economic recovery at the global level. However, regional analysis reveals a full mediation effect in Africa and for low-income countries in 2021. Disaggregating governance by indicators in Africa reveals complete mediation for control of corruption, government effectiveness, regulatory quality, and the rule of law. Achieving sustainable economic recovery requires strengthening local governance structures and encouraging international cooperation. The research motivates the establishment of international governance institutions in the spirit of United Nations-driven frameworks. This can be complemented by country-specific, multi-agency, cross-sector collaborations led by the state, the development of governance systems that reduce mistrust among stockholders, and investment in artificial intelligence and e-governance systems.}
}
@article{HUA2025104119,
title = {Macroeconomic effects of CBDC negative interest policy in an open economy: A comparison of quantity and price rules},
journal = {International Review of Economics & Finance},
volume = {100},
pages = {104119},
year = {2025},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2025.104119},
url = {https://www.sciencedirect.com/science/article/pii/S1059056025002825},
author = {Qiuling Hua and Zepeng Qiu and Tingfeng Jiang and Ke Tang},
keywords = {CBDC, Negative interest rate, Quantity-based monetary policy, Price-based monetary policy},
abstract = {We construct a Dynamic Stochastic General Equilibrium (DSGE) model of a small open economy to investigate the effects of central bank digital currency (CBDC) negative interest rates on various sectors of the macroeconomy. Furthermore, we analyze the heterogeneous responses of quantity-based and price-based monetary policies. Our findings can be summarized as follows. (1) In an open economy, the CBDC negative interest rate policy can enhance the central bank's macroeconomic regulatory capacity during recessions by breaking the zero lower bound constraint on deposit interest rates. This provides a novel monetary policy tool to prevent "liquidity trap". (2) CBDC negative interest rates can strengthen the effectiveness of the quantity-based and price-based monetary policies. Specifically, it not only amplifies the short-term effects of quantity-based monetary policy and the medium- and long-term effects of price-based monetary policy, but also prolongs the effective duration of quantity-based monetary policy and reduces the transmission time lag of price-based monetary policy. (3) CBDC negative interest rates can strengthen the linkage between long-term and short-term monetary policy tools, increase the sensitivity of macroeconomic sectors to foreign monetary policy shocks, and enhance the smooth functioning of the monetary policy transmission mechanism.}
}
@article{2024105859,
title = {DLA Piper EU update},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105859},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105859},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923000699}
}
@article{NGUYEN2024,
title = {Usability, Engagement, and Report Usefulness of Chatbot-Based Family Health History Data Collection: Mixed Methods Analysis},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/55164},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006101},
author = {Michelle Hoang Nguyen and João Sedoc and Casey Overby Taylor},
keywords = {family health history, chatbots, conversational agents, digital health tools, usability, engagement, report usefulness, evaluation, crowdsourcing, mixed methods},
abstract = {Background
Family health history (FHx) is an important predictor of a person’s genetic risk but is not collected by many adults in the United States.
Objective
This study aims to test and compare the usability, engagement, and report usefulness of 2 web-based methods to collect FHx.
Methods
This mixed methods study compared FHx data collection using a flow-based chatbot (KIT; the curious interactive test) and a form-based method. KIT’s design was optimized to reduce user burden. We recruited and randomized individuals from 2 crowdsourced platforms to 1 of the 2 FHx methods. All participants were asked to complete a questionnaire to assess the method’s usability, the usefulness of a report summarizing their experience, user-desired chatbot enhancements, and general user experience. Engagement was studied using log data collected by the methods. We used qualitative findings from analyzing free-text comments to supplement the primary quantitative results.
Results
Participants randomized to KIT reported higher usability than those randomized to the form, with a mean System Usability Scale score of 80.2 versus 61.9 (P<.001), respectively. The engagement analysis reflected design differences in the onboarding process. KIT users spent less time entering FHx information and reported more conditions than form users (mean 5.90 vs 7.97 min; P=.04; and mean 7.8 vs 10.1 conditions; P=.04). Both KIT and form users somewhat agreed that the report was useful (Likert scale ratings of 4.08 and 4.29, respectively). Among desired enhancements, personalization was the highest-rated feature (188/205, 91.7% rated medium- to high-priority). Qualitative analyses revealed positive and negative characteristics of both KIT and the form-based method. Among respondents randomized to KIT, most indicated it was easy to use and navigate and that they could respond to and understand user prompts. Negative comments addressed KIT’s personality, conversational pace, and ability to manage errors. For KIT and form respondents, qualitative results revealed common themes, including a desire for more information about conditions and a mutual appreciation for the multiple-choice button response format. Respondents also said they wanted to report health information beyond KIT’s prompts (eg, personal health history) and for KIT to provide more personalized responses.
Conclusions
We showed that KIT provided a usable way to collect FHx. We also identified design considerations to improve chatbot-based FHx data collection: First, the final report summarizing the FHx collection experience should be enhanced to provide more value for patients. Second, the onboarding chatbot prompt may impact data quality and should be carefully considered. Finally, we highlighted several areas that could be improved by moving from a flow-based chatbot to a large language model implementation strategy.}
}
@article{WANG2025111077,
title = {Toward sustainable diffusion-based AIGC: Design and online orchestration in distributed edge networks},
journal = {Computer Networks},
volume = {259},
pages = {111077},
year = {2025},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2025.111077},
url = {https://www.sciencedirect.com/science/article/pii/S1389128625000453},
author = {Fei Wang and Lei Jiao and Konglin Zhu and Lingjun Pu and Lin Zhang},
keywords = {AIGC, Edge networks, Diffusion model, Fine-tuning, Online optimization},
abstract = {Artificial intelligence-generated content (AIGC) is an automated method that generates the content according to its knowledge and the intent information. Deploying the AIGC model in edge networks unlocks new possibilities. Unfortunately, realizing AIGC in distributed edge networks faces critical challenges, including the interdependent control decisions, the complex trade-offs between energy consumption and total variation (TV) distance, the extension of TV distance due to insufficient denoising steps, the restriction of AIGC inferencing deadlines, and the uncertainty of AIGC task arrivals. In this paper, targeting AIGC tasks, we design polynomial-time online algorithms to overcome all these challenges. Firstly, we formulate distributed AIGC as a non-linear mixed-integer program for long-term total cost optimization. Subsequently, we propose a novel algorithmic approach that generates candidate inferencing schedules, reformulates the original problem into a new schedule selection problem, and solves this new problem using an online primal–dual-based algorithm. Moreover, we rigorously prove that our approach leads to a constant competitive ratio for the long-term total cost. Through extensive evaluations using real-world data, the superior practical performance of our approach is demonstrated, reducing the total cost by more than 50% compared to various alternative methods.}
}
@article{ZARCHI2025113698,
title = {Explainable nature-inspired optimization via virtual and actual multi-objective strategies to establish a smart earthquake early warning system},
journal = {Applied Soft Computing},
volume = {184},
pages = {113698},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.113698},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625010099},
author = {Milad Zarchi and Reza A. Nazari and Kong Fah Tee},
keywords = {Geosynthetic reinforced soil structures, Seismic sliding displacement, Newmark method, Multiobjective optimization, Feature learning, Risk assessment, Early warning system, Nondominated sorting genetic algorithm, Particle swarm optimization, Reliability analysis},
abstract = {Geosynthetic-reinforced soil (GRS) structures are considered for reducing displacement and providing economical reinforcement solutions. The risk assessment of these structures against earthquakes, based on the prediction of seismic sliding displacement, is a major challenge in this field. Multi-objective optimization is a powerful machine learning tool for selecting efficient features for high-performance forecasting. This research investigates two strategies based on swarm intelligence and genetic programming for a comprehensive evaluation. These frameworks integrate multiobjective optimization algorithms and Newmark methods for utilizing effective physics-informed features. The first strategy is virtual multi-objective (VMO) optimization by applying particle swarm optimization (PSO) based on minimizing one function via variations of other functions. In this approach, the error function, as a computational error object, is minimized versus the nomination of interpretable feature space as a computational cost object through the virtual Pareto front. The second strategy is actual multi-objective (AMO) optimization by exploiting nondominated sorting genetic algorithm II (NSGA-II) based on minimizing several functions simultaneously with two various approaches, including bi-objective and many-objective algorithms through actual Pareto-optimal solutions. In this approach, the computational error value, computational cost value, and computational time value are minimized at the same time. The main novelty of the first technique is low computational complexity, resulting in high speed due to definite search space dimension-based exploration and exploitation to forecast seismic sliding displacement, whereas the major achievement of the second technique is high computational accuracy due to multiobjective structure-assisted exploitation and exploitation. Through numerical validation by employing the Newmark methods, the resultant model predicts the seismic sliding displacement of these structures using two algorithms efficiently. Nevertheless, both strategies have good performance for intelligent forecasting. The actual many-objective optimization algorithm is a more effective switchable machine learning tool based on the proposed adaptable performance index for developing a smart earthquake early warning software that can precisely detect imminent natural hazards.}
}
@article{BECKER2024103917,
title = {System shift in rice: Processes and pathways of change in rice-based production systems of Southeast Asia},
journal = {Agricultural Systems},
volume = {217},
pages = {103917},
year = {2024},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2024.103917},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X24000672},
author = {Mathias Becker and Richelyn Clavero and Ohnmar Min Khin and Sichantha Kong and Zar Ni Maung and Punlork Men and Shyam Pariyar and Manuel José C. Regalado and Sophoanrith Ro and Kyaw Kyaw Win},
keywords = {Cambodia, Diversification, Myanmar, , Philippines, Sustainable intensification, System shifts},
abstract = {CONTEXT
Predicting future production trends and associated land use and management practices requires an understanding of past changes in productivity and of pathways of evolving system configurations. We argue that rice systems' evolution reflects a process of adaptation to changing availabilities of production resources and the adoption of technological innovations and that differs by marginality/favorability of sites. Understanding past change trends and their determinants can help avoiding undesirable future developments and guide policy decisions for a sustained supply of rice.
OBJECTIVE
We aimed to assess agronomic system change and practices, to quantify pathways of change and to identify likely drivers of but also possible risks or opportunities associated with observed patterns of agronomic system transformation in six representative lowland rice production environments in Asia.
METHODS
We implemented a diachronic analysis (years 2000 vs. 2018) of rice production practices and yield attributes in 1024 households. We documented changes between 2000 and 2018 in lowland rice-based systems in six rice-growing environments in the Philippines, Myanmar and Cambodia, differentiating marginal and favorable sites in irrigated and in rainfed environments.
RESULTS AND CONCLUSION
Farmers' household attributes, resource endowment, rice yields, and key constraints differed among sites. We observed relatively low annual yield in marginal (2.2–3.0 t/ ha) than favorable (3.3–8.9 t/ ha) sites depending on countries and seasons. The farmers adopted intensification related agronomic practices has increased significantly between 2000 and 2018, especially in dry season (i.e., improved seeds by 28%, mechanical tillage by 52%, direct seeding by 21%, combined harvester by 62%). The marginality of climatic and edaphic conditions, the systems' evolutionary state in 2000, and differential pressures (policy environment), opportunities (technological change) and household attributes (resource endowment) determined the observed transition pathways across study sites.
SIGNIFICANCE
The trends towards maximizing land use intensity (double or multiple cropping), converging in the emergence of high-input and highly mechanized (laborsaving) in irrigated, and diversified rotations in rainfed rice production systems may help to elucidate agricultural research needs and potentially predict the requirements for future sustainable intensification of rice-based systems. Additionally, we argue for a continued need for further mechanization of rice establishment, especially shifting from transplanting to direct seeding during dry and wet seasons, and of the rice harvest, especially during wet season under fully irrigated environments.}
}
@incollection{2023443,
title = {Index},
editor = {Kevin Gillmann and Kaweh Mansouri},
booktitle = {The Science of Glaucoma Management},
publisher = {Academic Press},
pages = {443-458},
year = {2023},
isbn = {978-0-323-88442-6},
doi = {https://doi.org/10.1016/B978-0-323-88442-6.00049-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884426000492}
}
@article{HE2025105245,
title = {A systematic review of the use of log-based process data in computer-based assessments},
journal = {Computers & Education},
volume = {228},
pages = {105245},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105245},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000132},
author = {Surina He and Ying Cui},
keywords = {Computer-based assessment, Log-based process data, Systematic review},
abstract = {In recent decades, log-based process data has been increasingly used in computer-based assessments to examine test-takers' response patterns and latent traits. This study provides a systematic review of the use of log-based process data in computer-based assessments. Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guideline, we identified 2548 publications, of which 330 were finally included in this study after careful screening and full-text review. The results of this study can assist researchers in better understanding: (1) what are the trends in using log-based process data in computer-based assessments, (2) which process indicators have been constructed from raw log files, (3) what latent constructs have been inferred from process indicators and at what inferential levels, and (4) what are the benefits, challenges, and future recommendations for using log-based process data. By examining these questions, we conclude that the use of log-based process data in computer-based assessment shows many potentials for enhancing the assessment. Therefore, more study using log-based process data in various fields is encouraged to better understand test-takers’ underlying response processes during assessments. Additionally, there is also a considerable demand for validating process indicators and the generalizability of findings.}
}
@article{ZHOU2024117951,
title = {Machine learning models of intermittent operation of RO wellhead water treatment for salinity reduction and nitrate removal},
journal = {Desalination},
volume = {588},
pages = {117951},
year = {2024},
issn = {0011-9164},
doi = {https://doi.org/10.1016/j.desal.2024.117951},
url = {https://www.sciencedirect.com/science/article/pii/S0011916424006623},
author = {Yang Zhou and Nora Marki and Bilal Khan and Christian Aguilar and Yakubu Jarma and Yoram Cohen},
keywords = {Wellhead water treatment, Reverse osmosis, Machine learning, Nitrate removal, Salt passage, Intermittent RO operation},
abstract = {Machine learning models were developed for intermittent multi-mode operation of a wellhead reverse osmosis water purification and desalination system to predict salt passage, nitrate passage, and permeate flux. The models, based on long short-term memory (LSTM) recurrent neural network (RNN) architecture, included an attention mechanism to increase model performance in proximity of the regulatory limit for nitrate. Training and testing of the models for the Startup, Production, Shutdown and Flushing operational modes were based on operational data (consisting of 22 process variables per data sample) acquired every 2–5 s over a six-month period. The significant sets of model input attributes for the different operational modes were assessed via Spearman ranking correlation, Self-Organizing Map (SOM) analysis and feed forward feature selection (FFFS). Although the variability of nitrate passage, salt passage and permeate flux was significant over the four operational modes, prediction performance for the three outcomes were with R2 and Average Absolute Relative Error (AARE) of 0.78–0.95 and 2.96–6.16 %, respectively. Model updates post membrane elements replacement demonstrated similar levels of prediction accuracy. The study results suggest that there is merit in exploring the utility of multi-mode models for sensor fault detection, data imputation, and for potential use in model-predictive control.}
}
@article{LI2026743092,
title = {A comparative analysis of immune memory in bivalve and gastropod Mollusks: From mechanistic insights to practical applications},
journal = {Aquaculture},
volume = {612},
pages = {743092},
year = {2026},
issn = {0044-8486},
doi = {https://doi.org/10.1016/j.aquaculture.2025.743092},
url = {https://www.sciencedirect.com/science/article/pii/S0044848625009780},
author = {Hongyu Li and Hairun Li and Ling Zhao and Jialu Xu and Xianwei Li and Qingzhi Zhao and Yijie Zhang and Yuqing Shao and Ruke Wang and Jiyuan Wang and Lijun Lin and Xiaodong Yao and Xiaofen Zhang and Keda Chen},
keywords = {Bivalve, Gastropod, Immune, Immune memory, Innate immunity},
abstract = {Mollusks play essential ecological roles in marine and freshwater ecosystems, with their diversity reflected not only in morphological structures but also in immune strategies. Although traditionally considered to possess only innate immunity due to the absence of a typical adaptive immune system, recent studies have revealed that certain mollusks, such as oysters, exhibit “trained immunity”-an enhanced response upon secondary exposure to pathogens. This review summarizes the latest research advances on the immune systems of bivalve and gastropod mollusks, covering both humoral and cellular immunity, with a focus on systematically comparing their commonalities and differences in immune memory mechanisms. Core mechanisms include immune gene expression reprogramming, epigenetic modifications, metabolic reprogramming, and functional differentiation of immune cells. Bivalves primarily rely on complement and lectin pathways, while gastropods are characterized by the involvement of fibrinogen-related proteins (FREPs) and specific lectins. Transgenerational immune priming (TGIP) has been confirmed in bivalves, whereas relevant studies in gastropods remain limited. Furthermore, this review highlights the potential applications of immune memory in aquaculture disease management, such as developing immunostimulatory strategies to combat pathogens like Ostreid herpesvirus 1 (OsHV-1). A deeper understanding of the underlying mechanisms is expected to advance the development of immune-enhancing approaches.}
}
@article{MA202452,
title = {Eye tracking measures of bicyclists’ behavior and perception: A systematic review},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
volume = {107},
pages = {52-68},
year = {2024},
issn = {1369-8478},
doi = {https://doi.org/10.1016/j.trf.2024.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S136984782400233X},
author = {Shiyu Ma and Wenwen Zhang and Robert B. Noland and Clinton J. Andrews},
keywords = {Eye-tracking, Cycling experiments, Gaze metric, Safety, Stress},
abstract = {With improved portability and affordability, eye tracking devices have facilitated an expanding range of cycling experiments aimed at understanding cycling behavior and potential risks. Given the complexity of cyclists’ visual behavior and gaze measurements, we provide a comprehensive review with three key focuses: 1) the adoption and interpretation of various gaze metrics derived from cycling experiments, 2) a summary of the findings of those experiments, and 3) identifying areas for future research. A systematic review of three databases yielded thirty-five articles that met our inclusion criteria. Our review results show that cycling experiments with eye tracking allow analysis of the viewpoint of the cyclist and reactions to the built environment, road conditions, navigation behavior, and mental workload and/or stress levels. Our review suggests substantial variation in research objectives and the consequent selection of eye-tracking devices, experimental design, and which gaze metrics are used and interpreted. A variety of general gaze metrics and gaze measurements related to Areas of Interest (AOI) are applied to infer cyclists’ mental workload/stress levels and attention allocation respectively. The diversity of gaze metrics reported in the literature makes cross-study comparisons difficult. Areas for future research, especially potential integration with computer vision are also discussed.}
}
@article{MENI2024121239,
title = {Entropy-based guidance of deep neural networks for accelerated convergence and improved performance},
journal = {Information Sciences},
volume = {681},
pages = {121239},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121239},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524011538},
author = {Mackenzie J. Meni and Ryan T. White and Michael L. Mayo and Kevin R. Pilkiewicz},
abstract = {Neural networks have dramatically increased our capacity to learn from large, high-dimensional datasets across innumerable disciplines. However, their decisions are not easily interpretable, their computational costs are high, and building and training them are not straightforward processes. To add structure to these efforts, we derive new mathematical results to efficiently measure the changes in entropy as fully-connected and convolutional neural networks process data. By measuring the change in entropy as networks process data effectively, patterns critical to a well-performing network can be visualized and identified. Entropy-based loss terms are developed to improve dense and convolutional model accuracy and efficiency by promoting the ideal entropy patterns. Experiments in image compression, image classification, and image segmentation on benchmark datasets demonstrate these losses guide neural networks to learn rich latent data representations in fewer dimensions, converge in fewer training epochs, and achieve higher accuracy.}
}
@article{JACKSON2024740786,
title = {A high-density genetic linkage map and QTL identification for growth traits in dusky kob (Argyrosomus japonicus)},
journal = {Aquaculture},
volume = {586},
pages = {740786},
year = {2024},
issn = {0044-8486},
doi = {https://doi.org/10.1016/j.aquaculture.2024.740786},
url = {https://www.sciencedirect.com/science/article/pii/S0044848624002473},
author = {Tassin Kim Jackson and Clint Rhode},
keywords = {Aquaculture, Genetic architecture, Candidate genes, Marker-assisted selection, Selective breeding},
abstract = {High-density genetic linkage maps provide a foundational genomic resource for quantitative trait locus (QTL) mapping, candidate gene localisation, and comparative genomic analysis. Data from such studies, in turn, can be incorporated into marker or genomics assisted selective breeding strategies for increased efficiency of aquaculture production. This study aimed to construct the first high-density genetic linkage map for dusky kob (Argyrosomus japonicus) a commercially important marine finfish in Asia, Australia, and South Africa. Using 2b-restriction site-associated DNA (2b-RAD) sequencing for genome-wide single nucleotide polymorphism (SNP) genotyping, in three full-sib F1 families (Family 1, n = 69 offspring; Family 2, n = 73 offspring; and Family 3, n = 70 offspring), led to the discovery and genotyping of 22,789 SNPs. Among these, 3992 quality filtered and informative SNPs were mapped to 24 linkage groups (LGs), aligning with the species' proposed haploid chromosome number. The total integrated genetic map spanned a length of 2550 cM with an average marker spacing of 0.68 cM, achieving a genome coverage of 98.61%. Male and female specific maps highlighted differential map lengths and recombination rates between the sexes, with the female map slightly longer and a lower recombination frequency than for the male. QTL analysis for growth-related traits, weight (W), standard length (sL), and Fulton's condition factor (K), identified a total of 25 QTLs of which five QTLs were found to be consistently shared across the traits. Collectively, these shared QTLs explained a substantial proportion of the phenotypic variance, accounting for between 9.30 and 19.30% of the observed variation, suggesting potential major genes for growth. These QTL regions led to the discovery of 65 potential candidate genes, including 11 genes that were specifically located within the shared QTLs. These genes were linked to essential biological processes, including metabolism, immunity, stress response, development, and vascular function. The establishment of this high-density genetic linkage map represents an important genomic resource for comprehending the genetic determinants of economically important traits in A. japonicus, with implications for advancing and optimising ongoing marker-assisted selection (MAS) initiatives in the species.}
}
@article{CUBILLOSPINILLA2025106354,
title = {Neural foundations of creativity: A voxel-based meta-analysis of the activations and deactivations underlying creativity across linguistic, musical, and visual domains},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {178},
pages = {106354},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2025.106354},
url = {https://www.sciencedirect.com/science/article/pii/S0149763425003550},
author = {Leidy Cubillos-Pinilla and Allegre L. Hadida and Sandra Baez and Hernan Hernandez and Mert Kizilyamac},
keywords = {Creativity, FMRI deactivations, Alternative uses task, Seed-based d mapping},
abstract = {The neuroscience of creativity has proposed that shared and domain-specific brain mechanisms underlie creative thinking. However, greater nuance is needed in characterizing these mechanisms, and limited neuroimaging analyses, especially regarding the relationship between the Alternative Uses Task (AUT) and other linguistic tasks, have so far prevented a comprehensive understanding of the neural basis of creativity. This paper offers to fill these gaps with a closer examination of the contributions of the specific domains and the deactivations associated with creativity. We conduct a voxel-based meta-analysis of 43 neuroimaging studies involving 1118 participants. Using Seed-Based d Mapping, we investigate the spatial activity maps in the brain associated with overall creativity and with specific domains. Our findings reveal various domain-general mechanisms related to creativity, including working memory, the ability to connect distantly related concepts, the inhibition of conventional thought, interoception, internal goal orientation, mind wandering, and mental motor simulations. We also identify domain-specific mechanisms of creativity that differ by modality. Linguistic creativity requires inhibiting typical semantic associations, musical creativity involves auditory-motor integration and spontaneous expression, and visual creativity depends on inhibiting habitual visuospatial associations. Additionally, AUT is more effective at capturing novel tool manipulation and ideation rather than elaborative creative processes, which limits its scope. This meta-analysis underscores that creativity depends on multi-component neural circuits and highlights the need for future research to report deactivations, investigate neurofeedback applications, and analyze long-term and collaborative creative processes.}
}
@article{LI2025e41159,
title = {Digital technology administrative penalties and green technology innovation: Evidence from China},
journal = {Heliyon},
volume = {11},
number = {1},
pages = {e41159},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e41159},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024171903},
author = {Hong Li and Xiaohui Chen},
keywords = {Digital technology administrative penalties (DTAP), Green technology innovation (GTI), Digital industrialization, Financial technology, Signal theory},
abstract = {In digital economy era, digital technology is a key force to promote green technological innovation (GTI). Digital technology administrative penalties (DTAP) are an important means to regulate the development of digital technology enterprises, but its ability to effectively guide digital technology enterprises to better serve GTI remains to be further examined. DTAP sends signals to physical enterprises, financial enterprises, and individuals, thereby affecting the allocation of resources, such as technology, talent, and funds. Green technology innovation requires support from these resources, raising the question of how DTAP may affect GTI. This study proposes hypotheses based on signal theory and uses urban data from China from 2008 to 2020 to empirically test the impact of DTAP on GTI. The research findings indicate that DTAP is conducive to improving regional GTI, and DTAP facilitates GTI by fostering digital industrialization and financial technology development. The heterogeneity analysis reveals that the DTAP has a stronger promotion effect on the GTI in municipalities and low-carbon pilot cities. Our research is meaningful and can serve as a reference for other developing countries to standardize administrative supervision of the digital economy and promote the green economic transformation.}
}
@article{MIRINDI2025100275,
title = {Prediction of flexural and split tensile strength of waste glass-concrete composite using machine learning algorithms},
journal = {Green Technologies and Sustainability},
pages = {100275},
year = {2025},
issn = {2949-7361},
doi = {https://doi.org/10.1016/j.grets.2025.100275},
url = {https://www.sciencedirect.com/science/article/pii/S2949736125001095},
author = {Derrick Mirindi and David Sinkhonde and Tajebe Bezabih and Frederic Mirindi and Oluwakemi Oshineye and Patrice Mirindi},
keywords = {Waste glass concrete, Machine learning, AdaBoost, XGBoost, LightGBM, Gaussian process},
abstract = {Waste material, including glass, presents significant environmental challenges due to its non-biodegradable nature and low global recycling rates. Incorporating waste glass into concrete offers a sustainable solution, but predicting its effects on mechanical properties, particularly flexural (fb) and split tensile (ft) strengths, remains complex. This study utilizes machine learning (ML) algorithms (decision tree (DT), extreme gradient boosting (XGBoost), adaptive boosting (AdaBoost), light gradient boosting machine (LightGBM), support vector regression (SVR), and gaussian process (GP)) to predict fb and ft strengths based on compressive strength (fc), concrete age, and glass replacement percentage of glass-concrete composites. Thirteen experimental studies were utilized using secondary data. Results demonstrate that Pearson correlation analysis reveals strong interdependence among mechanical properties (fc-fb: 0.809-0.876, fc-ft: 0.927-0.948, fb-ft: 0.943-0.970), with negligible influence of glass type and moderate positive impact of replacement percentage. The ML algorithms each offer unique predictive strengths—most notably, XGBoost training model achieves near-perfect accuracy (with R2 equal to 0.9991). However, k-fold cross-validation revealed overfitting concerns limiting applicability to conventional concrete compositions. Non-parametric analyses reveal moderate fc-fb correlations (Spearman’s ρ=0.5879, p=0.0739) and statistically significant fc-ft relationships (ρ=0.6364, p=0.0479), while ML models achieve high predictive accuracy by exploiting multi-feature interactions beyond simple pairwise correlations. These ML models enable optimized mix designs, advancing sustainable construction through efficient waste glass utilization as a partial aggregate replacement.}
}
@incollection{DORNELAS20241,
title = {Chapter One - Living in the age of artificial intelligence: advancement or fate?},
editor = {Carolina Machado and J. Paulo Davim},
booktitle = {Artificial Intelligence in Production Engineering and Management},
publisher = {Woodhead Publishing},
pages = {1-28},
year = {2024},
series = {Woodhead Publishing Reviews: Mechanical Engineering Series},
isbn = {978-0-12-819471-3},
doi = {https://doi.org/10.1016/B978-0-12-819471-3.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194713000069},
author = {Jairo Simião Dornelas},
keywords = {Artificial intelligence, artificial intelligence applications, artificial intelligence areas, artificial intelligence social perspective, artificial intelligence organizational perspective, information technology},
abstract = {The advancement of information technology in society—seen in the use of computers—on the threshold of its 60 years of methodical, visible, and continuous appearance, brought it an aura of indispensability, which is denoted in the transposition of programmable activities from man to machine. This finding reinforces the evolution of use, since now and potentially increasingly in the future, it will be cognitive tasks, and perhaps creative ones, that will be the object of this transfer, in the so-called rise of artificial intelligence. It is precisely this trajectory with some academic structure that we wanted to narrate. To this end, based on a concept of intelligence and its understanding when transposed to the scope of machines, it is outlined what would be the most widespread approaches for this achievement. This implies knowing implementation tactics and areas of use, as well as predicting areas in which efforts to develop new artifacts are added. Examples of how companies to appropriate artificial intelligence routines in real applications, which are required in the software market, are also highlighted, and the report concludes with a brief but intriguing survey with any people, about artificial intelligence, its use and possibilities management, and social governance.}
}
@article{GALLI20251,
title = {The quadratic knapsack problem},
journal = {European Journal of Operational Research},
volume = {326},
number = {1},
pages = {1-12},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724009743},
author = {Laura Galli and Silvano Martello and Paolo Toth},
keywords = {Quadratic knapsack problem, Linearization, Upper bounds, Exact solution, Approximation, Heuristics, Metaheuristics, Computational results},
abstract = {The quadratic knapsack problem is a relevant NP-hard combinatorial optimization problem, inspired, since the Seventies, by a number of real-world applications. After its formal definition in 1980, it was subject to intensive research, especially in the last two decades. No recent review on this problem appeared in the literature after a well-known survey, published in 2007 but updated to 2003. The purpose of this work is to provide a thorough overview of classical and recent results on the quadratic knapsack problem. We examine mathematical models, linearizations and reformulations. We review upper bounds, exact algorithms, heuristic and metaheuristic approaches, and provide a comparison of their computational performance.}
}
@article{SAURA2024100597,
title = {Is AI-based digital marketing ethical? Assessing a new data privacy paradox},
journal = {Journal of Innovation & Knowledge},
volume = {9},
number = {4},
pages = {100597},
year = {2024},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2024.100597},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001367},
author = {Jose Ramon Saura and Vatroslav Škare and Durdana Ozretic Dosen},
keywords = {Artificial intelligence, Digital marketing, Ethics, Privacy, Multiple correspondence analysis, R},
abstract = {The rapid development of artificial intelligence (AI) has significantly transformed digital marketing enhancing its effectiveness and raising new ethical and privacy concerns. This study investigates the ethical implications of AI-based digital marketing, particularly focusing on user privacy. In terms of methodology, a systematic literature review (SLR) was conducted to identify relevant variables, followed by Multiple Correspondence Analysis (MCA) using R within the framework of homogeneity analysis of variance using alternating least squares (HOMALS). The MCA analysis identified 3 multivariate groupings, and 21 individual variables extracted from 28 studies. The MCA identified a total of 4 clusters in the eigenvalues/variances analysis, and 5 clusters in the biplot analysis. The findings emphasize the need for a balanced approach that respects user privacy and ethical use of data when developing actions using AI-based digital marketing. However, no significant relationship is evident between the study of variables such as cross-device tracking or data-driven technologies and, the ethics of AI-based digital marketing, despite these being the most profitable actions in this environment. There is no evidence of developing personalized social media content or ads linked to privacy standards. However, a strong connection between behavioral analytics, smart content and metaverse is identified, highlighting the risks of this emerging technology in this research field, as it is not linked to privacy or ethics. Among the results, the strong proximity of real-time tracking, IoT, and surveillance variables underscores the critical need to ethically understand how user behavior in real-time is being monitored, as they do not offer a strong link to privacy or ethics. Additionally, this study provides 21 future research questions that address whether these practices are being ethically implemented, following standards like “privacy-by-default” or “privacy-by-design,” and complying with privacy laws in AI-based digital marketing. To ensure these practices align with ethical standards, it is essential to adopt frameworks prioritizing data dignity, which calls for treating user data as an extension of personal identity, requiring responsible and ethical handling throughout the data collection and processing lifecycle.}
}
@article{BRAHMACHARY2025129272,
title = {Large language model-based evolutionary optimizer: Reasoning with elitism},
journal = {Neurocomputing},
volume = {622},
pages = {129272},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129272},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224020435},
author = {Shuvayan Brahmachary and Subodh M. Joshi and Aniruddha Panda and Kaushik Koneripalli and Arun Kumar Sagotra and Harshil Patel and Ankush Sharma and Ameya D. Jagtap and Kaushic Kalyanaraman},
keywords = {Large language models, Evolutionary Optimizers, Multi-objective optimization, Aerodynamic Design},
abstract = {Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, prompting interest in their application as black-box optimizers. This paper asserts that LLMs possess the capability for zero-shot optimization across diverse scenarios, including multi-objective and high-dimensional problems. We introduce a novel population-based method for numerical optimization using LLMs called Large Language-Model-Based Evolutionary Optimizer (LEO). Our hypothesis is supported through numerical examples, spanning benchmark and industrial engineering problems such as supersonic nozzle shape optimization, heat transfer, and windfarm layout optimization. We compare our method to several gradient-based and gradient-free optimization approaches. While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions.}
}
@article{LI2025102009,
title = {Unveiling civil servants' preferences: Human-machine matching vs. regulating algorithms in algorithmic decision-making——Insights from a survey experiment},
journal = {Government Information Quarterly},
volume = {42},
number = {1},
pages = {102009},
year = {2025},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2025.102009},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X25000036},
author = {Huanhuan Li and Zongfeng Sun and Jiacheng Xi},
keywords = {Algorithmic decision-making, Human-machine matching, Algorithm regulation, Trust perception, Usage inclinations},
abstract = {While research has explored trust in algorithmic decision-making, the factors shaping civil servants' trust perceptions remain underexamined. Using public value theory and technology adoption frameworks, this study employs a survey experiment to analyze the effects of human-machine matching and algorithm regulation on civil servants' trust and adoption inclination. The findings indicate that both factors independently influence adoption inclination, with trust perceptions mediating this relationship, but no interaction effect is observed. Addressing gaps in technology acceptance and ethical frameworks, this study highlights the importance of algorithm regulation and human-machine matching in advancing algorithmic governance and achieving public value through procedural and performance dimensions, offering practical implications for policy and governance.}
}
@article{BALAHA2025100647,
title = {An analytical review of data integration for decision support in smart manufacturing},
journal = {Decision Analytics Journal},
volume = {17},
pages = {100647},
year = {2025},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2025.100647},
url = {https://www.sciencedirect.com/science/article/pii/S2772662225001031},
author = {Farah Balaha and Hamad Albinali and Haneen Alrabiah and Mohamed Ali and Zied Bahroun},
keywords = {Decision support, Process optimization, Operational intelligence, Smart manufacturing, Data integration, Predictive insights},
abstract = {The increasingly complex and globalized business environment demands effective data integration and management to support decision-making. Enterprise Resource Planning (ERP) systems play a pivotal role in managing and integrating information in modern organizations, particularly within the evolving landscape of Industry 4.0. This paper presents a systematic review, conducted using PRISMA guidelines, of peer-reviewed journal articles published between 2019 and 2024. A comprehensive search in the Scopus database retrieved 439 publications, of which 69 were selected after assessing relevance. These studies were categorized into five key themes: Resource Allocation and Planning, Information Management, Supply Chain Improvement, Automation and Process Optimization, and Exploratory and Explanatory Studies, with further subthemes providing deeper insights. The review reveals that while ERP-Industry 4.0 integration offers significant potential for enhancing operational efficiency, supply chain resilience, real-time data visibility, and automation capabilities, it also presents challenges related to system complexity, data security, interoperability, and workforce readiness. The analysis further uncovers a tendency for case-specific or sector-specific studies, highlighting the need for comprehensive frameworks and cross-sectoral research. A bibliometric analysis using VOSviewer visualized co-authorship networks, research trends, prominent keywords, and the most integrated Industry 4.0 technologies. Notably, IoT, Big Data Analytics, and Cloud Computing emerged as the most prevalent technologies associated with ERP integration. This paper synthesizes the current state of research, identifies significant gaps, and offers actionable recommendations to guide future research. These include: developing scalable integration architectures, addressing cybersecurity and data governance challenges, and fostering cross-functional collaboration to support data-driven decision-making and enhance ERP integration in dynamic Industry 4.0 manufacturing environments.}
}
@article{SHAHIN2024102462,
title = {Harnessing customized AI to create voice of customer via GPT3.5},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102462},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102462},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001101},
author = {Mohammad Shahin and F. Frank Chen and Ali Hosseinzadeh},
keywords = {ChatGPT, VoC, Lean Six Sigma, Industry 5.0, Artificial General Intelligence},
abstract = {The integration of customer feedback is universally acknowledged as crucial in the product development process. Yet, traditional feedback collection methods employed by companies, such as interviews and surveys, have remained mainly unchanged and come with limitations. Interviews often fail to accurately capture customers' needs due to communication barriers, while surveys prompt only incremental changes instead of inspiring innovation. This challenge is compounded in the service industry, where feedback is intangible and more difficult to quantify. Text analysis presents a promising solution to delve into customer preferences more deeply, providing insights that can guide the development of new products and services. Our research advances the use of generative AI, specifically the GPT engine, beyond its conventional role as a chatbot. We innovate by adapting it to extract actionable insights from customer-service interactions, offering real-time, valuable data for decision-making and representing a significant leap forward in Voice of the Customer (VoC) analysis.}
}
@article{KALIISA2025105246,
title = {A Topical Review of Research in Computer-Supported Collaborative Learning: Questions and Possibilities},
journal = {Computers & Education},
volume = {228},
pages = {105246},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105246},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000144},
author = {Rogers Kaliisa and Sonsoles López-Pernas and Kamila Misiejuk and Crina Damşa and Márta Sobocinski and Sanna Järvelä and Mohammed Saqr},
keywords = {Computer supported collaborative learning, CSCL, Collaborative learning, Collaborative processes, Bibliometric analysis},
abstract = {This study maps the evolution and state of Computer-Supported Collaborative Learning (CSCL) research, analyzing 6388 documents published between 1990 and 2022. The findings highlight the sustained engagement of a core group of scholars and the field's geographic concentration in Western countries, particularly the USA and Europe. While the field remains productive and diverse, recent trends reflect a growing emphasis on integrating emerging technologies such as learning analytics, augmented and virtual reality, and artificial intelligence (AI) into collaborative learning contexts. The study uncovers a tension within CSCL between its epistemological and methodological diversity and the need for theoretical coherence. This diversity has allowed the field to adapt and innovate but raises concerns about fragmentation and the risk of losing a unified identity. For example, while scripting remains a foundational topic, debates persist on balancing instructional guidance with learner agency to foster productive collaboration. Looking ahead, the study underscores the need for CSCL to develop integrative theoretical frameworks that bridge its rich historical foundations with the challenges posed by large-scale, distributed, and technology-mediated collaboration. Addressing these challenges, such as aligning AI innovations with existing theories and ensuring cumulative knowledge-building, will be critical for the field's ability to sustain its relevance and influence in understanding collaborative learning in complex educational environments.}
}
@article{JANSON2023107954,
title = {How to leverage anthropomorphism for chatbot service interfaces: The interplay of communication style and personification},
journal = {Computers in Human Behavior},
volume = {149},
pages = {107954},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107954},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223003059},
author = {Andreas Janson},
keywords = {Chatbots, Conversational agents, Anthropomorphic design, Social presence, Empathy, Trust},
abstract = {Although chatbots are oftentimes used in customer service encounters, interactions are oftentimes perceived as not satisfactory. One key aspect for designing chatbots is the use of anthropomorphic design elements. In this experimental study, we examine the two anthropomorphic chatbot design elements of personification, which includes a human-like appearance, and social orientation of communication style, which means a more sensitive and extensive communication. We tested the influence of the two design elements on social presence, satisfaction, trust and empathy towards a chatbot. First, the results show a significant influence of both anthropomorphic design elements on social presence. Second, our findings illustrate that social presence influences trusting beliefs, empathy, and satisfaction. Third, social presence acts as a mediator for both anthropomorphic design elements for satisfaction with a chatbot. Our implications provide a better understanding of anthropomorphic chatbot design elements when designing chatbots for short-term interactions, and we offer actionable implications for practice that enable more effective chatbot implementations.}
}
@article{PENG2025104303,
title = {3D modeling from a single image via a novel dual-decoder framework for Agile design},
journal = {Computers in Industry},
volume = {169},
pages = {104303},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104303},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525000685},
author = {Jieyang Peng and Andreas Kimmig and Simon Kreuzwieser and Zhibin Niu and Xiaoming Tao and Jivka Ovtcharova},
keywords = {Computer-aided design, 3D entities prediction, Dual-decoder architecture, Agile design, Intelligent manufacturing},
abstract = {In the fast-paced manufacturing industry, rapid and efficient product design is essential for meeting customer demands and maintaining a competitive edge. Despite advancements, transforming 2D design concepts into accurate 3D models remains a complex challenge, primarily due to the non-differentiability of traditional rendering processes that hinder gradient-based optimizations. To address this limitation, this paper introduces an innovative dual-decoder architecture that effectively separates the shape and color components of 3D models. By assigning separate decoders for vertex positions and color assignment, our proposed model enables targeted optimization of each, leading to more refined and authentic 3D reconstructions. Moreover, we have overcome the non-differentiability issue, enabling gradient-based learning through the incorporation of differentiable rendering techniques. These techniques facilitate gradient-based optimization, paving the way for data-driven enhancements in the design process. Our empirical research has demonstrated the effectiveness of our approach in generating high-fidelity 3D models from 2D inputs. Additionally, we have shed light on the sensitivity of hyperparameters within our framework, offering valuable insights for future model refinement and optimization. In summary, our research provides valuable insights into enhancing 3D modeling frameworks, thereby contributing to incremental progress in the field of computer-aided design and manufacturing.}
}
@article{CHAO2024e35468,
title = {From hate to harmony: Leveraging large language models for safer speech in times of COVID-19 crisis},
journal = {Heliyon},
volume = {10},
number = {16},
pages = {e35468},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e35468},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024114995},
author = {August F.Y. Chao and Chen-Shu Wang and Bo-Yi Li and Hong-Yan Chen},
abstract = {This study investigates the rampant spread of offensive and derogatory language during the COVID-19 pandemic and aims to mitigate it through machine learning. Employing advanced Large Language Models (LLMs), the research develops a sophisticated framework adept at detecting and transforming abusive and hateful speech. The project begins by meticulously compiling a dataset, focusing specifically on Chinese language abuse and hate speech. It incorporates an extensive list of 30 pandemic-related terms, significantly enriching the resources available for this type of research. A two-tier detection model is then introduced, achieving a remarkable accuracy of 94.42 % in its first phase and an impressive 81.48 % in the second. Furthermore, the study enhances paraphrasing efficiency by integrating generative AI techniques, primarily Large Language Models, with a Latent Dirichlet Allocation (LDA) topic model. This combination allows for a thorough analysis of language before and after modification. The results highlight the transformative power of these methods. They show that the rephrased statements not only reduce the initial hostility but also preserve the essential themes and meanings. This breakthrough offers users effective rephrasing suggestions to prevent the spread of hate speech, contributing to more positive and constructive public discourse.}
}
@article{GUARRASI2024110825,
title = {Multimodal explainability via latent shift applied to COVID-19 stratification},
journal = {Pattern Recognition},
volume = {156},
pages = {110825},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110825},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005764},
author = {Valerio Guarrasi and Lorenzo Tronchin and Domenico Albano and Eliodoro Faiella and Deborah Fazzini and Domiziana Santucci and Paolo Soda},
keywords = {XAI, Multimodal deep learning, Joint fusion, Classification, COVID-19},
abstract = {We are witnessing a widespread adoption of artificial intelligence in healthcare. However, most of the advancements in deep learning in this area consider only unimodal data, neglecting other modalities. Their multimodal interpretation necessary for supporting diagnosis, prognosis and treatment decisions. In this work we present a deep architecture, which jointly learns modality reconstructions and sample classifications using tabular and imaging data. The explanation of the decision taken is computed by applying a latent shift that, simulates a counterfactual prediction revealing the features of each modality that contribute the most to the decision and a quantitative score indicating the modality importance. We validate our approach in the context of COVID-19 pandemic using the AIforCOVID dataset, which contains multimodal data for the early identification of patients at risk of severe outcome. The results show that the proposed method provides meaningful explanations without degrading the classification performance.}
}
@article{LU2023102867,
title = {Information sharing decisions in all-pay auctions with correlated types},
journal = {Journal of Mathematical Economics},
volume = {107},
pages = {102867},
year = {2023},
issn = {0304-4068},
doi = {https://doi.org/10.1016/j.jmateco.2023.102867},
url = {https://www.sciencedirect.com/science/article/pii/S0304406823000605},
author = {Jingfeng Lu and Hongkun Ma and Zhewei Wang},
keywords = {All-pay auctions, Contests, Correlated types, Information disclosure, Pure/mixed strategy equilibrium},
abstract = {In many real-life competitions, contestants may not be aware of their own type (e.g., value or ability) prior to the contest. Furthermore, contestants’ types, which are observed privately after entering the competition, are frequently correlated with one another. We examine a two-stage competition that involves two players with correlated (binary) types. In the first stage, players decide simultaneously or sequentially on the probabilities they use to disclose or conceal their type, which will become their private information later on. In the second stage, each player privately observes their own type and commits to disclosing or concealing it, after which they compete in an all-pay auction. We discover that information sharing does not occur when players’ types are negatively correlated. However, when players’ types are positively correlated, information is partially shared in all equilibria examined in this study. In an asymmetric pure strategy equilibrium, one player shares his information with probability one and the other player with probability zero. In a symmetric mixed strategy equilibrium, each player shares his information with the same positive probability.}
}
@article{ALIYEV2025101006,
title = {Business simulation games integrated with emerging technologies in business education: A review},
journal = {Entertainment Computing},
volume = {55},
pages = {101006},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2025.101006},
url = {https://www.sciencedirect.com/science/article/pii/S1875952125000862},
author = {Murad Aliyev and Vagif Fatullayev and Sadagat Aliyeva},
keywords = {Business simulation games, BSGs, Emerging technologies, Simulation-based learning, Business education, Review},
abstract = {Business simulation games (BSGs) have gained popularity in higher education, offering students hands-on experiences. Recent advancements in emerging technologies have further fuelled their growth, contributing to the global expansion of BSG market. This review aims to examine the integration of AI, generative AI chatbots, extended reality (XR), virtual reality (VR) (semi-immersive – 3D animated; fully immersive – business training simulators; mobile VR – 360-degree videos), blockchain, mobile and web-based technologies into BSGs and their potential to enhance learning outcomes. This literature review was conducted following the PRISMA framework. Through thematic synthesis analysis of 46 empirical studies, this study identified three major themes: (1) the pedagogical benefits of integrating emerging technologies in BSGs; (2) the factors that shape active student participation, including contextual relevance, instructional design, and intrinsic motivation; and (3) the persistent challenges related to technology integration, cultural adaptability, and inclusive access. The review highlights the dominance of quantitative and mixed-methods approaches, a lack of research in non-Western contexts, and the underrepresentation of long-term impact studies. It also emphasizes the need for pedagogically grounded, culturally responsive, and inclusive BSG designs supported by well-prepared instructors.}
}
@article{MA2025112700,
title = {Semantic-based topic model for public opinion analysis in sudden-onset disasters},
journal = {Applied Soft Computing},
volume = {170},
pages = {112700},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112700},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625000110},
author = {Yulong Ma and Xinsheng Zhang and Runzhou Wang},
keywords = {Sudden-onset disaster, Topic model, Latent Dirichlet allocation, Distributional semantics},
abstract = {Sudden-onset disasters have put forward more stringent requirements for the government to carry out public opinion analysis work. However, most existing topic models ignore the contextual semantics of disaster texts, and fail to balance the robustness and the training cost. To address these issues, a neural clustering topic model is proposed in this work. The topic probability distribution of the LDA model is integrated with the distribution semantic vector generated by a lite BERT. The fused vectors are reconstructed by a nonlinear manifold learning algorithm, and re-clustered into topics by a mini-batch based k-means++ algorithm. Compared to state-of-the-art models on three sudden-onset disaster datasets, the proposed model shows an increase of 1.79 % in average topic coherence and 33.87 % in topic diversity. Meanwhile, the inference time is reduced by 84.09 % on average. The visual study of the latent process of the proposed model reflects that its ability to compact intra-cluster vector distances and sparse inter-cluster vector distances is the potential reason for its better performance. It can be considered that the application of the proposed model can help the government enhance its ability to manage negative public opinions in sudden-onset disasters.}
}
@article{QUINTANILLA2025801,
title = {Artificial intelligence and robotics in the hydrogen lifecycle: A systematic review},
journal = {International Journal of Hydrogen Energy},
volume = {113},
pages = {801-817},
year = {2025},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2025.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0360319925010973},
author = {Paulina Quintanilla and Ayman Elhalwagy and Lijia Duan and Salman {Masoudi Soltani} and Chun Sing Lai and Pantea Foroudi and Md Nazmul Huda and Monomita Nandy},
abstract = {Hydrogen lifecycle, encompassing production, storage, and transportation, is crucial in the global transition to clean energy. Integrating artificial intelligence (AI) and robotics into hydrogen lifecycle offers promising solutions to enhance efficiency, safety, and scalability. This paper presents a comprehensive review of the current advancements published over the past two decades (2005–2025), analyzing AI and robotics applications across hydrogen production, storage, and transportation. We systematically examine the role of AI in optimizing hydrogen production processes, improving the safety and efficiency of storage systems, and enhancing transportation logistics through real-time monitoring and route optimization. Additionally, the paper explores the use of robotics to handle complex tasks in hazardous environments within the hydrogen lifecycle. We identify key challenges and gaps in the literature and propose future research directions to fully leverage AI and robotics across hydrogen technologies. This review serves as a foundation for researchers and practitioners seeking to advance the integration of AI and robotics in the hydrogen economy.}
}
@article{CHAN2025112449,
title = {Identifying inconsistent software defect predictions with symmetry metamorphic relation pattern},
journal = {Journal of Systems and Software},
volume = {227},
pages = {112449},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112449},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225001177},
author = {Pak Yuen Patrick Chan and Jacky Keung and Zhen Yang},
keywords = {Metamorphic testing, Metamorphic relation patterns, Software defect prediction, Explainable artificial intelligence},
abstract = {Determining inconsistent software defect predictions in machine learning-based systems poses a significant challenge. To address this issue, we propose the utilization of Metamorphic Testing (MT) incorporating the “symmetry” metamorphic relation pattern (MRP) to transform the training datasets for training follow-up systems. In contrast, original datasets are employed to train source systems. By comparing the occurrence of inconsistent predictions between source and follow-up systems and analysing the efficacy of this approach, we aim to shed light on its effectiveness. Additionally, Explainable Artificial Intelligence (XAI) is employed to explain the inconsistencies observed. The results demonstrate that the “symmetry” MRP can induce inconsistent predictions, and XAI techniques can effectively elucidate such inconsistencies. Moreover, we find that the ordering of small-sized and imbalanced datasets can contribute to inconsistencies when using the KMeans, Random Forests or Convolutional Neural Network algorithm for software defect prediction systems. To further advance this research, future studies can extend the proposed approach by incorporating additional MRPs in domains that utilize machine learning algorithms to identify and explain inconsistencies. Another promising research avenue involves investigating the relationship between data imbalance, dataset size, and MRPs to enhance the identification of inconsistencies and derive more robust MRs.}
}
@article{SAHUT2024114572,
title = {Antecedents and consequences of fake reviews in a marketing approach: An overview and synthesis},
journal = {Journal of Business Research},
volume = {175},
pages = {114572},
year = {2024},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.114572},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324000766},
author = {Jean Michel Sahut and Michel Laroche and Eric Braune},
keywords = {Fake reviews, Technology, Behavior, Regulation, Strategy, Artificial intelligence},
abstract = {Fake reviews, characterized as misleading, can be positive, negative, or neutral, varying greatly across sectors and products. Their detrimental effects include reducing the informativeness and credibility of genuine reviews, leading to a distorted perception of products and affecting the development of online product reviews. This literature synthesis proposes a multidisciplinary approach to understand and combat fake reviews, emphasizing the need to differentiate them from genuine feedback through psychological and technological tactics. It outlines the importance of exploring the credibility routes influencing consumer behavior, the dynamics of fake review production, and the consequential effects on businesses and consumers. It also highlights the emergence of artificial intelligence as a powerful tool in identifying and combating fake reviews, advocating for continued exploration of detection strategies to preserve integrity and trust in online marketplaces. Lastly, it suggests avenues of research to deepen our knowledge of the antecedents and consequences of fake reviews, as well as of the various means available to prevent them, including technological, behavioral, and regulatory strategies.}
}
@article{VINTHERDAUGAARD2024101386,
title = {Blockchain solutions with consensus algorithms and immediate finality: Toward Panopticon-style monitoring to enhance anti-money laundering},
journal = {Electronic Commerce Research and Applications},
volume = {65},
pages = {101386},
year = {2024},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2024.101386},
url = {https://www.sciencedirect.com/science/article/pii/S1567422324000310},
author = {Thomas {Vinther Daugaard} and Jakob {Bisgaard Jensen} and Robert J. Kauffman and Kwansoo Kim},
keywords = {Anti-money laundering (AML), Blockchain, Compliance, Distributed ledger technology (DLT), Exploratory research, Know-your-customer (KYC), Transaction cost theory (TCE), Transaction monitoring},
abstract = {Banks can reduce resources spent on anti-money laundering (AML) compliance with blockchain-based transaction infrastructure. We consider AML compliance as a superset of know-your-customer (KYC) and transaction monitoring capabilities. We carried out this research with Danske Bank and Concordium, using internal documents and interviews that served as empirical data. We show how storing digital representations of verified IDs with a blockchain can automate tasks and reduce redundant verification in KYC onboarding. Blockchain transparency also improves identifying counterparties, determining funds sources, and creating alerts in transaction monitoring. These reduce time and labor costs for AML compliance, which may lead to smaller banks. When more banks commit to layer-1 blockchain technology, the benefits of blockchain-based AML will increase. We carried out this theory-based qualitative research and encourage ECRA readers to recognize that the emerging technology innovations we study in this article have not yet been widely adopted and implemented by financial services firms. We also include a theoretical model with study hypotheses to make the main constructs that we investigate easily understood by non-technical ECRA readers. The findings we have developed are consistent with early-stage exploration in our research context and are intended to encourage more well-developed empirical results as the passage of time permits such work to be undertaken.}
}
@article{CIVAI2025106192,
title = {The role of attention and frames on third-party punishment and compensation choices},
journal = {Cognition},
volume = {263},
pages = {106192},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2025.106192},
url = {https://www.sciencedirect.com/science/article/pii/S0010027725001325},
author = {Claudia Civai and Valerio Capraro and Luca Polonio},
keywords = {Injustice perception, Third-party punishment and compensation, Attentional processes, Information frames, Choice process},
abstract = {People often forgo their own self-interest to react to fairness and justice violations, even when not directly affected by the infraction. There are different ways to react to an injustice: some may prefer to punish the perpetrator, and others to compensate the victim. Here, our focus is on the role played by attention to determine these choices, investigating the relationship between attentional mechanisms and punishment/compensation in five preregistered experiments (N = 1157). Two eye-tracking experiments showed that people who focus more on the offender's payoff are more likely to punish, and when an exogenous stimulation increases the focus on the offender's payoff, people spend more to punish. An offender bias was also found, meaning that people, overall, prefer to focus on the offender's, rather than the victim's, payoff, and punish more than compensate. This was confirmed in three behavioural experiments, where people were exposed to either the offender's or the victim's payoff: when given the choice, people prefer to reveal the offender's payoff, and then punish; however, when randomly exposed to the victim's payoff, the preference for punishment disappears. Affective empathy boosts this effect: higher empathy leads to more punishment (or compensation) when the offender's (or victim's) payoff is revealed. These findings suggest that, whilst people have an intrinsic motivation to search for information that matches their preference (i.e., the offender's payoff and punishment), when exposed to an alternative piece of information (i.e., the victim's payoff), they modify their behaviour. Implications for understanding information bubbles and ways to overcome them are discussed.}
}
@article{TABATABAEI2023212023,
title = {EOR screening using optimized artificial neural network by sparrow search algorithm},
journal = {Geoenergy Science and Engineering},
volume = {229},
pages = {212023},
year = {2023},
issn = {2949-8910},
doi = {https://doi.org/10.1016/j.geoen.2023.212023},
url = {https://www.sciencedirect.com/science/article/pii/S2949891023006103},
author = {S. Mostafa Tabatabaei and Nikta Attari and S. Amirali Panahi and Mojtaba Asadian-Pakfar and Behnam Sedaee},
keywords = {EOR, Sparrow search algorithm (SSA), Particle swarm optimization (PSO), Artificial neural network (ANN), Deep learning, Meta-heuristic algorithms},
abstract = {Enhanced oil recovery (EOR) is a crucial aspect of reservoir engineering, and the use of machine-learning algorithms in the initial stages of screening has been widely accepted as a fast and efficient method for screening the most suitable EOR method. This study presents an artificial neural network (ANN) that recommends the most suitable EOR method based on historical reservoir data. Data from EOR projects worldwide were collected, pre-processed, and then used to build the ANN, which initially achieved a 69% accuracy. The neural network was optimized using the Sparrow Search Algorithm (SSA) and compared with the Particle Swarm Optimization (PSO) algorithm, with a focus on weight and hyperparameter optimization. Validation of the neural network’s prediction was done using recall, precision, and the F1 score. Weight optimization yielded an accuracy of 68% with SSA and 34% with PSO, which were insufficient results for EOR prediction. However, hyperparameter optimization was applied, resulting in an accuracy of 94% with SSA and 90% with PSO. The SSA approach demonstrated faster convergence and higher accuracy in both optimization paths, highlighting its potential for optimizing the neural network in predicting the appropriate EOR method for a given reservoir.}
}
@article{WILKHO2024102293,
title = {FF-BERT: A BERT-based ensemble for automated classification of web-based text on flash flood events},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102293},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102293},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004214},
author = {Rohan Singh Wilkho and Shi Chang and Nasir G. Gharaibeh},
keywords = {Flash flood, Text classification, Multi-label text classification, BERT},
abstract = {The web is a rich information repository that can be mined to uncover additional data about past flash flood (FF) events, currently missing from existing structured databases. However, this information originates from multiple sources (news articles, government records, and weather records among others) and may cover several topics. Furthermore, these topics may be disproportionately covered on the web. The large size and heterogenous nature of web information render manual review difficult. To address this challenge, we have developed a multi-label text classification model, FF-BERT. FF-BERT is designed to classify FF-related web paragraphs into one or more of seven categories: (1) Damage and Economic Impact (DI), (2) Fatalities, Injuries, and Rescue (FIR), (3) Hydrometeorology (HM), (4) Warning and Emergency (WE), (5) Response and Recovery (RR), (6) Public Health (PH), and (7) Mitigation (MG). To develop FF-BERT, we labeled 21,180 paragraphs from FF-related webpages and performed experiments with multiple model architectures based on the widely used language model Bidirectional Encoder Representation from Transformers (BERT). Our final model outperforms the baseline by 11.83%, as measured by the micro-F1 score. In addition, FF-BERT significantly improves the prediction of minority labels (RR-32.1%, PH-260.4%, and MG-138.6%). We demonstrate using real world examples that FF-BERT can be used to uncover new information about flash flood events. This information can be used to enhance existing databases, such as NOAA’s Storm Events Database.}
}
@incollection{KORNHAUSER2024111,
title = {Chapter 5 - The role of automotive artificial intelligence},
editor = {Alain L. Kornhauser and Michael L. Sena},
booktitle = {The Real Case for Driverless Mobility},
publisher = {Elsevier},
pages = {111-130},
year = {2024},
isbn = {978-0-443-23685-3},
doi = {https://doi.org/10.1016/B978-0-443-23685-3.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443236853000017},
author = {Alain L. Kornhauser and Michael L. Sena},
keywords = {Artificial intelligence (AI), Artificial general intelligence (AGI), Automotive artificial intelligence (AAI), Big data, Driver monitoring systems},
abstract = {Driverless vehicles will use artificial intelligence (AI). AI-equipped vehicles will interpret and learn from external data and imitate the physical and mental processes used to drive motorized vehicles. The purpose of this chapter is to explain the different types of artificial intelligence, to what extent they are being used today in the design and operation of motorized vehicles, and how they can be put to use for both driving vehicles without human intervention and improving their safe operation. We also discuss the non-technological implications of automotive AI, including its effects on privacy, the assignment of liability in case of an accident, and the reasons for establishing laws and standards that precede the introduction of driverless vehicles, not reacting to their presence.}
}
@article{KNOTH2024100225,
title = {AI literacy and its implications for prompt engineering strategies},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100225},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100225},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000262},
author = {Nils Knoth and Antonia Tolzin and Andreas Janson and Jan Marco Leimeister},
keywords = {Large language model, AI literacy, Prompt engineering, AI interaction, Education},
abstract = {Artificial intelligence technologies are rapidly advancing. As part of this development, large language models (LLMs) are increasingly being used when humans interact with systems based on artificial intelligence (AI), posing both new opportunities and challenges. When interacting with LLM-based AI system in a goal-directed manner, prompt engineering has evolved as a skill of formulating precise and well-structured instructions to elicit desired responses or information from the LLM, optimizing the effectiveness of the interaction. However, research on the perspectives of non-experts using LLM-based AI systems through prompt engineering and on how AI literacy affects prompting behavior is lacking. This aspect is particularly important when considering the implications of LLMs in the context of higher education. In this present study, we address this issue, introduce a skill-based approach to prompt engineering, and explicitly consider the role of non-experts' AI literacy (students) in their prompt engineering skills. We also provide qualitative insights into students’ intuitive behaviors towards LLM-based AI systems. The results show that higher-quality prompt engineering skills predict the quality of LLM output, suggesting that prompt engineering is indeed a required skill for the goal-directed use of generative AI tools. In addition, the results show that certain aspects of AI literacy can play a role in higher quality prompt engineering and targeted adaptation of LLMs within education. We, therefore, argue for the integration of AI educational content into current curricula to enable a hybrid intelligent society in which students can effectively use generative AI tools such as ChatGPT.}
}
@incollection{HARTSON2025411,
title = {Chapter 22 - Empirical UX Evaluation:: Data Collection},
editor = {Rex Hartson and Pardha S. Pyla},
booktitle = {The UX Book (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
pages = {411-441},
year = {2025},
isbn = {978-0-443-13443-2},
doi = {https://doi.org/10.1016/B978-0-443-13443-2.00022-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443134432000224},
author = {Rex Hartson and Pardha S. Pyla},
keywords = {empirical UX evaluation data collection, critical incident identification, user think-aloud technique, co-discovery, task perform metrics, questionnaires, emotional impact and meaningfulness evaluation data collection, evaluation session protocol, specialized UX evaluation methods},
abstract = {Empirical UX evaluation data collection techniques yield data directly observed and/or measured. They depend on actual experience rather than, say, theory or analysis. In practice, collection of objective quantitative empirical data usually means involving usage by real user participants performing representative or real tasks. Collection of subjective quantitative empirical data usually depends on questionnaires, often featuring semantic differential scales. Qualitative empirical data are used to identify UX problems in a design. Useful techniques include user think-aloud sessions and the identification of critical incidents, usage events that reveal something significant about the quality of the user experience, observed during task performance.}
}
@article{KLIMAS2025287,
title = {The 5-dimensional behavioural coopetitor profile: How to measure it?},
journal = {Industrial Marketing Management},
volume = {124},
pages = {287-303},
year = {2025},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2024.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019850124002013},
author = {Patrycja Klimas and Karina Sachpazidu and Sylwia Stańczyk and Arkadiusz Kawa and Michał Nadolny},
keywords = {Coopetition, Coopetitor profiles, Behavioural approach, Behavioural features, Operationalization, Scale development, Scale validation},
abstract = {This paper investigates the behavioural component of the coopetitor profile, its operationalization and measurement. First, the general coopetitor profile framework is presented together with its four main components. Second, the behavioural component of this profile is shown as covering specific behavioural features describing coopetitors. Third, operationalizations for these behavioural features are offered and tested. Our research process combines desk and field research. We develop the concept of the coopetitor profile through literature analysis, as well as integrating existing measurement approaches to operationalize the identified behavioural features, and test these operationalizations via a large-scale study of 1216 Polish manufacturing companies. We present the original concept of the coopetitor profile, consisting of four components: strategic, cultural, resource-competency and behavioural. Using the lens of partner fit, we discuss the relevance of profiling coopetitors for partner selection, and coopetition establishment and management. We conceptualize and operationalize eleven behavioural features, resulting in a robust model for a five-dimensional behavioural profile covering conflict, formality, intensity of competition, investments and trust. This study contributes to the literature by examining the less-explored aspects of coopetition - coopetitor profile, behavioural approach and behavioural features. The primary methodological contribution is developing and validating a pioneering multi-item scale for measuring the multidimensional behavioural component of the profile of coopetitors.}
}
@article{POLAKOVA20242332,
title = {Examining the Reliability of ChatGPT as an Assessment Tool Compared to Human Evaluators},
journal = {Procedia Computer Science},
volume = {246},
pages = {2332-2341},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.543},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025882},
author = {Petra Poláková and Petra Ivenz and Blanka Klímová},
keywords = {ChatGPT, assessment tool, comparative study, artificial intelligence, foreign language learning},
abstract = {With the increasing development of artificial intelligence (AI), educators are exploring various AI-driven tools to enhance language teaching and learning processes. This study investigates the reliability of ChatGPT as an AI-powered assessment tool in the context of foreign language (FL) education. Using a mixed-methods approach, the research examines ChatGPT’s evaluative abilities compared to human evaluators, focusing on its assessment of students’ writing skills. The study employed a total of two research samples: one comprising eight students and another consisting of eight teachers of English language at the university where the research took place. The findings reveal that while ChatGPT demonstrates potential in providing feedback, its limitations, particularly in accurate assessment of grammatical nuances, emphasize the need for supplementation with human evaluation. The method section provides detailed information on the data collection process, including the procedures followed for evaluating student summaries and conducting interviews with teachers. The study highlights the importance of acknowledging AI’s limitations and adopting a complementary approach to its integration in language education.}
}
@article{PARASAR2024105351,
title = {Root exudation drives abiotic stress tolerance in plants by recruiting beneficial microbes},
journal = {Applied Soil Ecology},
volume = {198},
pages = {105351},
year = {2024},
issn = {0929-1393},
doi = {https://doi.org/10.1016/j.apsoil.2024.105351},
url = {https://www.sciencedirect.com/science/article/pii/S0929139324000829},
author = {Bhaskar Jyoti Parasar and Indrani Sharma and Niraj Agarwala},
keywords = {Abiotic stress factors, Root exudates, Nutrient cycling, Plant-microbe interactions},
abstract = {Plants under changing conditions release different blends of root exudates, which can play key role in modulating rhizospheric microbiome and soil nutrient cycling. Correlational network analysis of stress alleviated root exudate compounds with the different rhizospheric microbes, depicts a significant relationship of plant associated microbes and exuded chemicals during stress condition/alleviation. Therefore, understanding the root associated structural and functional attributes with respect to the change in the environmental factors, modulating rhizospheric microbiome is utmost necessary. Here, we highlight the current knowledge of abiotic stress induced alteration in root exudates composition; and resulting modulation in the rhizospheric microbiome to alleviate plants from stress regimes. Understanding plant-root exudation-microbiome dynamics during stress condition can provide a way to develop innnovative solutions that can revolutionize sustainable agriculture.}
}
@article{SELCUK2025106136,
title = {AI-driven civil litigation: Navigating the right to a fair trial},
journal = {Computer Law & Security Review},
volume = {57},
pages = {106136},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106136},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000094},
author = {Seyhan Selçuk and Nesibe {Kurt Konca} and Serkan Kaya},
keywords = {AI in legal proceedings, Automated legal reasoning, ECHR article 6, European union AI act, Judicial independence and AI, Publicity principle in AI justice, Right to a fair trial},
abstract = {The integration of artificial intelligence (AI) into legal proceedings has gained significant traction in recent years, particularly following the Covid-19 pandemic. As part of the broader movement toward the digitalization of legal systems, AI is seen as a tool to improve access to justice, enhance efficiency, and adopt a human-centered approach. However, the rapid advancement of AI necessitates careful consideration of fundamental human rights, especially the right to a fair trial as enshrined in Article 6 of the European Convention on Human Rights (ECHR). Recently, the European Union's Artificial Intelligence Act classifies AI systems used in the judiciary as high-risk, requiring impact assessments on fundamental rights, including the right to a fair trial. This paper explores the impact of AI-driven judicial tools on the right to a fair trial, focusing on key components such as the right to be heard, judicial independence, impartiality, and the principle of publicity. This paper explores the impact of AI-driven judicial tools on the right to a fair trial, focusing on key components such as the right to be heard, judicial independence, impartiality, and the principle of publicity, while examining the risks and opportunities posed by AI in civil litigation, including challenges like algorithmic discrimination, digital exclusion, and the potential erosion of human judges' cognitive abilities.}
}
@article{DHAIGUDE2025100640,
title = {Mapping responsible artificial intelligence in business and management: Trends, influence, and emerging research directions},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {11},
number = {4},
pages = {100640},
year = {2025},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2025.100640},
url = {https://www.sciencedirect.com/science/article/pii/S2199853125001751},
author = {Amol S. Dhaigude and Giridhar B. Kamath},
keywords = {Responsible Artificial Intelligence, Innovation management, Responsible innovation, Business and management, Bibliometric analysis, Bibliographic coupling, VOSviewer, Biblioshiny, Research clusters},
abstract = {The rapid integration of AI into business and management demands ethical and responsible technology design and deployment. While various policies and frameworks exist, there is limited understanding of operationalizing responsible artificial intelligence (RAI). The literature remains fragmented, lacking cohesion and clarity. This bibliometric analysis quantitatively evaluates RAI literature’s research trends, key authors, collaborations, and thematic evolution in the business and management domain. A carefully designed search protocol based on an extensive literature review was used to retrieve 1942 research papers from the Scopus database (1981–2025), reflecting a 13.12 % annual growth rate and an average of 25.79 citations per paper. The study applied bibliographic coupling, keyword co-occurrence, and thematic mapping techniques using VOSviewer and Biblioshiny to identify intellectual structures and conceptual linkages. The results reveal four key clusters: "Ethics and Social Impacts of AI", "AI Adoption and Human-AI Interaction", "Auditing, Explainability, and Accountability in AI", and "Corporate Governance and Data Responsibility in AI". Future research directions for each cluster are proposed, providing valuable insights for practitioners and academicians. The paper highlights critical implications for developing responsible AI strategies in business and offers guidance for advancing scholarly work in this growing field.}
}
@article{DEVILLA2024102316,
title = {Doing process research in international business},
journal = {International Business Review},
volume = {33},
number = {5},
pages = {102316},
year = {2024},
issn = {0969-5931},
doi = {https://doi.org/10.1016/j.ibusrev.2024.102316},
url = {https://www.sciencedirect.com/science/article/pii/S0969593124000635},
author = {Maria Andrea {De Villa} and Ann Langley},
keywords = {Process research, International business, Time, Process data, Process theory},
abstract = {Process research develops our understanding of the emergence, flow, and evolution of phenomena over time through ongoing activities and events. The field of international business has long been concerned with developing an understanding of various kinds of processes over time. Yet, several international business scholars have called for more studies that recognize the process-based nature of international business phenomena. This paper therefore discusses the challenges of process research in international business and offers insight into how they may be addressed. We draw on the broader methodological literature and on exemplar process studies in international business to explore and illustrate a repertoire of methodological tools that may be useful in assembling the three critical ingredients of high-quality process research: rich process data, insightful process theory, and credible but creative coupling between the two.}
}
@article{SHENG2024123550,
title = {A review of mechanistic insights into CO2 reduction to higher alcohols for rational catalyst design},
journal = {Applied Catalysis B: Environmental},
volume = {343},
pages = {123550},
year = {2024},
issn = {0926-3373},
doi = {https://doi.org/10.1016/j.apcatb.2023.123550},
url = {https://www.sciencedirect.com/science/article/pii/S0926337323011931},
author = {Yao Sheng and Mikhail V. Polynski and Mathan K. Eswaran and Bikun Zhang and Alvin M.H. Lim and Lili Zhang and Jianwen Jiang and Wen Liu and Sergey M. Kozlov},
keywords = {CO reduction, Catalysis, Alcohols, Oxygenates, Reaction mechanism},
abstract = {The utilization of captured CO2 for chemical synthesis could play an important role in reducing CO2 emissions. Higher alcohols stand out among various products of CO2 reduction due to high market prices and diverse applications, e.g., as fuel additives. However, developing catalysts for this reaction requires a profound understanding of the reaction mechanisms and catalyst design principles, which are discussed in the present review. Depending on the catalytic sites, higher alcohol synthesis could proceed via vastly different pathways. Herein, we outline how various proposed reaction mechanisms lead to different catalyst design strategies for optimizing the rate of CO2 conversion into reactive C1 intermediates (CO, CHx, CHxO, and HCOO) and their coupling into C2+ intermediates that are eventually converted into higher alcohols. Lastly, we discuss knowledge gaps in achieving rational catalyst design for higher alcohol synthesis and the breakthrough potential of machine-learning techniques for catalyst discovery.}
}
@article{WULFERT2024104035,
title = {Follow the flow: An exploratory multi-case study of value creation in e-commerce ecosystems},
journal = {Information & Management},
volume = {61},
number = {8},
pages = {104035},
year = {2024},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2024.104035},
url = {https://www.sciencedirect.com/science/article/pii/S0378720624001174},
author = {Tobias Wulfert and Robert Woroch and Gero Strobel},
keywords = {E-commerce, Business ecosystems, Platforms, Value creation, Value network analysis, Multi-case study},
abstract = {Platform-based ecosystems dominate e-commerce, generating value through participant growth and resulting network effects. However, research has lacked any conceptualization of value creation in e-commerce ecosystems. This paper fills this gap by providing a theoretically grounded and empirically validated conceptualization of value creation and exchange, including roles, value creation activities, and value flows among participants. The model integrates insights from a systematic literature review and a multi-case study of ten leading e-commerce ecosystems. Furthermore, an extension to the e3-value notation is proposed by introducing ecosystem segments, allowing for a higher level of abstraction of meta-roles and individual ecosystem participants.}
}
@article{VANNGUYEN2023109558,
title = {Digital transformation for cost estimation system via meta-learning and an empirical study in aerospace industry},
journal = {Computers & Industrial Engineering},
volume = {184},
pages = {109558},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109558},
url = {https://www.sciencedirect.com/science/article/pii/S036083522300582X},
author = {Tran Hong {Van Nguyen} and Pei-Min Huang and Chen-Fu Chien and Chung-Kai Chang},
keywords = {Digital transformation, Cost estimation, Aerospace manufacturing, Artificial intelligence, Decision support system},
abstract = {It is increasingly challenging to estimate product cost to optimize the quotation decision for aerospace manufacturing companies owing to the diversity of designations, intermittent demands, complicated decision procedure between different functional departments, and knowledge gaps among the involved decision makers. There is a need of effective solutions to support digital transformation of manual approach in which it can expedite and enhance the accuracy of cost estimation by automating specific tasks and streamlining decision processes. Although a number of studies have been done to enhance the performance of forecasting models, little research has been done to address the interrelations between cost estimation model and the associated decisions for quotation. Focusing on realistic needs, this study aims to develop a digital cost estimation system by integrating data-driven methodologies, search engines, and rule-based decision mechanism based on domain knowledge for improving the accuracy of cost estimation and the effectiveness of quotation for revenue management. An empirical study was conducted in a global aerospace manufacturer in Taiwan for validation. The results have shown the practical viability for the proposed framework with better performance than conventional approaches. The developed solution has been implemented.}
}
@article{OLIVEIRA2025527,
title = {Unveiling the potential of digital human avatars in modern marketing strategies},
journal = {International Marketing Review},
volume = {42},
number = {4},
pages = {527-555},
year = {2025},
issn = {0265-1335},
doi = {https://doi.org/10.1108/IMR-12-2023-0339},
url = {https://www.sciencedirect.com/science/article/pii/S0265133525000267},
author = {Fabio Goncalves de Oliveira and Maksim Belitski and Nada Kakabadse and Nicholas Theodorakopoulos},
keywords = {Digital human avatars, Global digital marketing strategy, International dynamic marketing capabilities, Digital transformation, Generative AI, Absorptive capacity},
abstract = {Purpose
This study aims to develop a theoretical framework that marketing practitioners and scholars can adopt to enhance their understanding of how firms can effectively deploy and use digital human avatars as part of their global digital marketing strategy. By doing so, we inform investors of ongoing digital transformations of marketing practices that will equip marketeers to provide scalable, tailored, reliable and relevant digital self-service interactions to users, consequently improving the user/customer experience.
Design/methodology/approach
Thematic analysis was used to discover factors to enable the successful implementation of digital human avatars, drawing on in-depth interviews with fourteen executives of digital human avatars developer companies worldwide and analysis of ten podcasts and webinars with artificial intelligence (AI) experts.
Findings
Digital human avatars revitalise the international dynamic marketing capabilities (IDMCs) of firms by integrating advanced technologies that transform user interactions, improve engagement and facilitate knowledge acquisition, dissemination and usage across various sectors and business units globally. This integration promotes a dynamic approach to international brands, customer relationships and marketing knowledge management capabilities, offering profound value to users and firms.
Research limitations/implications
Our first limitation is a lack of diversity in data sources. As digital human avatars are an emerging field, we had to limit our study to 14 experts in AI and 10 podcasts. While this method provides deep insights into the perspectives of those directly involved in the development and implementation of digital human avatars, it may not capture the views of end-users or consumers who interact with these avatars, which can be an avenue for further research. Our second limitation is the potential bias in the interpretation of our interview data and podcasts. This study’s approach to data analysis, where themes are derived from the data itself, carries a risk of subjective interpretation by the researchers. Future studies are encouraged to investigate the impact of digital human avatars across different organisational contexts and ecosystems, especially focusing on how these technologies are integrated and perceived in various international markets.
Practical implications
The novel framework has direct implications for innovators and marketing practitioners who aim to adopt digital human avatars in their marketing practices to enhance the effectiveness of international marketing strategies.
Social implications
The adoption of digital human avatars can alleviate loneliest elderly and vulnerable people by being a companion. The human-like characteristics can impact sense of presence and attachment.
Originality/value
The novelty of our study lies in exploring the characteristics of technologies and practical factors that maximise the successful adoption of digital human avatars. We advance and contribute to the emerging theory of avatar marketing, IDMCs and absorptive capacity by demonstrating how digital human avatars could be adopted as part of a firm’s global digital marketing strategy. We focus specifically on six dimensions: outcomes and benefits, enhancements and capabilities, applications and domains, future implications, foundational elements and challenges and considerations. This framework has direct implications for innovators and marketing practitioners who aim to adopt digital human avatars in their marketing practices to enhance the effectiveness of international marketing strategies.}
}
@article{HAFFNER2025101905,
title = {Directions for future IS research on sports digitalisation: A stakeholder perspective},
journal = {The Journal of Strategic Information Systems},
volume = {34},
number = {2},
pages = {101905},
year = {2025},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2025.101905},
url = {https://www.sciencedirect.com/science/article/pii/S0963868725000204},
author = {Lily Haffner and Ilan Oshri and Julia Kotlarsky},
keywords = {Sports Digitalisation, Stakeholder Perspective, Theoretical Review, Sports},
abstract = {In this theoretical review, we analyse the IS literature on the rapidly evolving strategic phenomenon of sports digitalisation. We provide insights on how sports digitalisation is currently understood in the IS literature when examined from a stakeholder perspective. Our analysis identifies the key stakeholders involved in sports, the competition stages (i.e., before, during, or after), the nature of digitalisation in sports, and the key outcomes on which the literature focuses (behavioural, cognitive, and performance-related). Our analysis also points out several gap areas in the extant literature: the lack of attention paid to non-competing stakeholders (e.g., managers, referees, spectators) and the behavioural impact of digitalisation on stakeholders during competitions; and the current focus on analysing the effect of a single digitalisation solution on a single stakeholder. Building on these findings, we propose three future directions to advance research on sports digitalisation. First, future research needs to examine the effect of digitalisation on an eco-system of sports stakeholders. Second, the use of naturalistic reality images as a data source would enhance research during competition. Last, future studies need to consider the blending of borrowed (behavioural and cognitive) theories and novel digitally-enhanced concepts to strengthen the theoretical foundations of sports digitalisation in Information Systems.}
}
@article{KAPPEL2024106303,
title = {Measuring affect-related attention bias to emotionally valenced visual stimuli in horses},
journal = {Applied Animal Behaviour Science},
volume = {275},
pages = {106303},
year = {2024},
issn = {0168-1591},
doi = {https://doi.org/10.1016/j.applanim.2024.106303},
url = {https://www.sciencedirect.com/science/article/pii/S0168159124001515},
author = {Sarah Kappel and Marco A.Ramirez Montes De Oca and Sarah Collins and Katherine Herborn and Michael Mendl and Carole Fureix},
keywords = {Emotional state, Affective measures, Visual attention, Horse behaviour, Animal welfare},
abstract = {Negative affect appears to mediate animals’ attention to competing emotional stimuli (e.g., threatening vs. non-threatening conspecific face images), similarly to anxiety-related enhanced attention to social threat reported in humans. To investigate this ‘attention bias’ (AB, i.e., the differential attention allocation to certain types of information over others) in horses, we developed a visual AB test assessing horses’ attention towards image pairs showing unfamiliar conspecifics’ facial expressions indicating, a) negative (social threat), b) more neutral (at rest), and c) positive (food anticipation) situations. We predicted that horses exhibit greater attention to negative compared to neutral or positive face images (as a normal adaptive response), and that horses in negative affective states (inferred from validated welfare indices comprising direct (health, behaviour) and indirect (housing, management) measures summarised as individual welfare scores and subscores reflecting health, social and environmental aspects) show greater AB to negative face stimuli and all images overall. Comparing AB to positive versus neutral social stimuli is rarely considered in AB studies, we therefore explored horses’ AB responses without a priori predictions. Over six trials, 44 horses from three facilities were shown stimulus pairs (negative/neutral, negative/positive, positive/neutral) presented simultaneously on two projector screens. Attention was assessed as absolute attention duration to each image, the proportion of time the negative/positive stimulus was attended to relative to the other stimulus, and overall attention (i.e., duration of head turns towards both stimuli combined). AB to stimulus type, side, effects of facility and individual characteristics (welfare and subscores, age) was analysed using linear and generalised mixed-effect models. Against our predictions, horses attended to the images within the three stimulus pairs for similar lengths of time (negative-neutral: W=1870.5, p=0.2572; negative-positive: W=2542.5, p=0.9296; positive-neutral: W=1762.5, p=0.1019). Due to Covid-19 interruptions, our sample size was lower than our estimated required number (N=113). Still, lower welfare (X21=4.71, p=0.03) and health scores (X21=4.13, p=0.04) significantly predicted shorter attention to the negative face stimuli, possibly reflecting threat avoidance previously reported in other animals. We found significant facility effects on overall attention to the stimuli (X22=77.42, p<0.001), likely due to varying yard-specific conditions (e.g., lighting, noise). This highlights that external influences on visual attention require consideration when conducting cognitive tests at different testing sites. Further methodological investigation (e.g., test cue suitability, perceptual processing of computer-generated images; test stimuli familiarity; individual differences) is needed to evaluate the potential of AB as an indicator of affective valence in horses.}
}
@article{2024A9,
title = {Guide for Authors},
journal = {Journal of the American Society of Echocardiography},
volume = {37},
number = {7},
pages = {A9-A18},
year = {2024},
note = {35th ASE Annual Scientific Sessions},
issn = {0894-7317},
doi = {https://doi.org/10.1016/S0894-7317(24)00239-6},
url = {https://www.sciencedirect.com/science/article/pii/S0894731724002396}
}
@article{EMSSAAD2025100221,
title = {Leveraging multilingual RAG for breast cancer RCPs: AI-driven speech transcription and compliance in Darija-French clinical discussions},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {8},
pages = {100221},
year = {2025},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2025.100221},
url = {https://www.sciencedirect.com/science/article/pii/S2666990025000461},
author = {Ilyass Emssaad and Fatima-Ezzahraa Ben-Bouazza and Idriss Tafala and Manal Chakour El Mezali and Bassma Jioudi},
keywords = {Multilingual AI, Retrieval-augmented generation, Clinical decision support, Automatic speech recognition},
abstract = {The integration of artificial intelligence (AI) into clinical decision-making has introduced new opportunities for automating and enhancing medical documentation, particularly in oncology, where multidisciplinary meetings are central to treatment planning. However, existing speech-to-text and retrieval-augmented generation (RAG) systems are not equipped to operate effectively in multilingual, dialect-rich environments such as those in North African hospitals where Moroccan Darija, Arabic, and French are frequently interwoven. These linguistic complexities, combined with the high-stakes nature of clinical dialogue, challenge transcription accuracy, contextual information retrieval, and regulatory compliance. This study presents a multilingual RAG system tailored to clinical meetings, integrating a fine-tuned Whisper ASR model with a sentence-level semantic retrieval pipeline and a compliance-aware generation framework. Evaluated on real-world clinical queries, the system demonstrates improved transcription quality and retrieval precision over standard pipelines, while enforcing factual grounding and safety through multi-stage output validation. These results highlight the potential of multilingual, speech-driven AI to support decision-making and compliance in linguistically diverse healthcare environments, offering a deployable foundation for clinical NLP in underserved regions.}
}
@article{HADAN2024100095,
title = {The great AI witch hunt: Reviewers’ perception and (Mis)conception of generative AI in research writing},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {2},
pages = {100095},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100095},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000550},
author = {Hilda Hadan and Derrick M. Wang and Reza Hadi Mogavi and Joseph Tu and Leah Zhang-Kennedy and Lennart E. Nacke},
keywords = {Artificial intelligence, Generative AI, Reviewer perception, Research writing, AI writing augmentation},
abstract = {Generative AI (GenAI) use in research writing is growing fast. However, it is unclear how peer reviewers recognize or misjudge AI-augmented manuscripts. To investigate the impact of AI-augmented writing on peer reviews, we conducted a snippet-based online survey with 17 peer reviewers from top-tier HCI conferences. Our findings indicate that while AI-augmented writing improves readability, language diversity, and informativeness, it often lacks research details and reflective insights from authors. Reviewers consistently struggled to distinguish between human and AI-augmented writing but their judgements remained consistent. They noted the loss of a “human touch” and subjective expressions in AI-augmented writing. Based on our findings, we advocate for reviewer guidelines that promote impartial evaluations of submissions, regardless of any personal biases towards GenAI. The quality of the research itself should remain a priority in reviews, regardless of any preconceived notions about the tools used to create it. We emphasize that researchers must maintain their authorship and control over the writing process, even when using GenAI's assistance.}
}
@article{KORKMAZ2024104954,
title = {From GitHub to GDP: A framework for measuring open source software innovation},
journal = {Research Policy},
volume = {53},
number = {3},
pages = {104954},
year = {2024},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2024.104954},
url = {https://www.sciencedirect.com/science/article/pii/S0048733324000039},
author = {Gizem Korkmaz and J. Bayoán {Santiago Calderón} and Brandon L. Kramer and Ledia Guci and Carol A. Robbins},
keywords = {Open source software, Cost measurement, GitHub, Innovation, Gross domestic product, Software investment, National accounts},
abstract = {Open source software (OSS) is software that anyone can review, modify, and distribute freely, usually with only minor restrictions such as giving credit to the creator of the work. The use of OSS is growing rapidly, due to its value in increasing firm and economy-wide productivity. Despite its widespread use, there is no standardized methodology for measuring the scope and impact of this fundamental intangible asset. This study presents a framework to measure the value of OSS using data collected from GitHub, the largest platform in the world with over 100 million developers. The data include over 7.6 million repositories where software is developed, stored, and managed. We collect information about contributors and development activity such as code changes and license detail. By adopting a cost estimation model from software engineering, we develop a methodology to generate estimates of investment in OSS that are consistent with the U.S. national accounting methods used for measuring software investment. We generate annual estimates of current and inflation-adjusted investment as well as the net stock of OSS for the 2009–2019 period. Our estimates show that the U.S. investment in 2019 was $37.8 billion with a current-cost net stock of $74.3 billion.}
}
@article{ALADINI2025100566,
title = {Self-directed writing development across computer/AI-based tasks: Unraveling the traces on L2 writing outcomes, growth mindfulness, and grammatical knowledge},
journal = {Computers in Human Behavior Reports},
volume = {17},
pages = {100566},
year = {2025},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100566},
url = {https://www.sciencedirect.com/science/article/pii/S2451958824001994},
author = {Alaa Aladini and Sayed M. Ismail and Mohamad {Ahmad Saleem Khasawneh} and Goodarz Shakibaei},
keywords = {AI-based tasks, Grammatical knowledge, Growth mindfulness, L2 writing outcomes, Self-directed writing},
abstract = {As technology continues to reshape the educational landscape, understanding how autonomous engagement with AI-driven platforms influences second language (L2) acquisition is increasingly important. This study examined the impact of self-directed writing development through computer- and AI-based tasks on L2 writing outcomes, growth mindfulness, and grammatical knowledge. The research was conducted in three private English institutes in Ahvaz, Iran, involving 561 intermediate-level EFL learners that were selected using a convenience sampling method. Using a quasi-experimental design, participants were divided into two groups: an experimental group (EG) of 276 students, which engaged in self-directed AI-based writing tasks, and a control group (CG) of 285 students, which followed traditional writing instruction. Quantitative data were gathered through pre- and post-tests measuring writing proficiency, growth mindfulness, and grammatical accuracy. Additionally, qualitative data were collected through interviews and questionnaires administered to the EG to assess their attitudes toward AI-based learning. The results indicated that the EG significantly outperformed the CG in writing outcomes and grammatical accuracy. Moreover, the EG demonstrated greater growth in mindfulness, with participants showing increased awareness of their writing processes and enhanced self-regulation strategies. Interviews and questionnaires revealed that participants in the EG held positive attitudes toward AI-based tasks, emphasizing increased engagement, autonomy, enjoyment, motivation, personalized learning, critical thinking, self-efficacy, time management, and collaboration in their English learning development. These findings underscore the potential of AI-based tasks in fostering both linguistic competence and metacognitive skills in L2 learners. The study provides valuable insights into the role of AI technologies in promoting independent learning and calls for further exploration of AI-based interventions in writing education.}
}
@article{MOHAMMADI2025118632,
title = {An interpretable machine learning-based model for shear resistance prediction of CFRP-strengthened RC beams using experimental and synthetic dataset},
journal = {Composite Structures},
volume = {351},
pages = {118632},
year = {2025},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2024.118632},
url = {https://www.sciencedirect.com/science/article/pii/S0263822324007608},
author = {Amirhossein Mohammadi and Joaquim A.O. Barros and José Sena-Cruz},
keywords = {Machine learning (ML), RC beams, Shear strengthening, Externally Bonded Reinforcement (EBR), Carbon Fibre Reinforced Polymer (CFRP), Synthetic dataset},
abstract = {Existing analytical models for predicting the shear resistance of RC beams strengthened with externally bonded CFRP reinforcements exhibit deficient performance due to their inability to accurately capture the complex resisting mechanisms. Combined with significant statistical uncertainties in shear failure, driven by its brittle nature, this further undermines the reliability of these models. To address these limitations, this study leverages Machine Learning (ML) to develop more robust and reliable predictive tool. A rigorous feature-selection process identified eight predictors as the most influential. Subsequently, nine ML-algorithms were trained on a refined experimental dataset comprising 239 beams, with XGBoost emerging as the top performer. This model also outperformed established models likefib Bulletin-90 and ACI 2023 models. However, the limited scope of the experimental dataset constrained the model’s predictive performance especially when separately evaluated on beams strengthened with U-wraps, full wraps or side-bonded FRP configurations. Therefore, to achieve a more reliable model a synthetic dataset was generated using Tabular Variational Auto-Encoder. The XGBoost model trained with the synthetic dataset significantly improved the performance of the former model and exhibited better predictions for all strengthening configurations. Finally, to ensure the physical consistency of predictions, values obtained from the SHapley Additive exPlanations method were analysed.}
}
@article{SOROOSHIAN2025146,
title = {Influencer marketing: service supplier selection},
journal = {Management Decision},
volume = {63},
number = {13},
pages = {146-173},
year = {2025},
issn = {0025-1747},
doi = {https://doi.org/10.1108/MD-12-2023-2366},
url = {https://www.sciencedirect.com/science/article/pii/S0025174725000035},
author = {Shahryar Sorooshian},
keywords = {Influencer marketing, Service supplier, Social media influencer, Internet celebrities, Influencer endorsement},
abstract = {Purpose
The main objective of this study is to lay the groundwork for a systematic approach to selecting social media influencers (SMI) for influencer marketing campaigns.
Design/methodology/approach
This study achieves its objective by presenting an innovative framework that combines the ordinal priority approach (OPA) with the Delphi method. This hybrid approach is applied to an academic event promotion case study. The original 22 selection criteria for SMIs were derived from the Delphi evaluation. These criteria were subsequently ranked using modified OPA to select influencers in a systematic and hierarchical fashion.
Findings
This research proves the effectiveness of the framework by applying it to a case study. Three top-level critical criteria, 13 intermediate-level criteria and six additional criteria are revealed by this hierarchical prioritization of SMI selection criteria. This methodical procedure allows for a more logical and educated decision-making process in selecting the best influencers for marketing campaigns. This research also proves the feasibility of the proposed model.
Practical implications
Better influencer marketing campaigns and marketing resource allocation are possible outcomes of the suggested framework, which marketers and businesses can use as a more organized and objective tool for selecting SMIs.
Originality/value
This study contributes to the field of influencer marketing by developing and validating a novel decision framework. This work not only fills the gap in existing research regarding quantitative decision-making models for SMI selection but also expands applications of the OPA method to address service supplier selection problems.}
}
@article{GARRARD2024171282,
title = {Identifying potential high-risk zones for land-derived plastic litter to marine megafauna and key habitats within the North Atlantic},
journal = {Science of The Total Environment},
volume = {922},
pages = {171282},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2024.171282},
url = {https://www.sciencedirect.com/science/article/pii/S0048969724014219},
author = {Samantha L. Garrard and James R. Clark and Nicola Martin and Sarah E. Nelms and Zara L.R. Botterell and Matthew Cole and Rachel L. Coppock and Tamara S. Galloway and Dannielle S. Green and Megan Jones and Pennie K. Lindeque and Heidi M. Tillin and Nicola J. Beaumont},
keywords = {Marine plastic pollution, Land-derived plastic litter, North Atlantic, Marine megafauna, Biogenic habitats, Risk assessment},
abstract = {The pervasive use of plastic in modern society has led to plastic litter becoming ubiquitous within the ocean. Land-based sources of plastic litter are thought to account for the majority of plastic pollution in the marine environment, with plastic bags, bottles, wrappers, food containers and cutlery among the most common items found. In the marine environment, plastic is a transboundary pollutant, with the potential to cause damage far beyond the political borders from where it originated, making the management of this global pollutant particularly complex. In this study, the risks of land-derived plastic litter (LDPL) to major groups of marine megafauna – seabirds, cetaceans, pinnipeds, elasmobranchs, turtles, sirenians, tuna and billfish – and a selection of productive and biodiverse biogenic habitats – coral reefs, mangroves, seagrass, saltmarsh and kelp beds – were analysed using a Spatial Risk Assessment approach. The approach combines metrics for vulnerability (mechanism of harm for megafauna group or habitat), hazard (plastic abundance) and exposure (distribution of group or habitat). Several potential high-risk zones (HRZs) across the North Atlantic were highlighted, including the Azores, the UK, the French and US Atlantic coasts, and the US Gulf of Mexico. Whilst much of the modelled LDPL driving risk in the UK originated from domestic sources, in other HRZs, such as the Azores archipelago and the US Gulf of Mexico, plastic originated almost exclusively from external (non-domestic) sources. LDPL from Caribbean islands - some of the largest generators of marine plastic pollution in the dataset of river plastic emissions used in the study - was noted as a significant input to HRZs across both sides of the Atlantic. These findings highlight the potential of Spatial Risk Assessment analyses to determine the location of HRZs and understand where plastic debris monitoring and management should be prioritised, enabling more efficient deployment of interventions and mitigation measures.}
}
@article{MENG2026104496,
title = {Can ChatGPT relate to you? Exploring consumer satisfaction with AI-generated product advice through the lens of consumption values},
journal = {Journal of Retailing and Consumer Services},
volume = {88},
pages = {104496},
year = {2026},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2025.104496},
url = {https://www.sciencedirect.com/science/article/pii/S0969698925002759},
author = {Kexin Meng and Jing Jian Xiao},
keywords = {Generative AI (GAI), Consumption values, Consumer satisfaction, Trust, Product attribute, Task type},
abstract = {As generative AI (GAI) tools increasingly assist consumers in making purchasing decisions, understanding how users evaluate such recommendations has become a timely and essential research question. This study applies the Theory of Consumption Values to examine how consumers develop satisfaction with ChatGPT's product recommendations, while also critically assessing the relevance of each value dimension in the context of AI-mediated interactions. Drawing on a randomized online experiment with U.S. participants, we investigate the relationships between customer satisfaction with ChatGPT's responses and five perceived consumption values: functional, emotional, epistemic, social, and conditional. We also investigate if trust plays a mediating role in these connections and whether they differ depending on the task type (requiring mostly human or machine skills) and product attribute (functional, experiential, hybrid). The findings indicate that, except for social value, all value dimensions are strongly linked to greater levels of satisfaction; the most significant correlation is found for functional value. Trust significantly mediates the value–satisfaction relationship, with conditional value exhibiting the most substantial indirect effect through trust. Interestingly, functional value plays a more prominent role in associating with satisfaction for experiential and hybrid products, as well as in human-skill-intensive tasks, rather than in functional products or machine-skill-oriented tasks. Conditional value consistently shows both direct and mediated associations with satisfaction across functional and hybrid products, as well as both task types. Epistemic value demonstrates both direct and mediated associations with satisfaction in functional and experiential product contexts, as well as in tasks requiring primarily machine-based skills. These findings advance theoretical understanding of value perception in GAI interactions and offer practical insights for designing context-sensitive and trust-oriented recommendation systems.}
}
@article{YI2025,
title = {Bridging Technology and Pretest Genetic Services: Quantitative Study of Chatbot Interaction Patterns, User Characteristics, and Genetic Testing Decisions},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/73391},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125012646},
author = {Yang Yi and Lauren Kaiser-Jackson and Jemar R Bather and Melody S Goodman and Daniel Chavez-Yenter and Richard L Bradshaw and Rachelle Lorenz Chambers and Whitney F Espinel and Rachel Hess and Devin M Mann and Rachel Monahan and David W Wetter and Ophira Ginsburg and Meenakshi Sigireddi and Kensaku Kawamoto and Guilherme {Del Fiol} and Saundra S Buys and Kimberly A Kaphingst},
keywords = {cancer genetics, chatbot delivery, user interaction, genetic testing, pretest genetic services},
abstract = {Background
Among the alternative solutions being tested to improve access to genetic services, chatbots (or conversational agents) are being increasingly used for service delivery. Despite the growing number of studies on the accessibility and feasibility of chatbot genetic service delivery, limited attention has been paid to user interactions with chatbots in a real-world health care context.
Objective
We examined users’ interaction patterns with a pretest cancer genetics education chatbot as well as the associations between users’ clinical and sociodemographic characteristics, chatbot interaction patterns, and genetic testing decisions.
Methods
We analyzed data from the experimental arm of Broadening the Reach, Impact, and Delivery of Genetic Services, a multisite genetic services pragmatic trial in which participants eligible for hereditary cancer genetic testing based on family history were randomized to receive a chatbot intervention or standard care. In the experimental chatbot arm, participants were offered access to core educational content delivered by the chatbot with the option to select up to 9 supplementary informational prompts and ask open-ended questions. We computed descriptive statistics for the following interaction patterns: prompt selections, open-ended questions, completion status, dropout points, and postchat decisions regarding genetic testing. Logistic regression models were used to examine the relationships between clinical and sociodemographic factors and chatbot interaction variables, examining how these factors affected genetic testing decisions.
Results
Of the 468 participants who initiated a chat, 391 (83.5%) completed it, with 315 (80.6%) of the completers expressing a willingness to pursue genetic testing. Of the 391 completers, 336 (85.9%) selected at least one informational prompt, 41 (10.5%) asked open-ended questions, and 3 (0.8%) opted for extra examples of risk information. Of the 77 noncompleters, 57 (74%) dropped out before accessing any informational content. Interaction patterns were not associated with clinical and sociodemographic factors except for prompt selection (varied by study site) and completion status (varied by family cancer history type). Participants who selected ≥3 prompts (odds ratio 0.33, 95% CI 0.12-0.91; P=.03) or asked open-ended questions (odds ratio 0.46, 95% CI 0.22-0.96; P=.04) were less likely to opt for genetic testing.
Conclusions
Findings highlight the chatbot’s effectiveness in engaging users and its high acceptability, with most participants completing the chat, opting for additional information, and showing a high willingness to pursue genetic testing. Sociodemographic factors were not associated with interaction patterns, potentially indicating the chatbot’s scalability across diverse populations provided they have internet access. Future efforts should address the concerns of users with high information needs and integrate them into chatbot design to better support informed genetic decision-making.}
}
@article{2025iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {253},
pages = {iii-xxii},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(25)00405-3},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925004053}
}
@article{KARTASHOV2025117742,
title = {A large language model and denoising diffusion framework for targeted design of microstructures with commands in natural language},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {437},
pages = {117742},
year = {2025},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2025.117742},
url = {https://www.sciencedirect.com/science/article/pii/S0045782525000143},
author = {Nikita Kartashov and Nikolaos N. Vlassis},
keywords = {Microstructure design, Large language models, Denoising diffusion probabilistic models, Natural language processing},
abstract = {Microstructure plays a critical role in determining the macroscopic properties of materials, with applications spanning alloy design, MEMS devices, and tissue engineering, among many others. Computational frameworks have been developed to capture the complex relationship between microstructure and material behavior. However, despite these advancements, the steep learning curve associated with domain-specific knowledge and complex algorithms restricts the broader application of these tools. To lower this barrier, we propose a framework that integrates Natural Language Processing (NLP), Large Language Models (LLMs), and Denoising Diffusion Probabilistic Models (DDPMs) to enable microstructure design using intuitive natural language commands. Our framework employs contextual data augmentation, driven by a pretrained LLM, to generate and expand a diverse dataset of microstructure descriptors. A retrained Named Entity Recognition (NER) model extracts relevant microstructure descriptors from user-provided natural language inputs, which are then used by the DDPM to generate microstructures with targeted mechanical properties and topological features. The NLP and DDPM components of the framework are modular, allowing for separate training and validation, which ensures flexibility in adapting the framework to different datasets and use cases. A surrogate model system is employed to rank and filter generated samples based on their alignment with target properties. This work introduces a comprehensive framework that bridges natural language processing and mechanics, addressing key challenges such as the lack of training data, syntax invariance in textual descriptors, and precision in text embeddings. Demonstrated on a database of nonlinear hyperelastic microstructures, this framework serves as a prototype for accessible inverse design of microstructures, starting from intuitive natural language commands.}
}
@article{KAUR2025113606,
title = {3D printed biomaterials: From fabrication techniques to clinical applications: A systematic review},
journal = {European Polymer Journal},
volume = {227},
pages = {113606},
year = {2025},
issn = {0014-3057},
doi = {https://doi.org/10.1016/j.eurpolymj.2024.113606},
url = {https://www.sciencedirect.com/science/article/pii/S001430572400867X},
author = {Amandeep Kaur and Sandeep Singh and Niraj Bala and Sushil {Kumar Kansal}},
keywords = {3D printing, 3D printing techniques Biomaterials, Medical applications},
abstract = {The type of biomaterial employed in the manufacturing of an implant determines its success. The material for implants should be easily adaptable, inert biocompatible, and mechanically robust. But the ideal properties may vary depending on the specific application and patient needs. In the pharmaceutical and medical industries, 3D printing technology has revolutionised the capacity to create customised implants for patients that are integrated with bioactive medications, cells, and proteins. Currently, a wide range of biomaterials such as ceramics, polymers, metals, hydrogels and composites are employed in medical 3D printing. Various applications of 3D printing such as manufacture of personalised implants, prostheses, drug delivery systems and 3D scaffolds for biological tissue engineering and regenerative medicine have grown quickly as a result of ongoing research and advancements in the biomaterials used in 3D printing. In this study, novel biomaterials utilised in medical 3D printing techniques are the main subject of interest. Furthermore, the present review sheds light on the most widely used medical 3D printing technologies, including fused deposition modelling, extrusion-based bioprinting, polyjet printing, and inkjet methods, as well as their clinical applications, different biomaterials that are presently under investigation by researchers, and important challenges.}
}
@article{DEMIRHAN2025125,
title = {A deep learning framework for prediction of crop yield in Australia under the impact of climate change},
journal = {Information Processing in Agriculture},
volume = {12},
number = {1},
pages = {125-138},
year = {2025},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2024.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2214317324000246},
author = {Haydar Demirhan},
keywords = {Cereal grains, Crop yield, Crop production, Deep learning, Temperature anomalies, Climate change},
abstract = {Accurate prediction of crop yields is essential to ensure food security. In this study, a new deep neural networks framework is developed to predict crop yields in Australia, considering the impact of climate change, fertilizer use, and crop area. It is implemented for oats, corn, rice, and wheat crops, and its forecasting performance is benchmarked against five statistical and machine learning methods. All the software codes for the implementation of the proposed framework are freely available. The proposed framework shows the highest forecasting performance for all the considered crop types. It provides 23%, 38%, 39%, and 40% lower average mean absolute error than the benchmark methods for oat, corn, rice, and wheat crops, respectively. The reductions in average root mean squared error are 19%, 25%, 37%, and 29% over the benchmark methods. Then, it is used to predict yields of the considered crops in Australia towards 2025 under six different climate change scenarios. It is observed that although climate change has some boosting impact on crop yield, it is not sustainable to meet the demand. However, it is possible to keep crop yields rising while mitigating climate change.}
}
@article{KEEGAN2023228,
title = {Examining the dark force consequences of AI as a new actor in B2B relationships},
journal = {Industrial Marketing Management},
volume = {115},
pages = {228-239},
year = {2023},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2023.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0019850123001918},
author = {Brendan James Keegan and Sophie Iredale and Peter Naudé},
keywords = {Artificial intelligence, B2B relationships, Dark forces, Tensions, Dehumanization},
abstract = {Artificial intelligence (AI) in industrial marketing has seen significant research attention through various theoretical lenses with an emerging thread examining the dark side effects of AI. Thirty-four semi-structured interviews were conducted with buyers and suppliers of AI marketing solutions to investigate the consequences of AI ‘dark forces’ on B2B relationships. We posit AI as a new actor that has blurred the lines of the actors-resources-activities model. Findings show AI is now considered a new actor within B2B networks wielding dark force consequences such as algorithmic gatekeeping, which initiates dehumanization effects. In addition, AI is reliant on access to datasets which drives up resource costs. A lack of accountability of AI marketing solutions leads to opportunistic behaviours compromising actor relationships. Our conceptual model maps our understanding of the dark force consequences underpinning theoretical and managerial implications and recommendations for increased awareness and mitigation of dark forces.}
}
@article{CORDEIRO2025111139,
title = {Object segmentation dataset generation framework for robotic bin-picking: Multi-metric analysis between results trained with real and synthetic data},
journal = {Computers & Industrial Engineering},
volume = {205},
pages = {111139},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111139},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225002852},
author = {Artur Cordeiro and Luís Freitas Rocha and José Boaventura-Cunha and Eduardo J. Solteiro Pires and João Pedro Souza},
keywords = {Robotic bin-picking, Object segmentation, Deep learning dataset, Data synthesis, Automation},
abstract = {The implementation of deep learning approaches based on instance segmentation data remains a challenge for customized scenarios, owing to the time-consuming nature of acquiring and annotating real-world instance segmentation data, which requires a significant investment of semi-professional user labour. Obtaining high-quality labelled data demands expertise and meticulous attention to detail. This requirement can significantly impact the overall implementation process, adding to the complexity and resource requirements of customized scenarios with diverse objects. The proposed work addresses the challenge of generating labelled data for large-scale robotic bin-picking datasets by proposing an easy-to-use automated framework designed to create customized data with accurate labels from CAD models. The framework leverages a photorealistic rendering engine integrated with physics simulation, minimizing the gap between synthetic and real-world data. Models trained using the synthetic data generated by this framework achieved an Average Precision of 86.95%, comparable to the performance of models trained on real-world datasets. Furthermore, this paper provides a comprehensive multi-metric analysis across diverse objects representing distinct industrial applications, including naval, logistics, and aerospace domains. The evaluation also includes the use of three distinct instance segmentation networks, alongside a comparative analysis of the proposed approach against two generative model techniques.}
}
@article{FALCINELLI2025102418,
title = {Micro-herbs: a valuable source of phytochemicals from aromatic and medicinal plants},
journal = {Journal of Agriculture and Food Research},
volume = {24},
pages = {102418},
year = {2025},
issn = {2666-1543},
doi = {https://doi.org/10.1016/j.jafr.2025.102418},
url = {https://www.sciencedirect.com/science/article/pii/S2666154325007896},
author = {Beatrice Falcinelli and Paolo Benincasa and Jouhaina Riahi and Roberta Bulgari},
keywords = {Sprout, Microgreen, Spice species, Bioactive, Antioxidant, Germination},
abstract = {In recent years, interest in medicinal and aromatic plants (MAPs) has grown, as they are key sources of traditional remedies and raw materials for many plant-based recipes products worldwide. Meanwhile, the consumption of edible sprouts and microgreens has grown substantially, driven by their appealing taste and high content in phytochemicals with potential health-promoting effects. An emerging research trend has combined these two subjects, dealing with sprouts and microgreens of MAPs, which have been found, or are supposed, to be extremely interesting from a nutraceutical point of view. However, existing studies predominantly focus on a limited number of commonly cultivated species. In contrast, many lesser-known MAPs remain underexplored and data on their sprouts and microgreens are occasional and fragmented. This review surveys the last 25-year literature on sprouts and microgreens of MAPs, here referred to under the term of micro-herbs, which is more intuitive to a broader audience. After describing sprouts and microgreens characteristics, as along with an overview on phytochemicals, their antioxidant properties and related health benefits, the review examines the available evidence on micro-herbs derived from either herbaceous and shrub/tree species, emphasizing their distinctive traits, and evaluating their potentialities and limits related to their use in human nutrition. Current gaps and perspectives for future research on this emerging subject are finally discussed.}
}
@article{LENG2024111373,
title = {ArchiDiffusion: A novel diffusion model connecting architectural layout generation from sketches to Shear Wall Design},
journal = {Journal of Building Engineering},
volume = {98},
pages = {111373},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.111373},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224029413},
author = {Hao Leng and Yuqing Gao and Ying Zhou},
keywords = {Shear wall structures, Intelligent design framework, Layout generation, Deep learning, Diffusion model, Automation in building design},
abstract = {Shear wall structures are widely used in residential and office buildings, but the overall design process is complex and inefficient. This study introduces a novel diffusion-based model, named ArchiDiffusion, designed to revolutionize the generation of architectural floor plans from user-drawn sketches. ArchiDiffusion employs a two-stage process that ensures both creativity and practicality, setting a new standard in automated design. Embedded within the End-to-End Shear Wall Design (EESD) framework, ArchiDiffusion seamlessly integrates with existing structural design processes, including the previously developed StructDiffusion model, to streamline the traditionally complex and inefficient design workflow. Experimental results demonstrate that ArchiDiffusion significantly outperforms existing models in layout generation, reducing design time and enhancing project quality. A case study reveals that the EESD framework, powered by ArchiDiffusion, increases the design efficiency by 50 times compared to traditional workflows. Notably, ArchiDiffusion achieves a 9.6%–15.2% improvement in layout generation accuracy over traditional GAN-based models, while the EESD framework demonstrates an approximate 20% enhancement in overall building performance, as measured by energy efficiency and spatial optimization metrics. This research highlights the significant potential of ArchiDiffusion in intelligent design processes for shear wall residential buildings, offering a promising solution to the challenges faced in integrating architectural and structural design.}
}
@article{KARALASINGHAM2024101333,
title = {Wavelet-fusion image super-resolution model with deep learning for downscaling remotely-sensed, multi-band spectral albedo imagery},
journal = {Remote Sensing Applications: Society and Environment},
volume = {36},
pages = {101333},
year = {2024},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2024.101333},
url = {https://www.sciencedirect.com/science/article/pii/S2352938524001976},
author = {Sagthitharan Karalasingham and Ravinesh C. Deo and David Casillas-Pérez and Nawin Raj and Sancho Salcedo-Sanz},
keywords = {Spectral albedo, Bifacial PV, PV yield modelling, Reflected solar radiation},
abstract = {Generating granular-scale surface albedo data is extremely important for solar photovoltaic site planning and to optimise renewable energy yield of bifacial panel installations. The albedo effect brings about a significant increase in power in bifacial photovoltaic systems, compared to their mono-facial counterparts, since the spectral response of bifacial solar panels correlates with the incident solar radiation wavelength on the back of the panel, to provide additional power generation capacity. Thus, harnessing the albedo data at relatively local scales is critical towards boosting solar power generation and providing greater power density in local electricity grids. This paper develops novel modelling approaches to produce high-resolution spectral albedo imagery across the Visible and Near Infrared (VNIR) bands, using the Wavelet-Fusion super-resolution model (i.e., Wavelet-FusionSR) trained with the Learned Gamma Correction approach by applying satellite image enhancement methodology. The proposed Wavelet-FusionSR model utilises the low-resolution moderate-resolution Imaging Spectroradiometer (MODIS) as well as high-resolution multi-spectral Advanced Space-borne Thermal Emission Reflection Radiometer (ASTER) satellite images, as critical inputs and ground-truth imagery, respectively, in order to perform sensor-to-sensor deep downscaling, without employing any synthetic or low-resolution satellite imagery data pairs. To augment the proposed deep learning algorithm across the decomposed sub-images of low-resolution inputs, we integrate local and global feature representation learning to train the proposed Wavelet-FusionSR model with Cauchy loss functions. In comparison with five competing benchmark models, the proposed Wavelet-FusionSR model demonstrates performance superiority using quantitative image downscaling metrics and visual assessments of the downscaled images for the visible band of solar radiation. The proposed Wavelet-FusionSR model yielded a Mean Square Error (MSE) of 0.00017, Signal-to-noise-ratio (PSNR) of 37.80, Structural Similarity Index (SSIM) of 0.999 and combined loss, MS-SSIMLoss, based on Multi Structural Similarity and Mean Absolute Error of 2.354 for the Visible Band images, and an MSE of 0.0014, PSNR of 28.43, SSIM of 0.999 and MS-SSIMLoss of 7.426 for the NIR spectral bands, demonstrating high efficacy of the proposed Wavelet-FusionSR method. The Wavelet-FusionSR method therefore attains high-resolution spectral albedo imagery outputs.}
}
@article{SHORE2024103063,
title = {Building entrepreneurial resilience during crisis using generative AI: An empirical study on SMEs},
journal = {Technovation},
volume = {135},
pages = {103063},
year = {2024},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2024.103063},
url = {https://www.sciencedirect.com/science/article/pii/S0166497224001135},
author = {Adam Shore and Manisha Tiwari and Priyanka Tandon and Cyril Foropon},
keywords = {Generative AI, Entrepreneurial orientation, Entrepreneurial resilience, Market turbulence, Dynamic capability view},
abstract = {Recently, Gen AI has garnered significant attention across various sectors of society, particularly capturing the interest of small business due to its capacity to allow them to reassess their business models with minimal investment. To understand how small and medium-sized firms have utilised Gen AI-based tools to cope with the market's high level of turbulence caused by the COVID-19 pandemic, geopolitical crises, and economic slowdown, researchers have conducted an empirical study. Although Gen AI is receiving more attention, there remains a dearth of empirical studies that investigate how it influences the entrepreneurial orientation of firms and their ability to cultivate entrepreneurial resilience amidst market turbulence. Most of the literature offers anecdotal evidence. To address this research gap, the authors have grounded their theoretical model and research hypotheses in the contingent view of dynamic capability. They tested the research hypotheses using cross-sectional data from a pre-tested survey instrument, which yielded 87 useable responses from small and medium enterprises in France. The authors used variance-based structural equation modelling with the commercial WarpPLS 7.0 software to test the theoretical model. The study's findings suggest that Gen AI and EO have a significant influence on building entrepreneurial resilience as higher-order and lower-order dynamic capabilities. However, market turbulence has a negative moderating effect on the path that joins entrepreneurial orientation and entrepreneurial resilience. The results suggest that the assumption that high market turbulence will have positive effects on dynamic capabilities and competitive advantage is not always true, and the linear assumption does not hold, which is consistent with some scholars' assumptions. The study's results offer significant contributions to the contingent view of dynamic capabilities and open new research avenues that require further investigation into the non-linear relationship of market turbulence.}
}
@article{TAN2025107715,
title = {EC5: Edge–cloud collaborative computing framework with compressive communication},
journal = {Future Generation Computer Systems},
volume = {166},
pages = {107715},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2025.107715},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2500010X},
author = {Jingwei Tan and Fagui Liu and Bin Wang and Qingbo Wu and C.L. Philip Chen},
keywords = {Edge computing, Edge–cloud collaborative computing, Collaborative inference, DNN partition, Feature compression},
abstract = {With an increasing number of deep neural network (DNN)-based applications being deployed at the edges, edge–cloud collaborative computing has emerged as a promising solution to alleviate the burden on resource-constrained edges by collaborative inference. However, simply offloading part of DNN to the cloud introduces significant communication overhead during inference. In this paper, we propose EC5, an Edge–Cloud Collaborative Computing framework with Compressive Communication. The compression of the intermediate feature is formulated using information theory and jointly optimized with the DNN through end-to-end multi-task learning. By decomposing DNN parameters into a new space, EC5 enables efficient storage and update of models across various compression levels. An Adaptive Exit scheme is designed to retain high-confidence inputs on the edge for fast inference, reducing reliance on the cloud. Experimental comparisons with baseline methods prove that EC5 significantly conserves network bandwidth and reduces communication instances, with low latency and acceptable accuracy loss, showing flexibility across different communication scenarios.}
}
@article{YAN2025105322,
title = {The effects of generative AI agents and scaffolding on enhancing students’ comprehension of visual learning analytics},
journal = {Computers & Education},
volume = {234},
pages = {105322},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105322},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000909},
author = {Lixiang Yan and Roberto Martinez-Maldonado and Yueqiao Jin and Vanessa Echeverria and Mikaela Milesi and Jie Fan and Linxuan Zhao and Riordan Alfredo and Xinyu Li and Dragan Gašević},
keywords = {Scaffolding, Generative AI, Artificial intelligence, Large language model, Visualisation literacy, Visual learning analytics, Learning analytics dashboard},
abstract = {Visual learning analytics (VLA) is becoming increasingly adopted in educational technologies and learning analytics dashboards to convey critical insights to students and educators. Yet many students experienced difficulties in comprehending complex VLA due to their limited data visualisation literacy. While conventional scaffolding approaches like data storytelling have shown effectiveness in enhancing students’ comprehension of VLA, these approaches remain difficult to scale and adapt to individual learning needs. Generative AI (GenAI) technologies, especially conversational agents, offer potential solutions by providing personalised and dynamic support to enhance students’ comprehension of VLA. This controlled lab study investigates the effectiveness of GenAI agents, particularly when integrated with scaffolding techniques, in improving students’ comprehension of VLA. A randomised controlled trial was conducted with 117 higher education students to compare the effects of two types of GenAI agents: passive agents, which respond to student queries, and proactive agents, which utilise scaffolding questions, against standalone scaffolding in a VLA comprehension task. The results show that passive agents yield comparable improvements to standalone scaffolding both during and after the intervention. Notably, proactive GenAI agents significantly enhance students’ VLA comprehension compared to both passive agents and standalone scaffolding, with these benefits persisting beyond the intervention. These findings suggest that integrating GenAI agents with scaffolding can have lasting positive effects on students’ comprehension skills and support genuine learning.}
}
@article{BIANCHINI2025124303,
title = {Drivers and barriers of AI adoption and use in scientific research},
journal = {Technological Forecasting and Social Change},
volume = {220},
pages = {124303},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124303},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525003348},
author = {Stefano Bianchini and Moritz Müller and Pierre Pelletier},
keywords = {Artificial intelligence, Technology adoption, Scientific discovery, Economics of science},
abstract = {We study the early adoption and use of artificial intelligence (AI) in scientific research. Using a large dataset of publications from OpenAlex (all fields, up to 2024) and building on theories of scientific and technical human capital, we identify key factors that influence AI adoption. We find that early adopters were domain scientists embedded in AI-rich collaboration networks and affiliated with institutions with strong AI credentials. Access to high-performance computing (HPC) mattered only in a few scientific disciplines, such as biology and medical sciences. More recently, as tools like Large Language Models (LLMs) have diffused, AI has become more accessible, and institutional advantages appear to matter less. However, social capital—especially ties to AI-experienced collaborators and early-career researchers—remains a persistent driver of adoption. We discuss the implications for science policy and the organization of research in the age of AI.}
}
@article{KULAL2024100329,
title = {Enhancing public service delivery efficiency: Exploring the impact of AI},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {3},
pages = {100329},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100329},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124001239},
author = {Abhinandan Kulal and Habeeb Ur Rahiman and Harinakshi Suvarna and N. Abhishek and Sahana Dinesh},
keywords = {Artificial Intelligence (AI), Public Service Delivery, Maturity Level, Efficiency Impact},
abstract = {This study aims to investigate the impact of Artificial Intelligence (AI) adoption on public service delivery efficiency in India. It addresses a significant gap in existing literature by investigating the impact of AI adoption on public service delivery efficiency in India, a context that has not been extensively explored. Through a comparative analysis approach, the study assesses the effectiveness of AI applications in enhancing public service delivery. The quantitative research design employed in the study draws on previous literature on AI integration in governance and focuses on Chief Information Officers (CIOs) as primary respondents. The findings reveal significant improvements in citizen-centric services and municipal processes due to AI adoption. However, the impact on human-centric aspects is found to be moderate. The study also underscores the importance of infrastructure readiness for successful AI implementation. Notably, only 25 % of organizations were found to be possessing advanced technological infrastructure. This research is original in its focus on Chief Information Officers (CIOs) as primary respondents and its comparative analysis approach to assess the effectiveness of AI applications in enhancing public service delivery. This study offers valuable insights for policymakers and practitioners. Emphasizing the need for effective policies and infrastructure development, it highlights the potential of AI to eliminate corruption risks and enhance overall efficiency and transparency in public service delivery mechanisms.}
}
@article{KHALIFA2024100142,
title = {RETRACTED: Advancing clinical decision support: The role of artificial intelligence across six domains},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {5},
pages = {100142},
year = {2024},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2024.100142},
url = {https://www.sciencedirect.com/science/article/pii/S2666990024000090},
author = {Mohamed Khalifa and Mona Albadawy and Usman Iqbal},
abstract = {This article has been retracted: please see Elsevier policy on article withdrawal (https://www.elsevier.com/about/policies-and-standards/article-withdrawal). This article has been retracted following an investigation conducted by the Publisher which determined that this paper was handled by the former Co-Editor-in-Chief of the journal, who was also an author of the paper. This compromised the impartiality of the peer review process and necessitated the retraction of the paper. As such this article represents a misuse of the scientific publishing system. The scientific community takes a very strong view on this matter and apologies are offered to readers of the journal that this was not detected during the submission process.}
}
@article{KARIOTIS2025,
title = {Patient-Accessible Electronic Health Records and Information Practices in Mental Health Care Contexts: Scoping Review},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/54973},
url = {https://www.sciencedirect.com/science/article/pii/S143888712500189X},
author = {Timothy Kariotis and Megan Prictor and Kathleen Gray and Shanton Chang},
keywords = {patient-accessible electronic health records, patient portals, psychiatry, mental health, health informatics, mental illness, scoping review},
abstract = {Background
Patients are increasingly being provided with access to their electronic health records. However, in mental health care contexts, concerns have been raised due to a perception that such access would pose risks to patients, third parties, and the therapeutic relationship. These perceived risks may affect the information practices of health care professionals (HCPs) and patients, such as how they document, share, and use information in mental health care services with a patient-accessible electronic health record (PAEHR). Although there is growing research interest in PAEHRs, no study has specifically examined how they impact information practices. Understanding the impacts on information practices may help explain other outcomes of implementing PAEHRs and inform their design.
Objective
This scoping review aimed to explore the research on PAEHRs in mental health care contexts and how PAEHRs affect information practices of HCPs and patients in this context.
Methods
A scoping review was considered the most appropriate method due to the relatively recent adoption of PAEHRs in mental health care contexts and the heterogeneous nature of the evidence base. A comprehensive search of electronic databases was conducted for original empirical studies that described the use of PAEHRs or associated systems in mental health care contexts. One author reviewed all full texts, with 3 other authors reviewing a subset of studies. The study characteristics and findings were documented, and a thematic synthesis and textual narrative analysis were used to develop descriptive and analytical themes that addressed the research questions.
Results
A total of 66 studies were considered eligible and included in the analysis. The impact of PAEHRs on information practices in mental health care contexts included the following: (1) they may change how HCPs document patient information, including a reduction in detail and a focus on information perceived by HCPs as objective rather than subjective; (2) they may negatively impact workflows due to changes in documentation practices and limited guidance for HCPs on how to use PAEHRs; and (3) they may contribute to improved communication between HCPs and patients, including constructive disagreements regarding what is documented in the health record. The changes to HCP information practices were influenced by a concern for the therapeutic relationship and patient safety. Furthermore, PAEHRs supported new information practices for patients, such as using their PAEHR to prepare for clinical encounters.
Conclusions
We have identified several ways in which PAEHRs shape the information practices of HCPs and patients in the mental health context. These findings can inform the design of PAEHRs to promote information practices that contribute to improving the quality of mental health care. Further research is necessary to understand how changes in information practices due to the implementation of PAEHRs impact clinical outcomes and patient experiences of care.}
}
@article{GAFFINET2025104230,
title = {Human Digital Twins: A systematic literature review and concept disambiguation for industry 5.0},
journal = {Computers in Industry},
volume = {166},
pages = {104230},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104230},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524001581},
author = {Ben Gaffinet and Jana {Al Haj Ali} and Yannick Naudet and Hervé Panetto},
keywords = {Human Digital Twin, Industry 5.0, Digital Twin, Human-centricity},
abstract = {Human Digital Twins (HDTs) are an emerging concept with the potential to create human-centric systems for Industry 5.0. The concept has rapidly spread to new application domains, most notably Healthcare, leading to diverging conceptual interpretations. This Systematic Literature Review analyses the conceptual understanding of HDTs across all application domains to clarify the conceptual foundation. Our review reveals a consensus that an HDT’s twinned entity is a human individual. However, there is little agreement on the data flows between the individual and their HDT. We address this shortcoming by proposing three categories based on the level of data integration: Human Digital Models, Human Digital Shadows, and Human Digital Twins. Finally, we synthesise our findings in a domain-agnostic general definition for HDT. We highlight an edge case where the twinned entity is a human individual alongside a strongly coupled technical system, and name it augmented Human Digital Twin (aHDT). The definition and categorisation scheme provide the needed conceptual clarity for inter-disciplinary collaboration to address open challenges. Notable challenges are sensing human data, reliable data transfers and modelling, especially behavioural modelling. Additional ethical issues concerning security, privacy and consent are central to successful HDT adoption. We call for cross-disciplinary efforts to establish a standardised framework and ethical guidelines to enable future developments.}
}
@article{TEUTLOFF2025106845,
title = {Winners and losers of generative AI: Early Evidence of Shifts in Freelancer Demand},
journal = {Journal of Economic Behavior & Organization},
volume = {235},
pages = {106845},
year = {2025},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2024.106845},
url = {https://www.sciencedirect.com/science/article/pii/S0167268124004591},
author = {Ole Teutloff and Johanna Einsiedler and Otto Kässi and Fabian Braesemann and Pamela Mishkin and R. Maria {del Rio-Chanona}},
keywords = {Generative AI technologies, Large language models, Automation and employment, Labor market implications of AI, Technological transition, Online labor markets},
abstract = {We examine how ChatGPT has changed the demand for freelancers in jobs where generative AI tools can act as substitutes or complements to human labor. Using BERTopic we partition job postings from a leading online freelancing platform into 116 fine-grained skill clusters and with GPT-4o we classify them as substitutable, complementary or unaffected by LLMs. Our analysis reveals that labor demand increased after the launch of ChatGPT, but only in skill clusters that were complementary to or unaffected by the AI tool. In contrast, demand for substitutable skills, such as writing and translation, decreased by 20–50% relative to the counterfactual trend, with the sharpest decline observed for short-term (1-3 week) jobs. Within complementary skill clusters, the results are mixed: demand for machine learning programming grew by 24%, and demand for AI-powered chatbot development nearly tripled, while demand for novice workers declined in general. This result suggests a shift toward more specialized expertise for freelancers rather than uniform growth across all complementary areas.}
}
@article{TAI2025102082,
title = {An exploration of agile government in the public sector: A systematic literature review at macro, meso, and micro levels of analysis},
journal = {Government Information Quarterly},
volume = {42},
number = {4},
pages = {102082},
year = {2025},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2025.102082},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X25000760},
author = {Kuang-Ting Tai and Pallavi Awasthi},
keywords = {Agile, Agility, Agile management, Agile governance, PRISMA, Systematic literature review},
abstract = {Originating from private sector software development, agile has permeated the public sector, fostering innovative reforms not just in project management but also in organizational management and collaborative governance. Despite its widespread adoption, there exists a paucity of research delving into the intricacies of agile practices, particularly for the potential conflicts and interactions with the traditional waterfall-based approaches. Employing the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) method, this systematic review aims to address three fundamental research questions concerning the conceptualization, implementation, and impacts of agile government. To deepen theoretical insight and practical application, our study classifies agile into three distinct levels: Micro (project management), Meso (organizational management), and Macro (governance structure). Our analysis uncovers substantial variations in agile practices across these levels, reflecting a deliberate strategy aimed at harmonizing with existing bureaucratic systems. This study concludes by offering policy implications and delineating avenues for future research endeavors.}
}
@article{CAI2025102587,
title = {Factors influencing engagement in EFL learning of higher education learners in blended learning environments},
journal = {International Journal of Educational Research},
volume = {131},
pages = {102587},
year = {2025},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2025.102587},
url = {https://www.sciencedirect.com/science/article/pii/S0883035525000618},
author = {Qianqian Cai},
keywords = {Engagement, Blended learning, Influencing factors, Higher education, English as a foreign language learning},
abstract = {Researchers have long been interested in engagement in blended learning. However, insufficient research has been conducted on factors influencing engagement in blended English as a foreign language learning. This study investigates the factors influencing engagement in blended English as a foreign language learning based on the hedonic-motivation system adoption model. This study aimed to develop a model exploring engagement in blended English as a foreign language learning employing quantitative structural equation modelling methods, supplemented by an open-ended review question. Based on feedback from 938 tertiary students, this study explored the relationship among perceived ease of use, usefulness, teacher support, enjoyment, social presence, L2 grit, and engagement in blended English as a foreign language learning using a structural equational modeling method. Through the thematic analysis of 418 responses to the open-ended question, the author gained insights into the merits, flaws, and recommendations of blended English as a foreign language learning. As a result, perceived ease of use positively affected the usefulness of blended English as a foreign language learning. Perceived ease of use, teacher support, and usefulness positively impacted the social presence of blended English as a foreign language learning. Perceived teacher support, ease of use, and social presence positively influenced the perceived enjoyment of blended English as a foreign language learning. Perceived enjoyment positively predicted second language (L2) grit of blended learning contexts. However, social presence insignificantly affected L2 grit in blended learning contexts. L2 grit, perceived enjoyment, usefulness, and social presence positively impacted engagement in blended English as a foreign language learning. Despite limitations, this study implies teachers, education researchers, and developers in further research and practice for blended language education.}
}
@article{BIEDERSTADT2025,
title = {Engineered natural killer cells for cancer therapy},
journal = {Cancer Cell},
year = {2025},
issn = {1535-6108},
doi = {https://doi.org/10.1016/j.ccell.2025.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1535610825004052},
author = {Alexander Biederstädt and Katayoun Rezvani},
keywords = {natural killer cells, cancer immunotherapy, cellular engineering, chimeric antigen receptor, CRISPR screening, perturbomics, precision gene editing, solid tumors, tumor microenvironment, adoptive cell therapy},
abstract = {Summary
Allogeneic natural killer (NK) cell immunotherapy is emerging as a promising and scalable, off-the-shelf platform for treating relapsed and refractory cancers. Early-phase clinical trials have demonstrated remarkable safety and encouraging therapeutic efficacy of chimeric antigen receptor (CAR)-NK cells in heavily pretreated patients with lymphoid malignancies. Current efforts are expanding these therapies to solid tumors, with translational research increasingly leveraging precision gene editing to enhance effector function, persistence, and resistance to the immunosuppressive tumor microenvironment. In this review, we summarize findings from early-phase clinical trials and discuss emerging synthetic biology and engineering approaches to improve NK cell potency. We also highlight advances in high-throughput discovery platforms that have identified actionable gene targets for NK cell reprogramming, offering a path to design multi-engineered CAR-NK cells to overcome the challenges of solid tumors. Together, these translational innovations define the trajectory of next-generation NK cell therapies and their integration into the broader cancer immunotherapy landscape.}
}
@article{KHA2026112739,
title = {Temporal diffuser: Timing scale-aware modulation for sign language production},
journal = {Engineering Applications of Artificial Intelligence},
volume = {163},
pages = {112739},
year = {2026},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.112739},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625027708},
author = {Kim-Thuy Kha and Anh H. Vo and Van-Vang Le and Oh-Young Song and Yong-Guk Kim},
keywords = {Sign language production, Space–Time U-Net, Timing scale-aware modulation, Temporal diffusion, Motion generation, Time series attention mechanism},
abstract = {Recent advances in Sign Language Production (SLP) highlight denoising diffusion models as promising alternatives to traditional autoregressive methods. Most existing approaches follow a two-stage pipeline that encodes sign motion into discrete latent codes, often sacrificing Space–Time fidelity and requiring gloss annotations or complex codebooks. Transformer-based models aim to simplify this, but often produce overly smooth, unnatural motions. We introduce Sign Language Production with Scale-Aware Modulation (SignSAM), a novel single-stage, gloss-free SLP framework that directly synthesizes motion in continuous space, preserving fine temporal details. At its core is a Space–Time U-Net that learns compact temporal features by jointly downscaling the frame and sign feature dimensions, thereby reducing computational cost compared to a no-pyramid UNet or a pyramid UNet without consistency between dimensions. To further enhance temporal precision, we propose a Timing Scale-Aware Modulation module that fuses multiscale temporal resolutions for better motion coherence. Experiments on PHOENIX14T and How2Sign show that SignSAM achieves state-of-the-art (SOTA) fluency, accuracy, and naturalness, offering an efficient and expressive solution for SLP. Our project homepage is https://kha-kim-thuy.github.io/SLP-Demo/.}
}
@article{MARTIN2025101529,
title = {The effects of climate change and environmental regulation analysing U.S. citizens' typology},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101529},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101529},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125002578},
author = {Juan Carlos Martín and Alessandro Indelicato},
keywords = {Climate change effects, Environmental regulation effects, Energy prices, Loss of individual freedom, Religion, Religious practices, Fuzzy clustering ECO-Extended apostle, PEW},
abstract = {This study analyses the relationship between the effects of climate change and environmental regulation for the first time, considering how some individuals' socioeconomic characteristics affect the different types of Americans' perceptions. A panoply of fuzzy set methods, including Fuzzy Hybrid TOPSIS, Fuzzy Clustering and Fuzzy Clustering ECO-Extended Apostle, are applied to a dataset of 10,156 respondents representing the USA. The latent variables are measured by two scales using an answer format based on how likely different effects of climate change and environmental regulation will happen within the next 30 years. The study determines the following respondents' categories: neither convinced of climate change and environmental regulation effects, convinced only of climate change, convinced only of environmental regulation, and convinced of both effects. The results indicate that Americans are more prone to be more convinced of both (73.8 %) than only environmental regulation effects (13.6 %), climate change effects (10.1 %) or none (2.5 %). Some socioeconomic, demographic, and other segmentation variables will be studied to analyse their impact on Americans' categorisation. The segmentation variables are mainly based on environmental attitudes, societal and political views, and political ideology. The implications and limitations of our results will be further discussed.}
}
@article{DEWILDE2025103454,
title = {Strategies for generating synthetic computed tomography-like imaging from radiographs: A scoping review},
journal = {Medical Image Analysis},
volume = {101},
pages = {103454},
year = {2025},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2025.103454},
url = {https://www.sciencedirect.com/science/article/pii/S1361841525000027},
author = {Daniel {De Wilde} and Olivier Zanier and Raffaele {Da Mutten} and Michael Jin and Luca Regli and Carlo Serra and Victor E. Staartjes},
keywords = {Artificial intelligence, Deep learning, Synthetic CT, Machine learning, Synthetic imaging, Image conversion},
abstract = {Background
Advancements in tomographic medical imaging have revolutionized diagnostics and treatment monitoring by offering detailed 3D visualization of internal structures. Despite the significant value of computed tomography (CT), challenges such as high radiation dosage and cost barriers limit its accessibility, especially in low- and middle-income countries. Recognizing the potential of radiographic imaging in reconstructing CT images, this scoping review aims to explore the emerging field of synthesizing 3D CT-like images from 2D radiographs by examining the current methodologies.
Methods
A scoping review was carried out following PRISMA-SR guidelines. Eligibility criteria for the articles included full-text articles published up to September 9, 2024, studying methodologies for the synthesis of 3D CT images from 2D biplanar or four-projection x-ray images. Eligible articles were sourced from PubMed MEDLINE, Embase, and arXiv.
Results
76 studies were included. The majority (50.8 %, n = 30) were published between 2010 and 2020 (38.2 %, n = 29) and from 2020 onwards (36.8 %, n = 28), with European (40.8 %, n = 31), North American (26.3 %, n = 20), and Asian (32.9 %, n = 25) institutions being primary contributors. Anatomical regions varied, with 17.1 % (n = 13) of studies not using clinical data. Further, studies focused on the chest (25 %, n = 19), spine and vertebrae (17.1 %, n = 13), coronary arteries (10.5 %, n = 8), and cranial structures (10.5 %, n = 8), among other anatomical regions. Convolutional neural networks (CNN) (19.7 %, n = 15), generative adversarial networks (21.1 %, n = 16) and statistical shape models (15.8 %, n = 12) emerged as the most applied methodologies. A limited number of studies included explored the use of conditional diffusion models, iterative reconstruction algorithms, statistical shape models, and digital tomosynthesis.
Conclusion
This scoping review summarizes current strategies and challenges in synthetic imaging generation. The development of 3D CT-like imaging from 2D radiographs could reduce radiation risk while simultaneously addressing financial and logistical obstacles that impede global access to CT imaging. Despite initial promising results, the field encounters challenges with varied methodologies and frequent lack of proper validation, requiring further research to define synthetic imaging's clinical role.}
}
@article{LIPPENS2024100054,
title = {Computer says ‘no’: Exploring systemic bias in ChatGPT using an audit approach},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100054},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100054},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000148},
author = {Louis Lippens},
keywords = {Large language models, ChatGPT, Systemic bias, Hiring discrimination, Correspondence audit},
abstract = {Large language models offer significant potential for increasing labour productivity, such as streamlining personnel selection, but raise concerns about perpetuating systemic biases embedded into their pre-training data. This study explores the potential ethnic and gender bias of ChatGPT—a chatbot producing human-like responses to language tasks—in assessing job applicants. Using the correspondence audit approach from the social sciences, I simulated a CV screening task with 34,560 vacancy–CV combinations where the chatbot had to rate fictitious applicant profiles. Comparing ChatGPT's ratings of Arab, Asian, Black American, Central African, Dutch, Eastern European, Hispanic, Turkish, and White American male and female applicants, I show that ethnic and gender identity influence the chatbot's evaluations. Ethnic discrimination is more pronounced than gender discrimination and mainly occurs in jobs with favourable labour conditions or requiring greater language proficiency. In contrast, gender bias emerges in gender-atypical roles. These findings suggest that ChatGPT's discriminatory output reflects a statistical mechanism echoing societal stereotypes. Policymakers and developers should address systemic bias in language model-driven applications to ensure equitable treatment across demographic groups. Practitioners should practice caution, given the adverse impact these tools can (re)produce, especially in selection decisions involving humans.}
}